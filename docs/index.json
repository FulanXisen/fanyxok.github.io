[{"content":" Diagram of PulseAudio Pipewire is an alternative. Preface I’m working on the Roc Toolkit open-source project, a development kit for realtime audio streaming over the network. You can read more about the project in these two articles: 1, 2.\nWe decided to implement a set of PulseAudio modules that will allow PulseAudio to use Roc as a network transport. Many Linux distros employ PulseAudio, and their users will be able to improve network service quality without changing the workflow. This led me to dig into PulseAudio internals and eventually to this document.\nMotivation PulseAudio has Documentation page covering many specific problems that may be encountered by user and developer. Modules page contains a complete list of existing modules with parameters. D-Bus API and C API are also documented well.\nUnfortunately, the available documentation doesn’t give a bird-eye view and an explanation of PulseAudio features and design and doesn’t cover many implementation details.\nIn result, the overall picture remains unclear. Advanced configuration looks mysterious because one need to understand what happens under the hood first. The learning curve for the module writer is high too.\nThis document tries to fill the gap and provide an overview of the PulseAudio features, architecture, and internals. More precisely, it has three goals:\ndescribe the available features explain their underlying design and important implementation details provide a starting point for writing clients and server modules It does not provide a detailed reference or tutorial for PulseAudio configuration and APIs. Further details can be obtained from the official documentation (for configuration and client APIs) and from the source code (for internal interfaces).\nDisclaimer I’m not a PulseAudio developer. This document reflects my personal understanding of PulseAudio, obtained from the source code, experiments, official wiki, mailing lists, and blog articles. It may be inaccurate. Please let me know about any issues.\nPulseAudio tends to trigger flame wars, which I believe are non-constructive. This document tries to be neutral and provide an unbiased overview of the implemented features and design.\nThanks I’d like to thank my friends and colleagues Mikhail Baranov and Dmitriy Shilin who read early drafts of the document and provided a valuable feedback.\nAlso big thanks to Tanu Kaskinen, a PulseAudio maintainer, who have found and helped to fix dozens of errors.\nThey all definitely made it better!\nMeta This document was last updated for PulseAudio 11.1.\nLast major update: 21 Oct 2017.\nAbout PulseAudio PulseAudio is a sound server for POSIX OSes (mostly aiming Linux) acting as a proxy and router between hardware device drivers and applications on single or multiple hosts.\nSee details on the About page on wiki.\nDesign goals PulseAudio is designed to meet a number of goals.\nAbstraction layer for desktop audio\nPulseAudio manages all audio applications, local and network streams, devices, filters, and audio I/O. It provides an abstraction layer that combines all this stuff together in one place.\nProgrammable behavior\nA rich API provides methods for inspecting and controlling all available objects and their both persistent and run-time properties. This makes it possible to replace configuration files with GUI tools. Many desktop environments provide such tools.\nAutomatic setup\nPulseAudio is designed to work out of the box. It automatically detects and configures local devices and sound servers available in the local network. It also implements numerous policies for automatic audio management and routing.\nFlexibility\nPulseAudio provides a high flexibility for the user. It’s possible to connect any stream of any application to any local or remote device, configure per-stream and per-device volumes, construct sound processing chains, and more.\nExtensibility\nPulseAudio provides a framework for server extensions, and many built-in features are implemented as modules. Non-official third-party modules exist as well, however, the upstream doesn’t provide a guarantee of a stable API for out-of-tree modules.\nFeature overview The following list gives an idea of the features implemented in PulseAudio.\nProtocols and networking\nPulseAudio supports a variety of network protocols to communicate with clients, remote servers, and third-party software.\nDevice drivers\nPulseAudio supports several backends to interact with hardware devices and controls. It supports hotplug and automatically configures new devices.\nSound processing\nPulseAudio implements various sound processing tools, like mixing, sample rate conversion, and acoustic echo cancellation, which may be employed manually or automatically.\nSample cache\nPulseAudio implements an in-memory storage for short named batches of samples that may be uploaded to the server once and then played multiple times.\nStream management\nPulseAudio manages all input and output streams of all desktop applications, providing them such features as clocking, buffering, and rewinding.\nTime management\nPulseAudio implements a per-device timer-based scheduler that provides clocking in the sound card domain, maintains optimal latency, and reduces the probability of playback glitches.\nPower saving\nPulseAudio employs several techniques to reduce CPU and battery usage.\nAutomatic setup and routing\nPulseAudio automatically sets parameters of cards, devices, and streams, routes streams to devices, and performs other housekeeping actions.\nDesktop integrations\nPulseAudio implements several features that integrate it into the desktop environment.\nCompatibility layers\nThere are several compatibility layers with other sound systems, so that existing applications may automatically run on top of PulseAudio without modification.\nUse cases Here are some practical examples of how PulseAudio features may be used on the desktop:\nSmart hotplug handling. For example, automatically setup Bluetooth or USB headset when it’s connected, or automatically switch to headphones when they’re inserted into the jack.\nA GUI for easy switching an audio card between various modes like stereo, surround, or S/PDIF.\nA GUI for easy switching an audio stream to any available audio device, like internal speakers, wired headphones, Bluetooth headset, or HDMI output.\nA GUI for making a single application louder than others, or muting it, and remembering this decision when the application will appear next time.\nA GUI for routing audio to a remote device available in LAN. For example, connecting a browser playing music on a laptop to speakers attached to a Raspberry Pi.\nAutomatically routing music or voice from a Bluetooth player or mobile phone to a sound card or Bluetooth speakers or headset.\nTransparently adding various sound processing tools to a running application, for example adding acoustic echo cancellation to a VoIP client.\nReducing CPU and battery usage by automatically adjusting latency on the fly to a maximum value acceptable for currently running applications, and by disabling currently unnecessary sound processing like resampling.\nSmart I/O scheduling, which may combine a high latency for playback (to avoid glitches and reduce CPU usage) and a low latency for user actions like volume changes (to provide smoother user experience).\nAutomatically integrating existing desktop applications into PulseAudio workflow, even if they are not aware of PulseAudio.\nProblems and drawbacks There are several known disadvantages of using PulseAudio, including both fundamental issues, and implementation issues that may be resolved in the future:\nadditional complexity, overhead, and bugs (more code always means more bugs) lack of comprehensive documentation non-intuitive command line tools and configuration weird features like autospawn and built-in watchdog higher minimum possible latency poor quality of service over an unreliable network like 802.11 (WiFi) no hardware mixing and resampling no hardware volumes when using ALSA UCM High-level components The diagram below demonstrates a simplified view of an example PulseAudio setup.\nIt shows three clients (employing three different APIs), one local PulseAudio server, two remote PulseAudio servers (connected via “native” and RTP protocols), one remote RTP receiver, ALSA backend, and a set of modules required to serve this setup.\nDiagram of Components The diagram shows most important PulseAudio components:\nlibpulse-simple\nClient library.\nProvides “Simple API” for applications. Implemented as a wrapper around libpulse.\nlibpulse\nClient and server library.\nProvides “Asynchronous API” for applications. Communicates with the server via the “native” protocol over a Unix domain or TCP stream socket.\nContains only definitions and code that are part of public API. The server also reuses definitions and some code from this library internally.\nlibpulsecommon\nClient and server library.\nContains parts from libpulsecore which are needed on both client and server but can’t be included into libpulse because they are not part of public API. For technical reasons, it also contains parts of libpulse.\nlibpulsecore\nServer library.\nProvides internal API for modules. Contains common environment and generic building blocks for modules.\nmodules\nServer extensions.\nMany server features are implemented in modules, including network protocols, device drivers, desktop integrations, etc.\nKey abstractions This sections discusses the key server-side object types.\nDevices and streams PulseAudio is built around devices (sources and sinks) connected to streams (source outputs and sink inputs). The diagram below illustrates these connections.\nSource\nA source is an input device. It is an active unit that produces samples.\nSource usually runs a thread with its own event loop, generates sample chunks, and posts them to all connected source outputs. It also implements clocking and maintains latency. The rest of the world usually communicates with a source using messages.\nThe typical source represents an input sound device, e.g. a microphone connected to a sound card line input or on a Bluetooth headset. PulseAudio automatically creates a source for every detected input device.\nSource output\nA source output is a recording stream. It is a passive unit that is connected to a source and consumes samples from it.\nThe source thread invokes source output when next sample chunk is available or parameters are updated. If the source and source output use different audio formats, source output automatically converts sample format, sample rate, and channel map.\nThe typical source output represents a recording stream opened by an application. PulseAudio automatically creates a source output for every opened recording stream.\nSink\nA sink is an output device. It is an active unit that consumes samples.\nSink usually runs a thread with its own event loop, peeks sample chunks from connected sink inputs, and mixes them. It also implements clocking and maintains latency. The rest of the world usually communicates with a sink using messages.\nThe typical sink represents an output sound device, e.g. headphones connected to a sound card line output or on a Bluetooth headset. PulseAudio automatically creates a sink for every detected output device.\nSink input\nA sink input is a playback stream. It is a passive unit that is connected to a sink and produces samples for it.\nThe sink thread invokes sink input when next sample chunk is needed or parameters are updated. If sink and sink input use different audio formats, sink input automatically converts sample format, sample rate, and channel map.\nThe typical sink input represents a playback stream opened by an application. PulseAudio automatically creates a sink input for every opened playback stream.\nObject hierarchy The diagram below shows the hierarchy of the server-side objects.\nCore\nThe core provides a shared environment for modules. Modules use it to find and register objects, install hooks for various events, register event loop handlers, etc.\nThere is only one core instance which is created at startup.\nModule\nA module represents a loadable server extension.\nModules usually implement and register other objects. A module can be loaded multiple times with different parameters and so have multiple instances.\nThe typical module implements a network or device discovery, a network or hardware device or stream, or a sound processing device. For example, PulseAudio loads a new instance of the module-alsa-card for every detected ALSA card.\nClient\nA client represents an application connected to the PulseAudio server.\nIt contains lists of playback and recording streams.\nThe typical client represents a local application, e.g. a media player, or a remote PulseAudio server. PulseAudio automatically creates a client for every incoming connection.\nCard\nA card represents a physical audio device, like a sound card or Bluetooth device.\nIt contains card profiles, device ports, and devices (sources and sinks) connected to device ports. It also has a single active card profile.\nThe typical card represents ALSA card or Bluetooth device. PulseAudio automatically creates a card for every detected physical device.\nCard profile\nA card profile represents an opaque configuration set for a card.\nIt defines the backend-specific configuration of the card, and the list of currently available devices (sources and sinks) and device ports. Only one card profile of a card may be active at the same time. The user can switch the active card profile at any time.\nThe typical card profile represents the sound card mode, e.g. analog and digital output, and mono, stereo, and surround mode. PulseAudio automatically creates a card profile for every available operation mode of a card.\nDevice port\nA device port represents a single input or output port on the card.\nA single card may have multiple device ports. Different device ports of a single card may be used simultaneously via different devices (sources and sinks).\nThe typical device port represents a physical port on a card, or a combination of a physical port plus its logical parameters, e.g. one output port for internal laptop speakers, and another output port for headphones connected via a line out. PulseAudio automatically creates device ports for every detected card, depending on the currently active card profile.\nDevice\nA device represents an active sample producer (input device) or consumer (output device).\nA device can have an arbitrary number of streams connected to it. A recording stream (source output) can be connected to an input device (source). A playback stream (sink input) can be connected to an output device (sink).\nThere are three kinds of devices:\nHardware device\nHardware source or sink is associated with an audio device. Usually, it is explicitly associated with a card object, except those limited backends that don’t create card objects. Such sources and sinks contain a subset of device ports provided by the device and have a single active device port, from which they will read or write samples. The user can switch the active device port of a source or sink at any time. PulseAudio automatically creates one or several pairs of a hardware source and sink for every detected card, depending on the currently active card profile. Virtual device\nVirtual source or sink is not associated with an audio device. It may represent a remote network device, a sound processing filter, or anything else, depending on the implementation. PulseAudio may automatically create a pair of virtual source and sink for every remote sound card exported by every PulseAudio server in the local network. Monitor device\nSink monitor is a special kind of virtual source associated with a sink. Every sink automatically gets a sink monitor, named as “\u0026lt;sink\\_name\u0026gt;.monitor”. Every time when the sink reads a chunk from its sink inputs, it also writes this chunk to the sink monitor. Typical usage of the sink monitor is capturing all sound that was sent to speakers and duplicating it somewhere else. PulseAudio automatically creates a sink monitor for every sink. Stream\nA stream represents a passive sample consumer (recording stream) or producer (playback stream).\nEvery stream should be connected to some device. A recording stream (source output) should be connected to an input device (source). A playback stream (sink input) should be connected to an output device (sink).\nThere are two kinds of streams:\nApplication stream\nAn application stream is associated with a client. It is created when an application connected to PulseAudio server starts playback or recording. Virtual stream\nA virtual stream is not associated with a client. It may represent a remote network server or anything else, depending on the implementation. Sample cache\nThe sample cache is an in-memory storage for short named batches of samples that may be uploaded to the server once and then played multiple times. It is usually used for event sounds.\nD-Bus API component\nlibpulsecore\nmodule-dbus-protocol\nPulseAudio server-side objects may be inspected and controlled via an experimental D-Bus API. Note that it can’t be used for playback and recording. These features are available only through the C API.\nUnfortunately, the D-Bus API has never left the experimental stage, and it has no stability guarantees and is not actively maintained. Applications are generally advised to use the C API instead.\nBuses and services D-Bus has several modes of communication:\nvia a system bus (system-wide) via a session bus (one bus per login session) peer-to-peer (direct communication between applications) PulseAudio implements several D-Bus services:\nDevice reservation API (on session bus) Server lookup API (on session bus) Server API (peer-to-peer) Server API extensions (peer-to-peer) Device reservation API Device reservation API provides methods for coordinating access to audio devices, typically ALSA or OSS devices. It is used to ensure that nobody else is using the device at the same time.\nIf an application needs to use a device directly (bypassing PulseAudio), it should first acquire exclusive access to the device. When access is acquired, the application may use the device until it receives a signal indicating that exclusive access has been revoked.\nThis API is designed to be generic. It is a small standalone D-Bus interface with no dependencies on PulseAudio abstractions, so it may be easily implemented by other software.\nServer lookup API PulseAudio server API uses peer-to-peer D-Bus mode. In this mode, clients communicate directly with the server instead of using a session bus, which acts as a proxy. In contrast to the session bus mode, this mode permits remote access and has lower latency. However, clients need a way to determine the server address before connecting to it.\nTo solve this problem, PulseAudio server registers server lookup interface on the session bus. A client should first connect to the session bus in order to discover PulseAudio server address and then connect to PulseAudio server directly for peer-to-peer communication.\nServer API Server API is available through a peer-to-peer connection to PulseAudio server.\nEvery object in the hierarchy is identified by a unique path. The hierarchy starts with the core object, which has a well-known path. It may be used to discover all other objects.\nThe diagram below shows the most important D-Bus interfaces.\nCore - A top-level interface that provides access to all other interfaces.\nModule - A loadable server extension.\nClient - An application connected to the PulseAudio server.\nCard - A physical audio device, like a sound card or Bluetooth device.\nCardProfile - An opaque configuration set for a card.\nDevicePort - A single input or output port on the card.\nDevice - The parent interface for Source and Sink.\nSource - An input device. May be associated with a card and device port.\nSink - An output device. May be associated with a card and device port.\nStream - May be either a recording stream (source output) or playback stream (sink input). Application stream is associated with a client.\nSample - A named batch of samples in the sample cache.\nIn addition to the core interface, PulseAudio modules can register custom server API extensions, that are also discoverable through the core. Several extensions are available out of the box:\nStreamRestore\ncomponent\nmodule-stream-restore\nQuery and modify the database used to store device and stream parameters.\nEqualizer\ncomponent\nmodule-equalizer-sink\nQuery and modify equalizer sink levers.\nLadspa\ncomponent\nmodule-ladspa-sink\nQuery and modify LADSPA sink control ports.\nC API PulseAudio provides C API for client applications.\nThe API is implemented in the libpulse and libpulse-simple libraries, which communicate with the server via the “native” protocol. There are also official bindings for Vala and third-party bindings for other languages.\nC API is a superset of the D-Bus API. It’s mainly asynchronous, so it’s more complex and harder to use. In addition to inspecting and controlling the server, it supports recording and playback.\nThe API is divided into two alternative parts:\nAsynchronous API (libpulse), complicated but complete Simple API (libpulse-simple), a simplified synchronous wrapper for the recording and playback subset of the asynchronous API Asynchronous API component\nlibpulse\nAsynchronous API is based on event loop and callbacks.\nThe diagram below demonstrates the workflow.\nMain Loop\nThe first step is creating an instance of one of the available Main Loop implementations. They differ in a way how the user can run the main loop: directly, in a separate thread, or using Glib.\nAll communications with the server happen inside the main loop. The user should run main loop iterations from time to time. The user can either register callbacks that are invoked from the main loop or use polling.\nFor a regular main loop, polling may be performed between iterations. For a threaded main loop, polling may be performed after obtaining a lock from another thread.\nContext\nThe second step is creating a Context object that represents a connection to the server. The user can set callbacks for context state updates.\nStream\nWhen context state becomes ready, the user can create one or multiple Stream objects for playback or recording.\nThe user can set callbacks for stream state updates and I/O events, invoked when the server wants to send or receive more samples. Clocking is controlled by the server. Stream also provides several management functions, like pause, resume, and volume control.\nSample Cache\nIn addition to regular streams, the user can also use Sample Cache to upload named batches of samples to the server without playing them and start playback later.\nIntrospection\nHaving a context in a ready state, Server Query and Control API may be used to query and modify various objects on the server. All operations are asynchronous. The object hierarchy is similar to D-Bus API described above.\nProperty Lists\nEvery server-side object has a Property List, a map with textual keys and arbitrary textual or binary values. Applications and modules may get and set these properties. Various modules implement automatic actions based on some properties, like routing, volume setup, and autoloading filters.\nThe typical usage in applications is to provide a property list when creating a context (for client properties) and when creating a stream (for stream properties). Higher-level frameworks that use PulseAudio (like GStreamer) usually do it automatically.\nOperations\nAll operations with server-side objects are asynchronous. Many API calls return an Operation object which represents an asynchronous request. It may be used to poll the request status, set completion callback, or cancel the request.\nEvents\nThe client can receive two types of events from server:\nsubscription events\n[Events API](https://freedesktop.org/software/pulseaudio/doxygen/subscribe.html) provides methods for subscribing events triggered for the server-side objects available through the introspection API. Every event has an integer type and arbitrary binary payload. stream events\n[Stream Events](https://freedesktop.org/software/pulseaudio/doxygen/stream_8h.html#a5690ed098466233860e632abfa61fe50) are generated to acknowledge the client of the stream state change or ask it to do something, e.g. pause the stream. Such event has a textual name and arbitrary binary payload. Simple API component\nlibpulse-simple\nSimple API is a convenient wrapper around the threaded main loop and stream. The user just chooses parameters, connects to the server and writes or reads samples. All operations are blocking.\nLimitations:\nonly single stream per connection is supported no support for volume control, channel mappings, and events Protocols and networking PulseAudio server supports a variety of network protocols to communicate with clients, remote servers, and third-party software. See Network page on wiki.\nPulseAudio implements two custom protocols:\n“native”, a full-featured protocol for most client-server and server-server communications “simple”, which is rarely useful It also supports several foreign transport and discovery protocols:\nmDNS (Zeroconf) RTP/SDP/SAP RAOP HTTP DLNA and Chromecast ESound And two control protocols:\nD-Bus API CLI protocol Native protocol component\nlibpulsecore\nmodule-native-protocol-{fd,unix,tcp}\nPulseAudio uses a so-called “native” protocol for client-server and server-server connections, which works over a Unix domain or TCP stream socket. It is a rich, binary, message-oriented, bidirectional, asynchronous protocol.\nThe Asynchronous API described above mostly mirrors the features provided by this protocol.\nThey are:\nauthentication - provide authentication data for the server streams - manage server-side stream state and exchange samples sample cache - manage server-side sample storage introspection - query and modify server-side objects events - subscribe server-side object events extensions - send custom commands to modules There are four message types, each with a header and an optional payload:\npacket\nControl message. May contain a command (from client to server), a reply (from server to client), or an event (from server to client). Each message has its own payload type.\nmemblock\nData message. Contains a chunk of samples. In the zero-copy mode payload with samples is omitted and the message contains only a header.\nshmrelease, shmrevoke\nShared pool management message. These messages are used to manage the shared memory pool employed in the zero-copy mode.\nWith the “native” protocol, the client is clocked by the server. Server requests client to send some amount of samples from time to time.\nSince the protocol uses stream sockets, it’s not real time. The delays introduced by the sender or network cause playback delays on the receiver.\nZero-copy mode When the “native” protocol is used for client and server on the same host, the zero-copy mode may be employed.\nIt requires Unix domain socket to be used, and POSIX shared memory or Linux-specific memfd to be supported and enabled in PulseAudio. It also requires the server and client to run as the same user, for security reasons.\nIn this mode, chunks are allocated in a shared memory pool and communication is done through a shared ring buffer channel that uses a shared memory block for data and two file descriptor-based semaphores for notifications, on top of POSIX pipe or Linux-specific eventfd.\nTo establish communication, the server should send to the client file descriptors of the shared memory region and semaphores. The algorithm is the following:\nthe server creates an anonymous in-memory file using shm_open (POSIX) or memfd_create (Linux-specific) the server maps the file to memory using mmap and initializes a memory pool there the server allocates one block from the pool, initializes the shared ring buffer, and creates semaphores using pipe (POSIX) of eventfd (Linux-specific) the server transfers file descriptors to the client via a Unix domain socket using sendmsg, which provides special API for this feature the client receives file descriptors and also maps the file to memory using mmap the server and client now use single shared memory pool and ring buffer to avoid races, the server and client use mutexes that are placed inside shared memory as well After this, all messages (packet, memblock, shmrelease, shmrevoke) are sent via the shared ring buffer.\nWhen the memblock message is sent in the zero-copy mode, it’s payload is omitted. Since the server and client use the same shared memory pool, the chunk payload can be obtained from the memory pool using the chunk identifier in the chunk header and is not needed to be transmitted.\nTwo additional messages are used in this mode:\nthe peer that received a chunk sends shmrelease when it finishes reading the chunk and wants to return it to the shared memory pool the peer that has sent a chunk may send shmrevoke when it wants to cancel reading from the chunk To achieve true zero-copy when playing samples, an application should use the Asynchronous API and delegate memory allocation to the library. When the zero-copy mode is enabled, memory is automatically allocated from the shared pool.\nAuthentication When a client connects to the server via the “native” protocol, the server performs several authentication checks in the following order:\nauth-anonymous\nIf the auth-anonymous option is set, then the client is accepted.\nuid\nIf a Unix domain socket is used, and the client has the same UID as the server, then the client is accepted. This check uses a feature of Unix domain sockets that provide a way to securely determine credentials of the other side.\nauth-group\nIf a Unix domain socket is used, and the auth-group option is set, and the client belongs to the group specified by this option, then the client is accepted.\nauth-cookie\nIf the auth-cookie option is set, and the client provided a correct authentication cookie, then the client is accepted.\nOn start, the server checks a cookie file, usually located at \u0026quot;~/.config/pulse/cookie\u0026quot;. If the file isn’t writable, the server reads a cookie from it. Otherwise, it generates a new random cookie and writes it to the file. Optionally, the server also stores the cookie into the X11 root window properties.\nClient searches for a cookie in an environment variable, in the X11 root window properties, in parameters provided by the application, and in the cookie file (in this order).\nauth-ip-acl\nIf TCP socket is used, and the auth-ip-acl option is set, and client’s IP address belongs to the address whitelist specified in this option, then the client is accepted.\nreject\nIf all checks have failed, then the client is rejected.\nTunnels component\nmodule-tunnel-{source,sink}\nmodule-tunnel-{source,sink}-new\nLocal applications may be connected with to audio devices using tunnel sources and sinks. The diagram below illustrates an example of such connections.\nEach tunnel connects a single pair of a local device and remote stream:\na local tunnel sink is connected to a remote sink input a local tunnel source is connected to a remote source output Each tunnel acts as a regular PulseAudio client and connects to a remote PulseAudio server via the “native” protocol over TCP. Tunnel sink creates a playback stream, and tunnel source creates a recording stream.\nTunnel devices may be created either manually by the user or automatically if the Zeroconf support is enabled.\nmDNS (Zeroconf) component\nmodule-zeroconf-{publish,discover}\nmodule-bonjour-publish\nmDNS (multicast DNS) protocol, a part of the Zeroconf protocol stack, resolves names in the local network without using a name server.\nPulseAudio may use Avahi (free Zeroconf implementation) or Bonjour (Apple Zeroconf implementation). If Avahi or Bonjour daemon is running and the Zeroconf support is enabled in PulseAudio, every sink and source on every PulseAudio server in the local network automatically become available on all other PulseAudio servers.\nTo achieve this, PulseAudio uses automatically configured tunnels:\npublishing\nPulseAudio server publishes every sink and source as an mDNS service. Each published entry contains the server address, device name and type, and audio parameters, like sample rate and channel map.\nPublishing is implemented for both Avahi (module-zeroconf-publish) and Bonjour (module-bonjour-publish).\ndiscovery\nPulseAudio server monitors services published on the local network. For every detected service, PulseAudio server creates a tunnel sink or source connected to the remote device and configured with the parameters of that device.\nDiscovery is implemented only for Avahi (module-zeroconf-discover).\nRTP/SDP/SAP component\nmodule-rtp-{send,recv}\nPulseAudio also has the RTP support. Unlike the “native” PulseAudio tunnels, this technology supports multicasting of a single local source to any number of remote sinks.\nTo achieve this, three protocols are used:\nRTP (Real-time Transport Protocol)\nA transport protocol for delivering audio and video over IP networks.\nSDP (Session Description Protocol)\nA format for describing multimedia session parameters. Usually used to describe RTP sessions.\nSAP (Session Announcement Protocol)\nA protocol for broadcasting multicast session information. Usually used to send SDP messages.\nPulseAudio implements both RTP sender and receiver. They may be used together, or with other software that supports RTP, for example VLC, GStreamer, FFMpeg, MPLayer, or SoX. See RTP page on wiki.\nThe diagram below shows an example workflow.\nRTP sender\nRTP sender creates an RTP source output.\nEvery RTP source output is connected to a single source and configured to send RTP packets to a single network address, usually a multicast one.\nWhen the RTP source output is created, it broadcasts RTP session parameters to the local network using SDP/SAP. When source writes samples to RTP source output, source output sends them to preconfigured address via RTP. When RTP source output is destroyed, it broadcasts goodbye message using SDP/SAP.\nRTP receiver\nRTP receiver listens to SDP/SAP announcements in the local network.\nWhen it receives an announcement for a new RTP session, it creates RTP sink input for it. When it receives goodbye message, it destroys the appropriate RTP sink input.\nEvery RTP sink input is connected to single sink and is configured to receive RTP packets from single RTP sender.\nWhen RTP sink input receives an RTP packet, it stores it in the queue. When sink requests samples from RTP sink input, RTP sink input reads samples from that queue.\nRTP sender can’t be clocked by RTP receiver because the sender has no feedback from the receiver and there may be multiple receivers for a single multicast sender. Since sender and receiver clocks are always slightly different, the receiver queue size is slowly drifting. To avoid this, RTP receiver adjusts resampler rate on the fly so that samples are played a bit slower or faster depending on the queue size.\nRTP is a real time protocol. The delays introduced by the sender or network cause playback holes on the receiver. Playback is never delayed and packets delivered too late are just dropped.\nRAOP (AirPlay) component\nmodule-raop-{discover,sink}\nRAOP (Remote Audio Output Protocol) is a proprietary streaming protocol based on RTP and RTSP, used in Apple AirPlay devices. RTP is a transport protocol, and RTSP is a control protocol.\nAirPlay devices use mDNS and are discoverable via Zeroconf. AirPlay uses AES encryption, but the RSA keys were extracted from Apple devices, and open-source RAOP implementations appeared.\nSince version 11.0, PulseAudio has built-in support for RAOP2. PulseAudio uses Avahi to receive mDNS RAOP announcements. Every AirPlay device in the local network automatically becomes available in PulseAudio.\nRAOP support consists of two parts:\ndiscovery\nPulseAudio server monitors services published on the local network. For every detected service, PulseAudio server creates an RAOP sink.\nsink\nEvery RAOP sink is connected to a single AirPlay device. It uses RTSP to negotiate session parameters and RTP to transmit samples.\nHTTP support component\nlibpulsecore\nmodule-http-protocol-{unix,tcp}\nThe HTTP support provides two features:\nweb interface\nA simple web interface provides a few bits of information about the server and server-side objects.\nstreaming\nIt’s possible to receive samples from sources and sink monitors via HTTP. This feature is used for DLNA support.\nEvery source or sink monitor has a dedicated HTTP endpoint.\nWhen a new HTTP client connects to the endpoint, PulseAudio first sends the standard HTTP headers, including the “Content-Type” header with the MIME type corresponding to the sample format in use.\nAfter sending headers, PulseAudio creates a new source output connected to the source or sink monitor which writes all new samples to the HTTP connection. Samples are sent as-is, without any additional encoding.\nDLNA and Chromecast component\nmodule-rygel-media-server\npulseaudio-dlna\nDLNA (Digital Living Network Alliance) is a set of interoperability guidelines for sharing digital media among multimedia devices. It employs numerous control and transport protocols, including UPnP, RTP, and custom HTTP APIs.\nChromecast is a line of digital media players developed by Google. It uses Google Cast, a proprietary protocol stack, based on Google Protocol Buffers and mDNS.\nThere are two implementations of DLNA and/or Chromecast support:\nmodule-rygel-media-server\nPulseAudio can become a DLNA media server so that other DLNA devices can discover and read PulseAudio sources. This feature is implemented using Rygel, a DLNA media server. See details here.\nPulseAudio registers a Rygel plugin, which exports PulseAudio sources and sink monitors via D-Bus. Every exported source or sink monitor includes an HTTP URL that should be used to read samples from PulseAudio.\nFor its part, Rygel publishes exported sources and sink monitors via UPnP, and ultimately DLNA clients may see them and read PulseAudio streams via HTTP.\npulseaudio-dlna\nA third-party pulseaudio-dlna project allows PulseAudio to discover and send audio to DLNA media renderers and Chromecast devices. Devices published in the local network automatically appear as new PulseAudio sinks.\nThis project is implemented as a standalone daemon written in Python. The daemon creates a null sink for every discovered remote device, opens the sink monitor associated with it, reads samples from the monitor, performs necessary encoding, and sends samples to the remote device.\nThe communication with the PulseAudio server is done via the D-Bus API (to query and configure server objects) and parec tool (to receive samples from a sink monitor).\nESound component\nlibpulsecore\nmodule-esound-protocol-{unix,tcp}\nPulseAudio server may be accessed via the protocol used in Enlightened Sound Daemon.\nThe documentation says that it supports playback, recording, and control commands, so switching to PulseAudio should be transparent for applications that are using ESound.\nSimple component\nlibpulsecore\nmodule-simple-protocol-{unix,tcp}\nThe “simple” protocol is used to send or receive raw PCM samples from the PulseAudio server without any headers or meta-information.\nThe user should configure the sample format and the source and sink to use. Then the user may use tools like netcat to send PCM samples over a Unix domain or TCP socket.\nCLI component\nlibpulsecore\nmodule-cli-protocol-{unix,tcp}\nmodule-cli\nPulseAudio server implements its own CLI protocol.\nIt is a simple text protocol that provides various commands to inspect and control the server:\nstatus commands - list and inspect server-side objects module management - load, unload, and inspect modules moving streams - move streams to devices killing clients and streams - remove clients and streams volume commands - setup volumes of devices and streams configuration commands - setup parameters of devices and device ports property lists - setup property lists of devices and streams sample cache - add, remove, or play samples in the server-side sample cache log and debug commands - configure logging, dump server configuration, etc meta commands - include and conditional directives The same syntax may be used in several places:\nin PulseAudio configuration files over a Unix domain or TCP socket (module-cli-protocol-{unix,tcp}) over the controlling TTY of the server (module-cli) The TTY version requires that the server should be started in foreground mode in a terminal. The pacmd tool uses the CLI protocol over a Unix domain socket.\nDevice drivers PulseAudio has several backends that implement audio I/O and device management. The diagram below illustrates backend-specific components.\ncard\nA card represents a physical audio device, like a sound card or Bluetooth device. It contains card profiles, device ports, and devices. It has a single active card profile.\ncard profile\nA card profile represents an opaque configuration set of a card, like an analog or digital mode. It defines the backend-specific configuration of the card, and the list of currently available device ports and devices.\ndevice port\nA device port represents a single input or output port on the card, like internal speakers or external line-out. Multiple device ports may belong to a card.\ndevice\nA device (source or sink) represents an active sample producer or consumer. A device is usually associated with a card and a set of device ports. It has a single active device port. Multiple devices may belong to a card.\nEvery backend should implement the following features of a source or sink:\nreading or writing samples to the device maintaining latency providing clocking in the device time domain Currently, the only full-featured backends are ALSA and Bluetooth, which implement all object types listed above. Other backends provide only sources and sinks.\nALSA backend component\nmodule-udev-detect\nmodule-alsa-{card,source,sink}\nALSA (Advanced Linux Sound Architecture) is a Linux kernel component providing device drivers for sound cards, and a user space library (libasound) interacting with the kernel drivers. It provides a rich high-level API and hides hardware-specific stuff.\nALSA backend in PulseAudio automatically creates PulseAudio cards, card profiles, device ports, sources, and sinks:\nPulseAudio card is associated with an ALSA card.\nPulseAudio card profile is associated with a configuration set for an ALSA card. It defines a subset of ALSA devices belonging to a card, and so the list of available device ports, sources, and sinks.\nPulseAudio device port is associated with a configuration set for an ALSA device. It defines a list of active inputs and outputs of the card and other device options.\nPulseAudio source and sink are associated with an ALSA device. When a source or sink is connected to a specific device port, they together define an ALSA device and its configuration.\nThe concrete meaning of PulseAudio card profile and device ports depends on whether the ALSA UCM is available for an ALSA card or not (see below).\nPulseAudio sources and sinks for ALSA devices implement a timer-based scheduler that manages latency and clocking. It is discussed later in a separate section.\nALSA device hierarchy ALSA uses a device hierarchy that is different from the PulseAudio hierarchy. See this post for an overview.\nThe ALSA hierarchy has three levels:\ncard\nALSA card represents a hardware or virtual sound card. Hardware cards are backed by kernel drivers, while virtual cards are implemented completely in user space plugins.\ndevice\nALSA card contains at least one playback or capture device. A device is something that is capable of processing single playback or recording stream. All devices or a card may be used independently and in parallel. Typically, every card has at least one playback device and one capture device.\nsubdevice\nALSA device contains at least one subdevice. All subdevices of the same device share the same playback or recording stream. For playback devices, subdevices are used to represent available slots for hardware mixing. Typically, there is no hardware mixing, and every device has a single subdevice.\nALSA device is identified by a card number and device number. PulseAudio by default interacts only with hardware ALSA devices. PulseAudio currently doesn’t use hardware mixing and so don’t employ multiple subdevices even if they’re available.\nALSA kernel interfaces Each ALSA device has a corresponding device entry in the \u0026quot;/dev/snd\u0026quot; directory. Their meta-information may be discovered through the \u0026quot;/proc/asound\u0026quot; directory. See some details in this post.\nThe diagram below shows the device entries involved when PulseAudio is running.\nFive device entry types exist:\npcm - for recording or playing samples\ncontrol - for manipulating the internal mixer and routing of the card\nmidi - for controlling the MIDI port of the card, if any\nsequencer - for controlling the built-in sound synthesizer of the card, if any\ntimer - to be used in pair with the sequencer\nPulseAudio interacts only with the pcm and control device entries.\nEvery ALSA card and device may have a set of kcontrols (kenrnel control elements) associated with it. The kernel provides generic operations for manipulating registered kcontrols using ioctl on the corresponding device entry.\nA kcontrol has the following properties:\nname - a string identifier of the kcontrol\nindex - a numerical identifier of the kcontrol\ninterface - determines what entity this kcontrol is associated with, e.g. “card” (for card kcontrols), “pcm” (for pcm device kcontrols) or “mixer” (for control device kcontrols)\ntype - determines the type of the kcontrol members, e.g. “boolean”, “integer”, “enumerated”, etc.\nmembers - contain the kcontrol values; a kcontrol can have multiple members, but all of the same type\naccess attributes - determine various kcontrol access parameters\nA kcontrol typically represent a thing like a volume control (allows to adjust the sound card internal mixer volume), a mute switch (allows to mute or unmute device or channel), a jack control (allows to determine whether something is plugged in, e.g. into an HDMI or a 3.5mm analog connector), or some device-specific option.\nThe concrete set of the available kcontrols are defined by the sound card driver, though drivers try to provide similar kcontrols. The driver usually just exposes all available hardware controls, and it’s up to the user space to provide a unified hardware-independent layer on top of them. This responsibility rests on the ALSA UCM and PulseAudio.\nALSA user space interfaces ALSA provides numerous user space interfaces to interact with ALSA cards and devices and their properties. See libasound documentation: 1, 2.\nThe diagram below provides an overview of the components involved when PulseAudio is running.\nHere is the list of involved ALSA interfaces:\nPCM\nPCM interface implements methods for playback and recording on top of the pcm device entry.\nApplications can setup the per-device kernel-side ring buffer parameters, write or read samples to the buffer, and issue flow control operations.\nThis interface is used in most ALSA applications.\nCTL\nCTL interface implements low-level methods for accessing kcontrols on top of the kernel ioctl API.\nApplications can inspect, read, and write CTL elements, which are mapped one-to-one to the kernel-side kcontrols.\nThis interface is usually not used directly.\nHCTL\nHCTL interface implements a caching layer on top of the CTL interface.\nApplications can inspect, read, and write HCTL elements, mapped one-to-one to the CTL elements, and in addition, can set per-element callbacks for various events.\nThis interface is usually not used directly.\nMixer\nMixer interface implements a higher-level management layer above the HCTL interface.\nIt provides a framework for managing abstract mixer elements. A mixer element is, generally speaking, a set of logically grouped HCTL elements. Applications register custom mixer element classes and implement a custom mapping of the HCTL elements to the mixer elements.\nThis interface provides a generic and rather complex asynchronous API. In most cases, applications may use the Simple Mixer interface instead.\nSimple Mixer\nSimple Mixer interface implements a mixer element class for the Mixer and provides a simple and less abstract synchronous API on top of it.\nA Simple Mixer element represents a logical group of the related kcontrols. An element may have the following attributes, mapped internally to the corresponding HCTL elements: a volume control, a mute switch, or an enumeration value. Each attribute may be playback, capture, or global. Each attribute may be also per-channel or apply to all channels.\nThis interface is supposed to be used by applications that want to control the mixer and do not need the complex and generic Mixer API.\nUCM\nUCM (Use Case Manager) interface implements high-level configuration presets on top of the Mixer interface.\nApplications can describe their use-cases by selecting one of the available presets instead of configuring mixer elements manually. The UCM then performs all necessary configuration automatically, hiding machine-specific details and complexity of the Mixer interface.\nALSA routing ALSA cards often have multiple inputs and outputs. For example, a card may have an analog output for internal speaker, an analog output for 3.5mm headphone connector, an analog input for internal microphone, an analog input for 3.5mm microphone connector, and an HDMI input and output.\nALSA interfaces represent this in the following way:\nan ALSA card contains separate ALSA devices for HDMI and analog audio\nan ALSA device has separate pcm device entries in \u0026quot;/dev/snd\u0026quot; for playback and capture\nan ALSA card has a control device entry in \u0026quot;/dev/snd\u0026quot; with various kcontrols allowing to determine what’s plugged in and configure input and output routing\nThe following kcontrols may be used:\njack controls\nJack controls may be used to determine what’s plugged in. Drivers usually create jack controls for physical connectors, however, details may vary.\nFor example, headphone and microphone connectors may be represented with a single jack control or two separate jack controls. An internal speaker or microphone can be sometimes represented with a jack control as well, despite the fact that there is no corresponding physical connector.\nmute controls\nMute controls may be used to mute and unmute a single input or output. Drivers usually create mute controls for every available input and output of a card.\nroute controls\nAn enumeration control may be sometimes provided to choose the active input or output.\nDifferent drivers provide different sets of kcontrols, and it’s up to the user space to build a unified hardware-independent layer on top of them. PulseAudio is able to do it by itself, or alternatively may rely on ALSA UCM.\nIn both cases, PulseAudio goal is to probe what inputs and outputs are available and map them to device ports somehow. However, details vary depending on whether UCM is in use or not.\nALSA cards with UCM ALSA Use Case Manager aims two goals:\nAbstract the applications which configure ALSA devices from the complexity of the Mixer interface.\nMake these applications portable across numerous embedded and mobile devices, by moving the machine-specific part to configuration files.\nUCM lets applications to operate with such high-level operations as “setup this device to play HiFi music via an external line-out” or “setup that device to capture voice for a phone call via an internal microphone”.\nUCM then looks into the local configuration files and maps such use-case description to the concrete values of mixer elements. These files may be part of the UCM package or may be provided by a device vendor.\nAn application provides the UCM with three strings:\nucm verb\nDefines the main operation mode of an ALSA device, e.g. “HiFi” or “Voice”. Only one UCM verb may be active at the same time.\nucm modifier\nDefines a supplementary operation mode of an ALSA device, e.g. “PlayMusic” or “PlayTone”. Available UCM modifiers are defined by currently active UCM verb. Zero or multiple UCM modifiers may be active at the same time.\nucm device\nDefines configuration of active inputs and outputs of an ALSA device, e.g. “Speaker” or “Headset”. Available UCM devices are defined by currently active UCM verb. Zero or multiple UCM devices may be active at the same time.\nA combination of one UCM verb, zero or multiple UCM modifiers, and one or multiple UCM devices define what ALSA device to use and how to configure its mixer elements.\nWhen UCM is available for a card, PulseAudio automatically employs it.\nThe diagram below illustrates relations between PulseAudio and ALSA objects when the UCM is active. Some diagrams and details are also available on Linaro Wiki: 1, 2, 3.\nThe mapping of the PulseAudio object hierarchy to the ALSA object hierarchy is the following:\nPulseAudio card is associated with the UCM interface of an ALSA card. One PulseAudio card is created for every ALSA card.\nPulseAudio card profile is associated with a UCM verb. For every card, one PulseAudio profile is created for every UCM verb available for the card.\nPulseAudio source is associated with the PCM interface of a capture ALSA device. For every card, one PulseAudio source is created for every available capture ALSA device that is associated with a UCM verb, UCM modifier, or UCM device available in the currently active card profile.\nPulseAudio sink is associated with the PCM interface of a playback ALSA device. For every card, one PulseAudio sink is created for every playback ALSA device that is associated with a UCM verb, UCM modifier, or UCM device available in the currently active card profile.\nPulseAudio device port is associated with a combination of a UCM modifier and UCM devices. For every source or sink, one PulseAudio device port is created for every possible valid combination of zero or one UCM modifier and one or multiple UCM devices.\nA valid combination includes:\nonly UCM modifiers and devices that are enabled by currently active card profile\nonly UCM modifiers and devices that are associated with the ALSA device of the source or sink\nonly non-mutually exclusive UCM devices\nEvery UCM modifier is mapped to a PulseAudio role. The UCM modifier of a device port is actually enabled only when there is at least one source output or sink input connected to the source or sink of the device port, which has a “media.role” property equal to the UCM modifier’s role.\nThis is how the mapping is used:\nThe card defines what ALSA card is used, and so what profiles are available.\nThe currently active card profile of the card defines what UCM verb is used, and so what sources and sinks are available.\nThe source or sink defines what ALSA device is used, and so what device ports are available.\nThe currently active device port of the source or sink defines what UCM modifier and UCM devices are used. Whether the UCM modifier is enabled depends on the roles of currently connected source outputs or sinks inputs.\nThe currently active UCM verb, UCM modifier, and UCM devices define what card inputs and outputs are active, what device options are set, and what volume controls are used.\nALSA cards w/o UCM Besides the UCM support, PulseAudio has its own configuration system on top of the ALSA Mixer. It was developed before UCM appeared. It is used when the UCM is not available for a card.\nMixer configuration is described in custom PulseAudio-specific configuration files. See Profiles page on wiki.\nConfiguration files define the following objects:\nprofile set\nProvides available profiles for an ALSA card. Contains a list of profiles.\nPhysically it is a .conf file under the \u0026quot;/usr/share/pulseaudio/alsa-mixer/profile-sets\u0026quot; directory.\nprofile\nRepresents a configuration set for an ALSA card. Contains a list of mappings.\nEvery mapping defines a playback or capture ALSA device that is available when this profile is active, and the configuration sets available for each device.\nPhysically it is a [Profile] section in the profile file.\nmapping\nRepresents a playback or capture ALSA device. Contains:\ndevice strings\nDevice string is a pattern used to match an concrete ALSA device belonging to the ALSA card. First matched device is used. channel map\nChannel mapping defines what channels are used for the ALSA device. input/output paths\nMapping contains multiple input and output paths that represent alternative configuration sets for the ALSA device. Every input or output path defines a single configuration set, which provides an ALSA mixer path and settings for ALSA mixer elements accessible through that path. Physically it is a [Mapping] section in the profile file.\npath\nRepresents a configuration set for a single capture or playback ALSA device. Contains a list of elements, and a list of jacks.\nEvery element or jack defines an ALSA mixer element and how it should be used when the configuration set defined by this path is active.\nPhysically it is a .conf file under the \u0026quot;/usr/share/pulseaudio/alsa-mixer/paths\u0026quot; directory.\njack\nRepresents an ALSA mixer element for a jack that should be used for probing. Contains identifier of the ALSA mixer element and its expected state (plugged or unplugged).\nEvery jack is probed and its state is compared with the expected one. This probing is used to activate only those paths that are actually available.\nPhysically it is a [Jack] section in the path file.\nelement\nRepresents an ALSA mixer element and defines how it should be handled. Contains:\nelement id\nDefines the name of the ALSA mixer element. volume policy\nDefines how to handle the volume of the ALSA mixer element. It may be either ignored, unconditionally disabled, unconditionally set to a constant, or merged into the value of PulseAudio volume slider. switch value\nDefines how to handle the value of a switch ALSA mixer element. It may be either ignored, unconditionally set to a constant, used for muting and unmuting, or made selectable by the user via an option. enumeration value\nDefines how to handle the value of enumeration ALSA mixer element. It may be either ignored, or made selectable by the user via an option. options\nEvery option defines one alternative value of a switch or enumeration ALSA mixer element. This value is made selectable by the user. Physically it is an [Element] section in the path file.\noption\nRepresents one alternative value of a switch or enumeration ALSA mixer element. Contains identifier of the ALSA mixer element and its value.\nPhysically it is an [Option] section in the path file.\nWhen UCM is not available for a card, PulseAudio uses Udev rules to select an appropriate profile set for the card:\nPulseAudio installs Udev rules that match known audio card devices by vendor and product identifiers and set PULSE_PROFILE_SET property for them. The property contains a name of a profile set .conf file.\nWhen PulseAudio configures a new ALSA card that has no UCM support, it reads the PULSE_PROFILE_SET property set by Udev rules and loads the appropriate profile set file. The file defines how to create and configure card profiles, device ports, sources, and sinks.\nIf an ALSA card was not matched by Udev rules and the PULSE_PROFILE_SET property was not set, PulseAudio uses default profile set which contains some reasonable configuration for most cards.\nThe diagram below illustrates relations between PulseAudio and ALSA objects when UCM is not used.\nThe mapping of the PulseAudio object hierarchy to the ALSA object hierarchy is the following:\nPulseAudio card is associated with an ALSA card and a profile set defined in configuration files. One PulseAudio card is created for every ALSA card, and one profile set is selected for every card.\nPulseAudio card profile is associated with a profile defined in configuration files. For every card, one PulseAudio profile is created for every profile in the profile set of the card.\nPulseAudio source is associated with a mapping defined in configuration files, and with the PCM interface of the capture device matched by the device mask of the mapping. For every card, one PulseAudio source is created for every mapping in the currently active profile.\nPulseAudio sink is associated with a mapping defined in configuration files, and with the PCM interface of the playback device matched by the device mask of the mapping. For every card, one PulseAudio sink is created for every mapping in the currently active profile.\nPulseAudio device port is associated with a combination of a path and options defined in configuration files. For every source or sink, one PulseAudio device port is created for every possible combination of one path and a subset of all options of all elements of this path.\nThis is how the mapping is used:\nThe card defines what ALSA card is used and what profile set is used, and so what profiles are available.\nThe currently active card profile of the card defines what mappings are available, and so what sources and sinks are available.\nThe source or sink defines what ALSA device is used, and what mapping is used, and so what device ports are available.\nThe currently active device port of the source or sink defines what path is used, what jacks are probed, what elements are used for getting and setting volume and how, and what combination of options of elements of the path is used.\nThe currently active elements, their volume policies, and their options define how to configure ALSA mixer elements of the ALSA device.\nBluetooth backend component\nmodule-bluetooth-discover\nmodule-bluez5-{discover,device}\nmodule-bluez4-{discover,device}\nPulseAudio supports Bluetooth, a wireless protocol stack for exchanging data over short distances. See Bluetooth page for details. PulseAudio relies on two backends for Bluetooth support:\nBlueZ (PulseAudio supports versions 4 and 5, but we discuss only version 5) oFono (for HFP support) Bluetooth specification defines numerous Bluetooth profiles which may be supported by a device. Each profile describes the protocols, codecs, and device roles to be used. A Bluetooth device may support a subset of defined profiles and roles. Note that Bluetooth profiles and roles are different from the PulseAudio card profiles and stream roles.\nPulseAudio supports three Bluetooth profiles:\nA2DP (Advanced Audio Distribution Profile)\nProfile for high-quality audio streaming. Usually used to stream music.\nThe roles of the two connected A2DP devices are:\nSource role (SRC) - the device that sends audio\nSink role (SNK) - the device that receives audio\nPulseAudio supports both roles. For every discovered A2DP device, two options are available:\nfor SRC device, the server may create a single PulseAudio source which acts as an SNK device\nfor SNK device, the server may create a single PulseAudio sink which acts as an SRC device\nHSP (Headset Profile)\nProfile for phone-quality audio playback and recording. Usually used for phone calls.\nThe roles of the two connected HSP devices are:\nHeadset role (HS) - the device with the speakers and microphone, e.g. a headset\nAudio Gateway role (AG) - the device that serves as a gateway to an external service, e.g. a mobile phone connected to a cellular network\nPulseAudio supports both roles. It can communicate with a headset or be a headset itself for other device. For every discovered HS or AG device, the server may create a pair of PulseAudio source and sink which together act as an AG or HS device.\nHFP (Hands-Free Profile)\nProvides all features of HSP plus some additional features for managing phone calls.\nThe roles of the two connected HFP devices are:\nHands-Free Unit role (HF) - the device with the speakers and microphone, e.g. a portable navigation device\nAudio Gateway role (AG) - the device that serves as a gateway to an external service, e.g. a mobile phone connected to a cellular network\nPulseAudio supports both roles. It can communicate with a HF unit or be a HF unit itself for other device. For every discovered HF or AG device, the server may create a pair of PulseAudio source and sink which together act as an AG or HF device.\nPulseAudio card profile is associated with a Bluetooth profile and role. The following card profiles are available:\nHigh Fidelity Playback (A2DP Sink) - the PulseAudio card will provide a single PulseAudio source which acts as an A2DP SNK device\nHigh Fidelity Capture (A2DP Source) - the PulseAudio card will provide a single PulseAudio sink which acts as an A2DP SRC device\nHeadset Head Unit (HSP/HFP) - the PulseAudio card will provide a pair PulseAudio source and sink which together act as an HF device\nHeadset Audio Gateway (HSP/HFP) - the PulseAudio card will provide a pair PulseAudio source and sink which together act as an AG device\nBluetooth backend listens to BlueZ and oFono events on D-Bus and automatically creates PulseAudio cards, sources, and sinks for all discovered Bluetooth devices.\nThe mapping of the PulseAudio object hierarchy to the Bluetooth object hierarchy is the following:\nPulseAudio card is associated with a Bluetooth device. One PulseAudio card is created for every discovered Bluetooth device.\nPulseAudio card profile is associated with a Bluetooth profile and role. One of the predefined PulseAudio card profiles created for every available operation mode supported by the Bluetooth device.\nOne PulseAudio source and/or one PulseAudio sink is created for every PulseAudio card depending on the currently active card profile.\nOne PulseAudio device port is created for every PulseAudio source or sink.\nThis is how the mapping is used:\nThe card defines what Bluetooth device is used and what profiles are available.\nThe currently active card profile defines what Bluetooth profile and role are used, and so what transport protocols and codecs are used.\nJACK backend component\nmodule-jackdbus-detect\nmodule-jack-{source,sink}\nJACK (JACK Audio Connection Kit) is a professional sound server that provides realtime low-latency connections between applications and hardware. Like PulseAudio, JACK may work on top of several backends, including ALSA.\nHowever, their design goals are different. See comments from PulseAudio authors and JACK authors:\nPulseAudio design is focused on consumer audio for desktop and mobile. It offers seamless device switching, automatic setup of hardware and networking, and power saving. It can’t guarantee extremely low latency. Instead, it usually adjusts latency dynamically to provide lower battery usage and better user experience even on cheap hardware.\nJACK design is focused on professional audio hardware and software. It offers the lowest possible latency and may connect applications directly to devices or each other. It doesn’t try to provide the smooth desktop experience to the detriment of performance or configurability and is targeted to advanced users.\nThere are three alternative options to use PulseAudio and JACK on the same system:\nUse them for different sound cards. JACK can ask PulseAudio to release an ALSA card via device reservation API. Suspend PulseAudio when JACK is running using pasuspender tool. Configure PulseAudio to use JACK backend instead of ALSA. JACK backend for PulseAudio monitors when JACK is started using the JACK D-Bus API, and then creates one source and sink that read and write samples to JACK.\nPulseAudio uses two threads for a JACK source or sink: one realtime thread for the JACK event loop, and another for the PulseAudio one. The reason for an extra thread is that it’s not possible to add custom event sources to the JACK event loop, hence PulseAudio event loop can’t be embedded into it. The extra thread costs extra latency, especially if PulseAudio is not configured to make its threads realtime using rtkit.\nOther backends The following backends are available but have limited functionality:\nOSS\ncomponent\nmodule-detect\nmodule-oss\nOSS (Open Sound System) is an older interface for making and capturing sound in Unix and Unix-like operating systems. Nowadays it is superseded by ALSA on Linux but is used on some other Unix systems. Many systems, including Linux and various *BSD variants, provide a compatibility layer for OSS applications.\nOSS backend implements source and sink for OSS devices. Each one is connected to a single device, usually /dev/dspN. At startup, PulseAudio can automatically create a sink and source for every available OSS device.\nSolaris\ncomponent\nmodule-detect\nmodule-solaris\nSolaris backend implements source and sink for /dev/audio device available in Solaris and some *BSD variants, also known as “Sun audio” or “Sunau” and originally appeared in SunOS. This device supports the Au file format.\nAt startup, PulseAudio can automatically create one sink and one source for /dev/audio device if it is present.\nCoreAudio\ncomponent\nmodule-coreaudio-{detect,device}\nCoreAudio is a low-level API for dealing with sound in Apple’s MacOS and iOS operating systems.\nCoreAudio backend monitors available devices and automatically creates card, sink, and source for every detected device. No card profiles and device ports are implemented.\nWaveOut\ncomponent\nmodule-detect\nmodule-waveout\nWaveOut backend implements source and sink for the legacy Win32 WaveIn/WaveOut interfaces. They are part of MultiMedia Extensions introduced in Windows 95 and still supported in recent Windows versions (with some issues).\nEach source or sink is connected to a single device. At startup, PulseAudio can automatically create one sink and one source for the first available device if it is running on Windows.\nESound\ncomponent\nmodule-esound-sink\nESound backend implements a sink acting as a client for Enlightened Sound Daemon. It doesn’t implement source. The documentation recommends avoiding using this sink because of latency issues.\nNote that PulseAudio server is also able to emulate ESound server.\nHotplug support component\nmodule-{udev,jackbus,coreaudio}-detect\nmodule-{bluetooth,bluez5,bluez4}-discover\nHotplug is currently implemented for the following backends:\nALSA (using libudev) Bluetooth (using BlueZ) JACK (using D-Bus JACK API) CoreAudio In particular, PulseAudio uses libudev to detect ALSA cards (both with and without UCM support). The server creates Udev monitor and filters events for sound card devices:\nwhen a new device is inserted, the server creates a card, card profiles, device ports, sources, and sinks, as described above when the device is removed, all these objects are removed as well Hardware controls PulseAudio server has support for hardware controls. The user should manually specify a sink, and the server will forward volume up/down and mute requests to it.\nTwo types of controls are supported:\nIR remote control\ncomponent\nmodule-lirc\nInfrared remote controls are handled using LIRC (Linux Infrared Remote Control).\nMultimedia buttons\ncomponent\nmodule-mmkbd-evdev\nMultimedia buttons available on some keyboards are handled using evdev, a generic input event interface in the Linux kernel, usually used in programs like X server and Wayland.\nSound processing PulseAudio implements various sound processing tools. Some of them are enabled automatically when necessary (like sample rate conversion), and others should be explicitly configured by the user (like echo cancellation).\nResampler component\nlibpulsecore\nEvery source, sink, source output, and sink input may use its own audio parameters:\nsample format (e.g. 32-bit floats in native endian) sample rate (e.g. 44100Hz) channel map (e.g. two channels for stereo) Source output and sink input are responsible for performing all necessary conversions when they are invoked by source or sink. To achieve this, they configure resampler with appropriate input and output parameters and then run it frame-by-frame.\nWhen resampler is configured, it tries to select an optimal conversion algorithm for requested input and output parameters:\nchooses sample rate conversion method chooses the working sample format for sample rate conversion (some methods benefit from using a higher precision) calculates channel mapping (taking into account channel names and meaning) For every frame, resampler performs the following steps:\nconverts frame from input sample format to working sample format maps input channels to output channels converts sample rate from input rate to output rate if LFE channel (subwoofer) is used, applies the LR4 filter converts frame from working sample format to output sample format Each step is performed only if it’s needed. For example, if input sample format and working sample format are the same, no conversion is necessary.\nSample rate conversion usually operates at fixed input and output rates. When a client creates a stream, it may enable variable rate mode. In this case, input or output rate may be updated on the fly by explicit client request.\nThe user can specify what method to use in the server configuration files. You can find a comparison of hardware and some software resampler methods in this post.\nThe following methods are supported:\nspeex\nFast resampler from Speex library. If PulseAudio was built with speex support, used by default.\nffmpeg\nFast resampler from FFmpeg library. If PulseAudio was built without speex support, and variable rate mode is not requested, used by default.\nsrc\nSlower but high-quality resampler from Secret Rabbit Code (libsamplerate) library. Used in some PulseAudio modules.\nsox\nSlower but high-quality resampler from SoX library. Not used by default.\ntrivial\nBuilt-in low-quality implementation used as a fallback when PulseAudio was built without speex support, and ffmpeg can’t be used because variable rate mode was requested.\nInstead of interpolation, it uses decimation (when downsampling) or duplication (when upsampling).\ncopy\nNo-op implementation used when input and output sample rates are the same.\npeaks\nPseudo resampler that finds peaks. It is enabled when a client requests peak detection mode. Instead of interpolation, it calculates every output sample as a maximum value in the corresponding window of input samples.\nThis mode is usually used in GUI applications like pavucontrol that want to display volume level.\nMixing and volumes component\nlibpulse\nlibpulsecore\nWhen multiple sink inputs are connected to one sink, sink automatically mixes them, taking into account per-channel volume settings. See Volumes and Writing Volume Control UIs pages.\nEvery source, sink, sink input, and source output has its own per-channel volume level that may be controlled via both C API and D-Bus API.\nA way how the sink and sink input volumes are combined is determined by the flat volumes mode (inspired by Windows Vista):\nWhen flat volumes are enabled, the volume of the sink is always the maximum volume of all sink inputs connected to it. When the sink input volume is updated, the sink volume is recalculated too. When the sink volume is updated, all sink input volumes are scaled equally.\nWhen flat volumes are disabled, each sink input has its own volume, considered to be relative to the volume of the sink to which it is connected.\nThis mode may be enabled per-sink or globally (default in many distros).\nThere are two kinds of volumes:\nhardware volumes are used for hardware sources and sinks that support it software volumes are used everywhere else, in particular for hardware sources and sinks that don’t support hardware volume, and for sink input mixing Volumes span from 0% to 100%, which are respectively the silence and the maximum volume that the sound hardware is capable of.\nSoftware volumes use the cubic scale. Hardware volumes generally use an unspecified scale. However, volumes of hardware sources and sinks that have the decibel volume flag and volumes of all sink inputs may be converted to and from the decibel scale using dedicated API functions.\nFinally, virtual source and sinks that are attached to a master source or sink usually use volume sharing mode. When it is enabled, the source or sink always uses the same volume as its master.\nVolume range component\nlibpulse\nlibpulsecore\nmodule-alsa-{source,sink}\nThe device volume range is virtually divided into the three subranges:\n[muted, base] [base, norm] [norm, norm * n] The points on the boundaries are the following:\n“muted”\nConstant. Complete silence (0%, -inf dB).\n“base”\nDetermined dynamically for every device. Defines backend-specific default volume that may be not as loud as the “norm” volume. May be equal to the “norm” volume.\nMapped to the volume where the analog output is at some kind of normalized, pre-defined voltage level. For S/PDIF cards set to the volume where the output PCM samples are unscaled.\n“norm”\nConstant. Maximum hardware volume of a card (100%, 0 dB). If a card includes a proper amplifier, this volume may be very loud.\nFor cards with an amplifier, volumes below this point employ hardware amplification, and volumes above this point employ digital (software) amplification.\nFor cards without an amplifier, digital amplification is always used, both for volumes below and above this point.\nFor cards without the decibel volume flag, volumes above this point are internally truncated the “norm” volume.\n“norm * n”\nMaximum volume that a GUI allows to set, e.g. “norm * 2”.\nA GUI uses a maximum volume above the “norm” to let the user to employ additional digital amplification.\nThis may be useful, for example, if the device is under-powered or the audio content has been mastered with too low volume. However it may cause distortion.\nPassthrough component\nlibpulsecore\nBy default, PulseAudio uses uncompressed PCM everywhere. However, some input and output devices support various compressed audio encodings.\nPulseAudio provides passthrough mode that may be enabled for a source, sink, source output, and sink input. With this mode, samples may be sent in the device-specific compressed encoding.\nWhen a client creates a stream, it may enable passthrough mode. In this case, passthrough mode is enabled for corresponding source output or sink input.\nNote that a passthrough source output may be connected only to a passthrough source, and a passthrough sink input may be connected only to a passthrough sink.\nCurrently, only several S/PDIF (IEC61937) encodings are supported:\nIEC61937 DTS IEC61937 AC3 IEC61937 EAC3 IEC61937 MPEG IEC61937 MPEG2 AAC PulseAudio server doesn’t automatically detect actual encodings supported by hardware. However, PulseAudio client can manually enable or disable encodings for every source and sink using introspection API. The server stores a list of enabled encodings for every source and sink in a database, so these settings are persistent.\nThe user can enable encodings via pavucontrol GUI. Other applications may check which encodings are enabled for a source or sink and use passthrough mode if they support one of the enabled encodings.\nVirtual devices and streams Pulseaudio provides several sound processing tools implemented as virtual devices (sources and sinks) and virtual streams (source outputs and sink inputs).\nsink monitor\ncomponent\nlibpulsecore\nSink monitor reads samples written to the sink.\nEvery sink automatically gets an associated sink monitor. Every time when the sink reads a chunk from its sink inputs, it writes this chunk to the sink monitor.\nThe sink monitor is a sink-to-source-output adapter.\nloopback\ncomponent\nmodule-loopback\nLoopback forwards audio from a source to a sink.\nLoopback is implemented as a pair of a source output and sink input and a queue in between. Source and sink may have different clocks. To deal with it, loopback adjusts resampler rate on the fly to maintain fixed latency calculated from the queue size.\nLoopback is a source-to-sink adapter.\nnull source and sink\ncomponent\nmodule-null-{source,sink}\nThe null sink silently drops all data from the sink inputs. The null source writes silence to the connected source outputs.\nAs any other sink, the null sink has an associated sink monitor, which can be used to read all data written to the sink. Hence, the null sink together with its sink monitor is a sink-input-to-source-output adapter.\ncombine sink\ncomponent\nmodule-combine-sink\nThis sink combines multiple sinks into one.\nAll data written to this sink is forwarded to all connected sinks. Combine sink creates a sink input for every connected sink and duplicates incoming data to every sink input.\nDifferent sinks may have different clocks. To deal with it, combine sink adjusts sample rate for every sink input, which performs resampling, which may be quite CPU intensive. By default, the “trivial” rate conversion method is used.\nsine source and sink input\ncomponent\nmodule-sine-source\nmodule-sine\nSine source and sine sink input generate a sine wave with the preconfigured frequency.\nSine source may be connected to source outputs (e.g. application recording stream or an RTP sender). Sine sink input may be connected to a sink (e.g. ALSA sink or tunnel sink).\npipe source and sink\ncomponent\nmodule-pipe-{source,sink}\nPipe source or sink reads or writes samples to a preconfigured file on disk. This file may be a named pipe (FIFO).\nFilter devices Filter sources and sinks are a special category of virtual devices. Such source or sink has a special virtual source output or sink input connected to another, master source or sink.\nThere are two kinds of filters:\nregular filters\nSuch filter creates one virtual source or sink and one virtual source output or sink input connected to the master source or sink.\ngroup filters\nSuch filter creates a pair of connected virtual source and sink and a pair of virtual source output and sink input, connected to a pair of master source and sink.\nPulseAudio treats filter devices specially in several cases:\nthread sharing\nUnlike regular sources and sinks, filter sources and sinks don’t have a dedicated thread. They are running inside the thread of the master source or sink.\nvolume sharing\nThe filter source or sink always uses the same volume as its master source or sink.\nautoloading\nThe filter source or sink may be automatically loaded and connected to a stream based on stream properties set by an application.\nrouting rules\nAutomatic routing rules have special cases for autoloaded filter sources and sinks.\nRegular filters Several regular filters are available:\nremap source and sink\ncomponent\nmodule-remap-{source,sink}\nRemap source and sink act as a proxy for a master source or sink, performing statically configured channel remapping on top of it.\nequalizer sink\ncomponent\nmodule-equalizer-sink\nThis sink acts as a proxy for a master sink, implementing STFT OLA-based digital equalizer on top of it. The equalizer may be configured on the fly via a D-Bus interface.\nvirtual surround sink\ncomponent\nmodule-virtual-surround-sink\nThis sink acts as a proxy for a master sink, implementing performing a convolution with a prerecorded HRIR WAV file to emulate surround sound when using headphones.\nvirtual source and sink\ncomponent\nmodule-virtual-{source,sink}\nThe source code of the virtual source and sink may be used as a reference when writing a new filter source or sink.\nVirtual source reads data from the master source and writes it to the connected source outputs. Virtual sink reads data from the connected sink inputs and writes it to the master sink.\nEcho cancellation filter component\nmodule-echo-cancel\nAcoustic echo cancellation (AEC) is implemented as a group filter. It creates a connected pair of virtual source and sink, each acting as a proxy for a master source and sink.\nHere is how it works:\nWhen data is sent to the virtual sink, it is forwarded to the master sink. Additionally, the virtual sink stores a frame of recently written samples.\nWhen data is read from the virtual source, it is forwarded from the master source. Additionally, the echo is canceled using the sample frame stored in the virtual sink.\nSeveral AEC engines are implemented:\nspeex\nAcoustic echo cancellation using Speex library.\nwebrtc\nAcoustic echo cancellation using an adapted copy of WebRTC implementation from Google Chromium. It also supports beamforming.\nadrian\nAcoustic echo cancellation NLMS-pw algorithm, originally implemented in VoIP Intercom by Andre Adrian. The implementation uses Orc code generator.\nnull\nNo-op engine.\nLADSPA plugin sink component\nmodule-ladspa-sink\nLADSPA (Linux Audio Developer’s Simple Plugin API) is a standard API for plugins implementing audio filters and audio signal processing effects.\nPulseAudio implements LADSPA support as a filter sink. Each LADSPA sink loads single LADSPA plugin from a shared library. Plugin parameters may be configured when the sink is created or via D-Bus API on the fly.\nA concrete example may be found in this post, which demonstrates how to configure PulseAudio to use an LADSPA plugin for Dynamic Range Compression.\nNote that LADSPA Version 2 (LV2) standard exists, but it’s not supported in PulseAudio.\nConstructing chains Modules add new functionality to the server by implementing sources, source outputs, sinks, and sink inputs. The user then may combine them into a chain. However, only two types of direct connections are allowed:\nsource output to a source sink input to a sink When this is not enough, the elements of a chain have to be connected indirectly using one of the available adapters.\nThe table below summarizes all possible direct and indirect connections. A table row defines from where to read the samples. A table column defines to where to write the samples.\nThe combinations not listed in the table aren’t possible. It’s not possible to read samples from a source output and write samples to a sink input and source.\nto source output\nto sink\nfrom source\ndirectly\nloopback\nfrom sink\nsink monitor\nsink monitor + loopback\nfrom sink input\nnull sink + sink monitor\ndirectly\nSample cache component\nlibpulsecore\nThe sample cache is an in-memory storage for short named batches of samples that may be uploaded to the server once and then played multiple times. It is usually used for event sounds.\nClients may create, remove, and play the sample cache entries using several protocols:\n“native” ESound D-Bus CLI There are several different methods of uploading samples to the sample cache:\nfrom stream\nThe client connects an existing playback stream to a sample cache entry. All samples written to the stream will be temporarily sent to the sample cache entry instead of the sink input associated with the stream.\nThis method is used in the “native” protocol.\nfrom payload\nThe client directly sends the samples to a sample cache entry.\nThis method is used in the D-Bus and ESound protocols.\nfrom file\nThe client asks the server to load the samples from an audio file on disk.\nThe file can be either loaded immediately or lazily. In the latter case, the server loads the file in memory only when it should be played, and automatically unloads it if it wasn’t used for some period of time.\nThis method is used in the CLI protocol.\nWhen the client asks the server to play a sample cache entry, the server creates a new sink input that reads samples from the entry.\nDepending on the protocol, the client may also provide additional properties for the new sink input, which, among other things, may be used by the routing algorithm to choose a sink to connect it to.\nStream management component\nlibpulsecore\nmodule-native-protocol-{fd,unix,tcp}\nClients which use the “native” protocol should create playback or recording streams in order to exchange samples with the server. Every stream is associated with a sink input or source output on the server, which may be connected to a sink or source. The client and server then exchange asynchronous commands and chunks of samples through the stream.\nThe diagram below illustrates the logical data flow from an application to a sound card. Note that it doesn’t reflect that the client application, the native protocol module, the sink module, and the sound card are actually separate execution threads connected via queues.\nStream types There are three types of the “native” protocol streams:\nrecording stream\nA recording stream has a corresponding source output that is connected to a source. Samples are sent from the server to client. Control commands are sent in both directions.\nplayback stream\nA playback stream has a corresponding sink input that is connected to a sink. Samples are sent from the client to server. Control commands are sent in both directions.\nupload stream\nAn upload stream has a corresponding sink input that is connected to a sample cache entry. Samples are sent from the client to server. Control commands are sent in both directions.\nClient to server The client may send the following stream commands to the server:\nwrite or read samples\nThe client sends (for playback streams) or receives (for recording streams) a chunk of samples. When sending samples, the client may either append samples to the stream or specify a seek offset and overwrite previously sent samples.\nstream properties\nThe client gets or sets various stream properties, including timing info (current position and latency), sample spec (sample size, sample rate, and number of channels), channel map (bitmask of enabled channels), format info (stream encoding, PCM or hardware-specific like S/PDIF), buffer attributes (maximum and target buffer size, prebuffering size, request size), and stream proplist (list of arbitrary named properties).\nstream state\nThe client gets current stream state. A stream may be playing (uncorked) or paused (corked).\npause and resume\nThe client corks (pauses) or uncorks (resumes) the stream. The stream is paused as soon as possible without waiting the full latency period.\nprebuffering\nThe client sends prebuf command (start prebuffering) or trigger command (stop prebuffering). When prebuffering is started for a playback stream, the stream is paused until the server-side buffer accumulates required amount of samples. When the server receives enough samples from the client, it automatically starts the stream and disables prebuffering.\nflush\nThe client drops all samples from the server-side stream buffer.\ndrain\nThe client asks the server to inform it when the server reads all samples from the server-side buffer of playback stream and it becomes empty and an underflow occurs. When this happens, all samples sent by the client are already sent to the sink, though they probably didn’t reach the sound card yet.\nServer to client The client may register callbacks for the following stream commands from server:\nrequest to write or read\nThe server requests the client to send (for playback streams) or receive (for recording streams) more samples. The client should send or receive samples only when requested by the server to be clocked by the sound card timer.\nunderflow and overflow\nThe server acknowledges the client that an underflow (underrun) or overflow (overrun) occurred. Underflow occurs when trying to read from an empty stream buffer. Overflow occurs when trying to write to a full stream buffer.\nstream started\nThe server acknowledges the client that the stream was automatically started after prebuffering or underrun.\nstream suspended\nThe server acknowledges the client that the device the stream is connected to was suspended. When all streams connected to a source or sink remain paused for some period of time, the source or sink is suspended to save power.\nstream moved\nThe server acknowledges the client that the stream was moved to another source or sink.\nstream event\nThe server may send custom events to the client with a textual name and arbitrary binary payload. Currently, three event types exist: request-cork (the client should pause stream), request-uncork (the client should unpause stream), format-lost (the stream was moved to another source or sink that doesn’t support encoding being used currently).\nBuffering The diagram below shows what buffers are used to transfer sample chunks from the client to the sound card. See also this post for an overview of buffering in PulseAudio.\nThe following buffers are employed:\ndevice buffer\nThe sink reads samples from connected sink inputs and stores them into the device buffer. In the case of an ALSA sink, this is the kernel-side ALSA ring buffer. The sound card reads samples from it.\nThe size of this buffer is equal to the minimum target stream buffer size among of the all sink inputs connected to sink. This is so to have the largest possible latency still meeting the requirements of all clients.\nrender queue\nThe sink input returns samples from its render queue. When the render queue doesn’t contain enough samples, the sink input pops a chunk from the stream buffer, converts it to the format requested by sink using resampler, and pushes to the render queue.\nThe size of this buffer is equal to the size of device buffer (for samples that were already passed to the sink, needed for rewinding) plus a zero or small amount of samples (for samples that were not yet passed to the sink, appearing only when the stream provided a larger chunk than requested and only a part of the chunk was read).\nresampler buffer\nDepending on the resampling method, resampler may introduce its own buffering between the stream buffer and render queue.\nThe size of this buffer is zero or one chunk.\nstream buffer\nThe server pushes chunks received from client to the stream buffer.\nThe server maintains the stream buffer size near to the target stream buffer size requested by the client via the buffer attributes. Server achieves this by adjusting the number of samples requested from client each time. Therefore, the properly written client should try to respond with the requested number of samples, in average.\nThe size of this buffer is equal to the size of render queue (for samples that were already passed to the render queue, needed for rewinding) plus some amount of samples (for samples that were not yet passed to the render queue, needed to achieve the target latency).\nsocket buffer(s)\nWhen the client sends a chunk to the server, the chunk may pass through zero, one, or two sockets buffers:\nIf a TCP socket is used, both client and server have their own kernel-side socket buffers. Two socket buffers in total.\nIf a Unix domain socket is used, the client and server share the same kernel-side socket buffer. One socket buffer in total.\nIf a Unix domain socket is used, and the zero-copy mode is enabled, the client and server use the same user space shared memory pool, so that no kernel-side buffer is used. In this case, the client allocates a chunk and sends its ID to the server, and the server pushes it to the stream buffer. Zero socket buffers in total.\nThe size of this buffer(s) is determined only by network or scheduling delay. The Nagle’s algorithm is disabled by PulseAudio for TCP sockets and is never used for Unix domain sockets.\nclient buffer\nThe client performs no buffering except a single chunk that it’s going to send to the server.\nSo the size of this buffer is no more than one chunk.\nRewinding Rewinding is a process of overwriting existing samples in buffers instead of appending to them. Implementation details are described in the Rewinding page.\nIt is an important feature that is necessary to combine a higher latency for playback (to reduce glitches) with a lower latency for out-of-band requests like pause and volume changes (to improve user experience).\nFor example, the volume change should be applied immediately even when the playback latency is 2s. To achieve this, all buffers that contain samples with an older volume value are rewound and refilled.\nRewinding and buffers Rewinding works by moving read and write pointers of the ring buffers.\nThe three buffers are employed:\ndevice buffer\nNormally, the sound card moves the read pointer forward, and the sink moves the write pointer forward. On rewind, the sink moves the write pointer backward. The read pointer can’t be moved backward because the sound card has already played the samples.\nrender queue\nNormally, the sink moves the read pointer forward, and the sink input moves the write pointer forward. On rewind, the sink input moves the read pointer backward, so that the sink can re-read required amount of samples.\nTo support this, the render queue always keeps some amount of samples before the read pointer, equal to the size of the device buffer.\nstream buffer\nNormally, the sink input moves the read and write pointers forward. On rewind, the read pointer is moved backward.\nTo support this, the stream buffer always keeps some amount of samples before the read pointer, equal to the size of the render queue, which includes the size of the device buffer.\nRewind processing A rewind is separated into two parts:\nrewind request rewind processing Rewind request may be issued on a sink input or sink. Sink input always propagates rewind requests to the sink. Rewind processing always starts from the sink and then goes down to all connected sink inputs.\nThe diagram below illustrates the overall algorithm.\nThe steps are:\nIf a rewind was requested for a sink input, it is propagated to the sink. If a rewind was requested for a sink, it is processed.\nThe sink processes the rewind. It moves back the write pointer of the device buffer as much as possible. The rewind request may be truncated if some samples were already played and can’t be rewound.\nThe sink asks all connected sink inputs to process the rewind and passes them the number of samples actually rewound.\nThe sink input processes the rewind. First, it moves back the read pointer of the render queue. If it’s not enough, it also moves back the read pointer of the stream buffer.\nRewind requests Here is the list of cases when a rewind request is issued on a sink input or sink:\nstream write with a non-zero seek offset\nThe client explicitly overwrites previously written samples by specifying a seek offset. Buffers are rewound to overwrite unplayed samples.\nstream flushed\nThe client explicitly drops buffered samples. Buffers are rewound to drop unplayed samples.\nsink input corked or uncorked\nThe sink input is paused or unpaused via the stream or introspection API or by an automatic rule. When the stream is paused, buffers are rewound to drop unplayed samples. When the stream is unpaused, buffers are rewound to start playing newly available samples immediately.\nsink input volume changed or muted\nThe sink input volume is changed or muted via the stream or introspection API, by the sink, or by an automatic rule. Buffers are rewound to apply the new volume immediately.\nsink input removed or moved to another sink\nThe sink input is removed or moved due to client disconnect, via the introspection API, or by an automatic rule. Buffers are rewound to drop unplayed samples of the stream.\nsink input underrun ended\nThe sink input was in underrun and finally had provided the samples. Buffers are rewound to overwrite silence with newly available samples and play them immediately.\nsink volume changed or muted\nThe sink volume is changed or muted via the introspection API, by hardware, or by an automatic rule. Buffers are rewound to apply the new volume immediately.\nsink latency decreased\nThe sink decreases its latency due to a new stream connection with a lower latency requirement or on a watermark decrease. Buffers are rewound to shrink the device buffer.\nsink parameters changed\nThe parameters of a virtual sink (like equalizer sink) are changed. Buffers are rewound to apply new parameters immediately.\nMoving streams At any time, a sink input or source output may be moved to another sink or source.\nThe move may be initiated explicitly by any application (typically via the mixer GUI like pavucontrol) or automatically by the routing policy (typically when a device is inserted or removed).\nWhen the stream is moved, a rewind is requested to drop its samples from the sink or source it was previously connected to.\nSynchronized streams When a client creates a stream, it may configure it to be synchronized with another stream.\nPulseAudio guarantees that all streams synchronized together always go sample-by-sample. To achieve this, it automatically propagates control commands issued on a stream (like pause and resume) to all synchronized streams.\nIt’s currently not possible to move a synchronized stream to another device.\nMonitoring The client can monitor existing devices and streams:\nTo monitor a source, the client just connects a recording stream to the source.\nTo monitor a source output, the client connects a recording stream to the source to which the source output is connected to. This is enough because all source outputs connected to the same source get the same data.\nTo monitor a sink, the client connects a recording stream to the corresponding sink monitor.\nTo monitor a sink input, the client connects a recording stream to the corresponding sink monitor and sets the monitor stream of the recording stream to the identifier of the specific sink input to be monitored.\nTime management Playback and recording are driven by a per-device timer-based scheduler that provides clocking and maintains optimal latency.\nThe diagram below illustrates the process. It shows the path of samples from an application (on the left) to a sound card (on the right) when using the “native” protocol.\nClocking There are no two devices with equal clocks. One of them is always slightly faster and another is slightly slower. This applies both to a pair of computers, as well as to a pair of separately clocked devices on the same computer.\nSince sound cards have their own clocks, an application can’t use a CPU timer to send samples to the sound card. Instead, the application should be clocked by the sound card, i.e. use a timer that runs in the sound card time domain.\nIn PulseAudio, clocking is provided by sources and sinks. Hardware source or sink runs a thread that writes or read samples to source outputs or sink inputs using a timer synchronized with the sound card.\nClocking and native protocol component\nlibpulsecore\nmodule-native-protocol-{fd,unix,tcp}\nEvery application stream running over the “native” protocol is clocked by the source or sink to which it is connected. Every client stream has an associated source output (for recording streams) or sink input (for playback streams) on the server.\nWhen source writes samples to the source output, source output forwards them to the client stream. When sink reads samples from the sink input, sink input requests desired amount of samples from the client stream.\nWhen the asynchronous API is used, a callback is invoked when the server requests more samples. The callback should respond with a desired amount of samples. When the simple API is used, the client is blocked until the server requests more samples.\nClocking and RTP component\nmodule-rtp-recv\nAn RTP sender can’t be clocked by an RTP receiver because the sender has no feedback from the receiver and there may be multiple receivers for a single multicast sender. In result, the receiver queue size is slowly but constantly increasing or decreasing.\nSooner or later, it will cause an underrun (the next sample to play is not received yet) or an overrun (the received sample is dropped because it came too early and the queue is still full). The user will hear glitches.\nWhen I was running a demo sender and receiver on two computers, the clock difference was about 0.0055%. This means that every hour the first timer outruns the second by approximately 200 milliseconds. In other words, if the latency is about 200 ms, the playback has to be restarted every hour.\nTo prevent this, PulseAudio RTP receiver adjusts resampler rate on the fly to maintain constant queue size:\nwhen the queue size becomes too high, the rate is slightly increased, the samples are played a bit faster, and after a while, the queue size decreases\nwhen the queue size becomes too low, the rate is slightly decreased, the samples are played a bit slower, and after a while, the queue size increases\nTo prevent oscillations, an exponentially weighted average of the estimated rate is used. To prevent latency jumps, the rate is updated gradually with small steps. Algorithm details are described in the source code.\nLatency component\nlibpulse\nlibpulsecore\nmodule-native-protocol-{fd,unix,tcp}\nmodule-alsa-{source,sink}\nSimply speaking, latency is the delay between sound being played and heard, or between being emitted and recorded. More accurately, we are interested in the delay between a client application and an analog input or output.\nWhen a playback stream is connected to a sink, the following happens:\non server request, the client sends samples to the server via a socket or a shared ring buffer the sink input gets an I/O event, reads samples and writes them to the stream buffer on timer tick, PulseAudio sink reads samples from the sink input stream buffer, performs resampling and mixing, and writes result to the ALSA ring buffer on timer tick, the DMA reads samples from the ALSA ring buffer and writes it to the sound card the sound card passes samples to the codec, which writes the result to the analog output The same is happening with a recording stream, in reverse order:\nthe codec reads samples from the analog input and passes them to the DMA the DMA writes the samples to the ALSA ring buffer on timer tick, PulseAudio source reads samples from the ALSA ring buffer and writes them to source outputs source output reads samples from the stream buffer and sends them to the client the client gets and I/O event and reads samples Accordingly, the latency can’t be less than the sum of:\nthe time to transmit a chunk of samples (or a chunk header in the zero-copy mode) between the client and the server the time to process samples, e.g. the time to resample and mix chunks from all sink inputs connected to the sink in case of a playback stream the number of queued samples in the ALSA ring buffer the time to transmit a chunk of samples via DMA the time to encode or decode samples in the sound card codec PulseAudio can control the stream buffer size and the ALSA ring buffer size. The rest components can’t be controlled and are determined by the hardware capabilities. To achieve the target overall latency, PulseAudio measures the current overall latency and then adjusts the buffer sizes accordingly.\nThe driver and hardware latency, which includes the size of the ALSA ring buffer, the DMA delay, and the sound card codec delay, is measured using the ALSA PCM timestamping.\nOn the diagram above, the minimum possible latency is shown as a vertical bar between the two red lines. Since the client application, the PulseAudio network thread, the PulseAudio device thread, and the sound card run in parallel and may be clocked independently, there are additional small delays caused by scheduler jitter and non-synchronous timers. So in practice, the minimum possible latency will be a bit higher than shown in the diagram.\nNote that compared to bare ALSA, PulseAudio increases the minimum possible latency:\nthe client, the network thread, and the device thread are separate processes, so there are additional context switches the client communicates with the server via IPC, so there are non-zero round trip times (even in the zero-copy mode) On the other hand, PulseAudio may operate at a lower latency than a naive implementation of an ALSA client based on select/poll, due to its advanced timer-based scheduler (see below).\nControlling latency Every source, sink, source output, and sink input has its own latency. PulseAudio server controls all of them and can adjust latency on the fly to reach the minimum acceptable value that causes no glitches. This value may depend on things like hardware capacity and current system load. See LatencyControl page.\nAn application can set its own latency requirements for a stream, using these four parameters of the stream buffer:\nmaxlength - the maximum number of bytes in buffer tlength - the desired number of bytes in buffer, i.e. the target latency prebuf - the minimum number of bytes to be accumulated in buffer before starting the stream, i.e. the start threshold minreq - the minimum number of bytes to be requested from client each time For every stream, PulseAudio server maintains a constant latency, depending on adjust latency mode that may be enabled per-stream by an application:\nIf adjust latency mode is disabled, tlength specifies target size of the stream buffer. PulseAudio server requests or sends samples to the client in such way that there is always about tlength bytes in the stream buffer.\nIf adjust latency mode is enabled, tlength specifies desired size of the stream buffer plus the device buffer. Device buffer size is controlled by the source or sink implementation. Let’s call it dlength.\nIn the case of ALSA source or sink, dlength corresponds to the driver and hardware latency, which includes the size of the ALSA ring buffer, the DMA delay, and the sound card codec delay.\nIn other words, in this mode tlength specifies the desired latency between the client and the sound card. To reach it, PulseAudio server does two things:\nadjusts the dlength to be the minimum tlength value among of the all sink inputs connected to sink\nrequests or sends samples to the client in such way that there is always about tlength - dlength bytes in the stream buffer\nNote that the actual stream latency may be higher than requested by an application. PulseAudio automatically increases the latency depending on hardware and OS scheduler constraints. In particular, the latency is increased in case of frequent ALSA underruns to avoid glitches.\nMeasuring latency PulseAudio provides clients with the stream timing info, containing the stream latency divided into the three components:\ntransport latency\nThe time required to send a sample from the client to the server-side stream buffer. Set to the half of the round trip time, calculated on the client when it sends the timing info request and receives the response.\nstream buffer latency\nThe size of the server-side stream buffer. Equal to the difference between the write index and read index.\nFor playback stream, the client advances the write index and the sink advances the read index. For recording stream, the source advances the write index and the client advances the read index.\nsink or source latency\nThe latency reported by sink or source. In general, it represents delay between the stream buffer and the sound card.\nFor ALSA devices, it is approximately equal to the size of the chunk queue between the sink and sink input, plus the size of the ALSA ring buffer, plus the DMA delay, plus the sound card codec delay.\nTiming info may be retrieved manually by application or automatically when the auto timing update flag is set for a stream.\nThe client library uses timing info to calculate two values:\nstream time\nStream time is the timestamp of the sample being currently played or recorded on the sound card:\nplayback streams: stream_time = streambuf_read_index - sink_latency + transport_latency\nrecording streams: stream_time = streambuf_write_index + source_latency + transport_latency\nThe transport latency is added to take into account the number of samples played or recorded on the server during the time elapsed between the timing info was sent from the server and received on the client.\nstream latency\nStream latency is the difference between the timestamps of the last sample sent or received by the client and the sample being currently played or recorded on the sound card:\nplayback streams: stream_latency = streambuf_write_index - stream_time\nrecording streams: stream_latency = stream_time - streambuf_read_index\nOr, equally:\nplayback streams: stream_latency = streambuf_latency + sink_latency - transport_latency\nrecording streams: stream_latency = streambuf_latency + source_latency - transport_latency\nUsually, the stream time value is not used directly in the calculation above. Instead, it is postprocessed before reporting to the application or calculating the stream latency:\nunless the not monotonic flag is set for the stream, the client ensures that the stream time never steps back;\nif the interpolate timing flag is set for the stream, the client interpolates and smooths the stream time between timing info updates.\nActual calculations in the source code differ from the formulas above in two details:\non signed overflows, negative values may be truncated to zero or reported separately;\nwhen recording stream is connected to a monitor source, the latency of the monitored sink is taken into account in addition to the latency of monitor source.\nLatency and backends Non-ALSA backends generally don’t support adjusting device buffer size. An application can determine if a source or sink supports it by checking the dynamic latency flag of the device.\nSome backends, including Bluetooth devices, don’t provide accurate information about the actual latency. This information is important for some applications, notably for the lip sync in video players.\nTo workaround problems with such backends, the user can manually set the latency offset for a device port, which is zero by default. When a source or sink is connected to a device port, the latency offset of the device port is added to the latency (device buffer size) reported by the source or sink.\nALSA challenges When a program uses ALSA, the program writes or reads samples from the ALSA ring buffer, and the sound card reads or writes samples from that buffer at a timer tick. The user can configure the buffer size (number of samples in whole ring buffer) and the period size (a.k.a. fragment size, the number of samples written or read per one timer tick).\nAs noted above, the program should be clocked by the sound card. Traditionally, this is achieved by either blocking on ALSA write or read operation until the sound card updates the ring buffer, or waiting for it using poll or select.\nThis way, the whole process is driven by the sound card timer, which is good. However, two problems may be encountered when developing ALSA client:\nIf poll or select is used, there is always a short delay between the moment when the sound card reads from the ring buffer and the program writes next chunk of samples to it. The time is spent for a context switch from the kernel space to the user space, returning from poll or select, issuing the next write call, and finally doing one more context switch from the user space to the kernel space.\nOn low period sizes, this delay can cause glitches. The delay may be avoided if the program uses blocking write instead of poll or select, but this doesn’t allow to do I/O multiplexing, which may be necessary.\nIt’s not easy to guess optimal buffer size and period size because they depend on the latency, hardware, CPU, and average system load. When default parameters chosen by ALSA doesn’t play well enough, the client programming becomes more tricky.\nTimer-based scheduler (tsched) component\nmodule-alsa-{source,sink}\nPulseAudio addresses these challenges by doing its own timer-based scheduling, also known as the glitch-free playback model. It was introduced in 0.9.11 version and inspired by audio systems in Windows Vista and MacOS.\nWith the timer-based scheduling, PulseAudio is able to fulfill the two requirements at the same time:\nthe server usually doesn’t introduce glitches by itself even with low latency values\nthe client doesn’t bother about the ring buffer parameters and advanced timing techniques and can just request the latency it needs\nWithout it, the glitches were more common when using PulseAudio with low latency values, and applications had to either switch to a higher latency or use ALSA directly.\nHere are its basic ideas:\nTimer\nInstead of using poll or select on ALSA device, PulseAudio configures its own timer and uses poll or select on the timer. To avoid glitches, PulseAudio timer is configured to fire some time before the sound card timer, so that PulseAudio has enough time to write next chunk of samples to the ring buffer.\nSynchronization\nPulseAudio monitors the size of ALSA ring buffer and adjusts the timer to be synchronous with the sound card timer. To avoid oscillations, the sleep period for the PulseAudio timer is updated smoothly.\nWatermark\nPulseAudio maintains a watermark for the number of unread bytes in the ring buffer. When this number becomes lower than the watermark, or (in the worst case) an underrun occurs, the watermark is increased. When this number becomes higher than the watermark, the watermark is decreased.\nThe key point of the watermark is that PulseAudio can detect and prevent an underrun until it really happens. The watermark value affects two things:\nThe delta between PulseAudio timer and ALSA timer. The higher is the watermark, the greater the delta, so that PulseAudio has more time to fill the ring buffer before ALSA timer tick happens.\nThe latency. When the watermark becomes too high, the latency is increased. When the watermark becomes low again, the latency is decreased back. This means that the actual latency may be higher than requested.\nInterrupts\nOn every tick of the sound card timer, an interrupt (a.k.a. period wakeup) is generated, and the process blocked on ALSA write or poll is woken up. PulseAudio doesn’t need this, so it tries to reduce the number of interrupts to lower CPU usage.\nTo reduce the number of interrupts, the buffer size and period size (a.k.a. fragment size) are set as large as supported by hardware. Typical values are 2s buffer size and 1s or 0.5s period size.\nIf supported by the sound card driver, interrupts for the sound card timer are disabled at all. This only works with recent ALSA versions and some drivers. This should work at least with recent Intel drivers.\nRewinding\nTo provide instant reaction on user-input, PulseAudio uses the ALSA feature of buffer rewriting. Whenever an application performs a seek, pause, or writes more samples, PulseAudio rewrites the ring buffer with actual data.\nClocking\nWhen a sink reads samples from a sink input, the sink input requests more samples from the application. The process is driven by the sink timer, which is kept synchronous with the sound card timer. The samples are requested some time before they should be written to the ring buffer, so the application has time to receive the request and response with a chunk of samples.\nThe timer-based scheduler may be enabled or disabled globally or per-sink. By default, it is automatically enabled when:\nthe card is a real hardware device the card driver supports mmap the card driver doesn’t use double buffering (batch mode), so that the real device buffer may be updated at any time PulseAudio is not running under a virtual machine (yes, it has some code to detect this) Power saving Several techniques are used to save the power. Some benchmarks may be found in this post.\nDevice states component\nlibpulsecore\nmodule-suspend-on-idle\nA source or sink may be in one of the following states:\nRUNNING\nActively performing I/O. There are non-paused connected streams.\nIDLE\nActively performing I/O, but there are no non-paused connected streams. In the case of a sink, zero samples are written to the device. In the case of a source, recorded samples are dropped.\nSUSPENDED\nNot performing any I/O.\nThe source or sink is marked idle when there are no non-paused connected streams. If it remains in this state for some time, it may be suspended. When a non-paused stream appears again, the source or sink is resumed.\nReducing interrupts component\nmodule-alsa-{source,sink}\nThe less frequently sound card interrupts occur, the less frequently the driver wakes up, the less power is used.\nWhen the timer-based scheduler is used, PulseAudio reduces the number of sound card interrupts or completely disables them if it’s supported by the driver.\nDefault latency component\nlibpulsecore\nmodule-native-protocol-{fd,unix,tcp}\nThe higher is latency, the less frequently the server wakes up, the less power is used.\nHigh latency may be set for a stream automatically:\nIf an application uses PulseAudio and didn’t specify the latency, PulseAudio automatically selects the default value, which is high, typically 2s.\nIf an application uses GStreamer, the user can configure GStreamer to select a high latency automatically for applications which media role is “music”, as described in this post. However, GStreamer doesn’t do it by default.\nGStreamer uses PulseAudio as a backend and is a backend itself for many applications and higher-level media frameworks like Xine and Phonon. See details here.\nAutomatic setup and routing PulseAudio automatically restores parameters for cards, devices, and streams, routes streams to devices, and performs other housekeeping actions.\nSome examples:\nWhen a new card, device, or stream appears, the server should restore previously configured parameters.\nWhen a card or device appears, the server may move existing streams to it. When a card or device disappears, the server may move existing streams to another device.\nWhen a client creates a new stream, the server should route it to some device.\nWhen a client creates a new stream, the server may perform some automatic setup depending on stream properties, like autoloading sound processing tools, or silencing less important streams.\nDatabases component\nlibpulsecore\nPulseAudio uses an embedded database to store parameters and routing rules persistently. It supports three database backends:\ngdbm (GNU dbm) tdb “simple” (built-in hashtable-based implementation) Two separate databases are involved in routing:\nrestoration database device manager database Stream roles component\nlibpulsecore\nThe routing and automatic setup are heavily based on roles. A role is a short string describing media type, like “video”, “music”, or “phone”.\nRoles are used in several places:\nevery stream may have the “media.role” property provided by application every device may have the “device.intended_roles” property provided by the device backend the restoration database may contain per-role routing rules the device manager database may contain per-role priority lists of routing rules Stream groups component\nlibpulsecore\nTwo streams belong to the same group if they have the same group identifier. PulseAudio checks the following stream properties and uses the first available one as the group identifier of the stream:\n“media.role” (e.g. “video”, “music”, or “phone”) “application.id” (e.g. “org.gnome.Totem”) “application.name” (e.g. “Totem Music Player”) “media.name” (e.g. “Pink Floyd - Astronomy Domine”) If none of these properties are present in the stream property list, PulseAudio uses the default group identifier, which is the same for all streams.\nNote that more generic properties are preferred over more specific ones.\nStream groups are used in two places:\nin the restoration database when autoloading group filters Stream routing component\nlibpulsecore\nmodule-stream-restore\nmodule-device-manager\nmodule-intended-roles\nRouting is a process of choosing to which device (source or sink) to connect a new stream (source output or sink input). A good routing overview can be found in this post.\nRouting consists of several steps:\napplication device\nIf the client has specified a device name, the stream is routed to that device.\nThis usually means that the user has configured the application to use a specific device. Some applications may provide a command line option or GUI for that.\nmodule-stream-restore\nOtherwise, module-stream-restore (enabled by default) checks the restoration database. If there is a stored device for the stream group, and the device is currently available, the stream is routed to that device.\nThis means that the user had moved a stream from the same stream group to that device earlier, and this decision was remembered. The restoration database is updated only when the user manually moves a stream via GUI. It’s not affected by automatic routing.\nmodule-device-manager\nOtherwise, module-device-manager (enabled in KDE) checks its per-role and global priority lists of devices. If there is a non-empty priority list for the stream role or non-empty global priority list, and there are currently available devices in the list, the stream is routed to the first such device.\nThis means that the user has configured some priority lists to be used for a role or globally. KDE provides a GUI for that.\nmodule-intended-roles\nOtherwise, module-intended-roles (enabled by default) searches for a device which intended role list contains the stream role. If such device exists, the stream is routed to that device.\nIn other words, a per-role default device is used. The intended role list of a device is provided by the device backend. It can also be set manually when creating the device.\nfallback device\nOtherwise, the stream is routed to the fallback device.\nFallback source and fallback sink may be changed by the user.\nSome side notes:\nAfter the user had once manually moved a stream, there is an entry for it in the restoration database, and the other routing steps are never executed again for this stream. Some GUI tools provide a function of removing routing rules from the restoration database.\nWhen the user moves a stream to another device, all streams of the same stream group are immediately moved as well, and a routing rule for the whole group is stored in the restoration database.\nWhen the user updates device manager routing rules, existing streams are immediately re-routed according to the new routing rules.\nWhen the user changes the fallback source or sink, nothing happens. The new fallback device will be used only when a new stream is routed.\nRestoration database component\nmodule-{card,device,stream}-restore\nmodule-default-device-restore\nThree parameter categories are stored in the restoration database:\nactive profiles of cards volume/mute settings and active ports of devices volume/mute settings and routing rules (stream group name plus device name) There is a separate module responsible for managing every category:\nmodule-card-restore module-device-restore module-stream-restore The modules implement two functions:\nall modules monitor server objects and read or write appropriate category of parameters to the restoration database\nsome modules also provide a protocol and API extension that enables client applications to read, write, and monitor the corresponding category of parameters in the database\nEach module monitors both existing and new objects:\nWhen an appropriate parameter of an existing object is changed by the user, the module stores the object ID, parameter ID, and parameter value into the database.\nWhen a new object (card, device, or stream) appears, the module checks if there are stored parameters for this object, and restores them, if any.\nWhat is used as the object ID depends on the object type:\nfor cards and devices (sources and sinks), the card or device name is used for streams, the stream group identifier is used, computed from the stream properties The outcome of using stream group as the object ID is the following:\nAll streams with the same role share the same volume and routing settings. For example, all music streams or all notification streams.\nAll streams without a role, but belonging to the same application, share the same volume and routing settings. For example, all instances of a music player.\nAll streams without a role and an application identifier, but with the same media name, share the same volume and routing settings. For example, all music players which are playing the same file and have specified media name in the same format.\nAll streams that didn’t specify any of the above properties share the same volume and routing settings.\nWhen the user changes the stream volume or moves it to another device, all other streams with the shared volume and routing are automatically updated or moved as well.\nBesides the three modules described above, the module-default-device-restore saves (on a timer event) and restores (on start) the fallback source and sink. These two device names are stored in two text files instead of the restoration database.\nDevice manager database component\nmodule-device-manager\nThe module-device-manager was developed for KDE, which uses it by default and provides a GUI tool for manipulating priority lists. The module performs three functions:\nMaintains a database with multiple priority lists of devices: one priority list per role, and one default priority list. The priority list may contain both devices that are available currently or were available in the past.\nImplements routing. When a new stream can’t be routed using the restoration database, device manager checks if there is a non-empty priority list for the stream role or non-empty default priority list. If a non-empty priority list is found, the first currently available device from the priority list is used.\nProvides a protocol and API extension that provides methods for inspecting and manipulating priority lists.\nDevice intended roles component\nmodule-intended-roles\nThe module-intended-roles implements a kind of automatic per-role fallback device.\nThis is how it works:\nWhen server backend creates a device, it may specify a list of roles intended for the device via the “device.intended_roles” property. This property may be also autodetected from the device form factor.\nWhen a client creates a context or a stream, it may specify its role via the “media.role” property. This property may be also autodetected from the application desktop file.\nWhen server routes a new stream, it searches for a device which has the stream role in its intended role list. The fallback device is always checked first, so if its intended roles match, it takes priority over other devices.\nThe intended role list is set for several types of sources and sinks:\nalsa\nALSA backend sets intended roles of sources and sinks if the ALSA card supports UCM.\nFor every source or sink, PulseAudio computes intended roles from the UCM modifiers associated with the device ports of the source or sink. Every UCM modifier name is converted to a PulseAudio role name.\nbluetooth\nBluetooth backend sets intended roles of sources and sinks depending on the device form factor.\nraop\nRAOP module unconditionally sets “music” role for RAOP sinks.\naec\nAcoustic echo cancellation module unconditionally sets “phone” role for its sources and sinks.\nPriority routing proposal component\nlibpulsecore\nAs discussed in the blog post mentioned above, existing routing scheme is non-intuitive, and its steps are quite isolated and inconsistent in several aspects:\nIf all applications will correctly provide stream roles, it will not be possible to move a single stream to another device without affecting all streams with the same role. Per-application stream moving works so far only because most applications don’t specify stream roles, and PulseAudio uses application ID instead.\nWhen a stream is moved manually or device manager rules are updated, existing streams are re-routed. When the fallback device is updated, existing streams are not affected, however.\nWhen configuring device manager, it’s clear that there are per-role device lists. When moving a stream, it’s not clear what rules will be overwritten (per-stream, per-role, per-application, or something else).\nDevice manager routing rules contain device lists with both available and unavailable devices, and the first available device is used. Restoration database routing rules contain only a single device, and the rule is used only when the device is available.\nRestoration database rules override the device manager rules, but this is not obvious. The overrides may suddenly appear or disappear depending on whether the corresponding device is currently available and what meta-information is provided by an application.\nIn result, it’s hard for the user to figure it out how and why the routing works. A non-implemented PriorityRouting proposal exists, aimed to make routing more consistent and transparent:\npriority lists are moved to the PulseAudio core and become first-class objects all existing routing steps are reworked to operate on top of these priority lists modules may implement routing policies by registering or manipulating priority lists the user can inspect and configure priority lists using GUI tools Third-party routing modules Some projects implement their own PulseAudio modules that replace or modify default routing scheme:\nTizen IVI\ncomponent\nmodule-murphy-ivi\nTizen IVI (Linux for In-Vehicle Infotainment) project uses Murphy, a resource policy daemon that manages things like audio, video and network connections. The pulseaudio-module-murphy-ivi PulseAudio module implements routing using Murphy as a backend.\nSee these slides for some details.\nIoT.bzh\ncomponent\nagl-audio-plugin\nIoT.bzh (Linux for Internet Of Things) project uses agl-audio-plugin PulseAudio module. It was forked from the Tizen IVI PulseAudio module and is its simplified version that doesn’t need Murphy and uses either a JSON configuration file or its own embedded configuration.\nThere are also some slides with details.\nNemo\ncomponent\nmodule-meego-*\nmodule-policy-enforcement\nmodule-stream-restore-nemo\nNemo is a Linux distribution for mobile devices. It uses Mer (a MeeGo fork). It implements several PulseAudio modules with custom routing, volume, and permission policies.\nSee details on their wiki.\nSailfish OS\ncomponent\nmodule-droid-*\nSailfish OS a mobile OS based on Nemo and Mer. It uses pulseaudio-modules-droid, which implements several PulseAudio modules allowing PulseAudio to work on top of the Android Audio HAL.\nSee details on these slides.\nAutodetecting properties component\nlibpulsecore\nmodule-augment-properties\nWhen an application connects to the server, a new client object is created on the server. The C API automatically sets the “application.process.binary” property of the client, which contains the name of the application executable.\nWhen a new client is created, PulseAudio may automatically find the desktop file for the application and use it to compute some client properties. The desktop file is usually searched in the \u0026quot;/usr/share/desktop\u0026quot; directory and should have the same name as the application executable.\nThe two desktop file entries are used:\nX-PulseAudio-Properties\nThis entry may define arbitrary properties for the client.\nCategories\nThis may be used to compute the “media.role” property for the client. Currently, the “Game” category is mapped to the “game” role, and the “Telephony” category is mapped to the “phone” role.\nWhen a new stream is created, it inherits all properties from the client, including the ones that were detected from the desktop file. If the stream has its own properties, they override the client properties.\nSources and sinks may have “device.form_factor” property. Form factor is a short string describing device type, e.g. “handset”, “headset”, “speaker”, or “microphone”.\nIf a new device has an empty intended role list property, but non-empty form factor property, in some cases PulseAudio may automatically compute intended roles from the form factor. Currently, “headset”, “handset”, and “hands-free” form factors are converted to the “phone” role.\nCurrently, the device form factor is set in two places:\nBluetooth backend computes device form factor from the device class.\nIf Udev rules can match the sound card model, the SOUND_FORM_FACTOR property is attached to the device. The server reads this property during source or sink initialization.\nAutoloading filters component\nmodule-filter-{heuristics,apply}\nPulseAudio may automatically load and setup filter sources and sinks based on the stream properties. An application specifies what sound processing tools it wants for a stream, and the server performs all necessary configuration.\nThis mechanism is based on a convention used for all filter sources and sinks:\nevery filter is implemented in a separate module the module name is the filter name every module creates one source, or one sink, or one paired source and sink every module accepts the “master” or the “source_master” and “sink_master” arguments Three stream properties are employed:\n“filter.want”\nThe name of the filter to load. The server is allowed to ignore the filter when it thinks it’s unreasonable.\n“filter.apply”\nThe name of the filter to load. The server unconditionally loads the filter. Overrides “filter.want”.\n“filter.suppress”\nThe name of the filter not to load. The server doesn’t load the filter, even if it’s specified in “filter.want” or “filter.apply”. Useful when “filter.want” or “filter.apply” property is set automatically, but application wants to disable it.\nThe autoloading support is divided into two modules:\nmodule-filter-heuristics\nTracks when a stream is created or moved.\nIf the “filter.apply” is unset and the “filter.want” property is set and the specified filter should not be ignored, sets the “filter.apply” property to the value of the “filter.want” property.\nCurrently, the only filter that may be ignored is “echo-cancel”. It is ignored if the “device.intended_roles” property of the stream device contains the “phone” role.\nmodule-filter-apply\nTracks when a stream is created, moved, or stream properties are updated.\nIf the “filter.apply” is set and not disabled by the “filter.suppress” property, and the filter is not loaded yet, the module does the following:\nChecks if this is a group filter, which uses a paired source and sink and therefore requires a paired source output and sink input. In this case, the filter is loaded only if the paired stream exists as well.\nTwo streams are considered paired if they have the same “filter.apply” property and the same stream group identifier. Currently, the only group filter is “echo-cancel”. Checks if the user did specify additional module paraterer via the “filter.apply.\u0026lt;filter_name\u0026gt;.parameters” property.\nLoads the “module-\u0026lt;filter_name\u0026gt;” module. The “master” or the “source_master” and “sink_master” arguments are set to the name of the source or sink to which the stream or paired streams are currently connected. If the user did specify additional parameters, they are also passed to the module.\nFinds the newly loaded filter source or sink.\nMoves the stream or paired streams to the filter source or sink.\nAutomatic actions Several modules implement various housekeeping actions that are performed automatically.\nswitch on port available\ncomponent\nmodule-switch-on-port-available\nWhen the availability of device ports changes, automatically switch the active device port and card profile.\nThe device port is switched when the currently active device port becomes unavailable, or a higher priority device port becomes available. The active card profile may be also changed if necessary. The priorities of ports and profiles are defined by their backend.\nTo avoid unwanted switches, the module tracks manual port and profile changes made by the user and uses some heuristics to determine what profile is preferred for every port, and what port is preferred for every profile.\nIn practice, this module is only relevant for ALSA cards. When the user plugs in something using the analog jack on a sound card, that typically makes some port available and may also make another port unavailable. Similarly, when the user unplugs something from an analog jack, that typically makes some port unavailable and may also make another port available.\nswitch on connect\ncomponent\nmodule-switch-on-connect\nWhen a new source or sink appears, automatically set it as the fallback device and move all active streams to it.\nrescue streams\ncomponent\nmodule-rescue-streams\nWhen a source or sink disappears, automatically move streams connected to it to another working source or sink.\nalways sink\ncomponent\nmodule-always-sink\nEnsure that there is always at least one non-filter sink. When all sinks disappear, automatically load the null sink.\nrole ducking and corking\ncomponent\nmodule-role-{ducking,cork}\nWhen an “important” stream is started, automatically duck (lower the volume) or cork (mute and request a pause) active streams. When the important stream finishes, unduck or uncork other active streams.\nWhether a stream is important or not is determined by its “media.role” property. By default, streams with the “phone” role are considered important.\nallow passthrough\ncomponent\nmodule-allow-passthrough\nWhen a new passthrough stream is moved to an existing sink, automatically create a null sink and move all other running streams to that null sink. When the passthrough finishes, move the streams back and remove the null sink.\nThe reason for such behavior is that passthrough streams are incompatible with regular PCM streams and they can’t be connected to the same sink at the same time. Therefore, if the user moves a passthrough stream to a sink, all other streams should be temporary disconnected.\nposition event sounds\ncomponent\nmodule-position-event-sounds\nWhen a stream is created or its properties are changed, adjust the volume balance of the stream depending on its on-screen 2-D coordinated provided by the application.\nFirst, PulseAudio checks that:\nthe “media.role” property of the stream is set to “event”, which is usually true for GUI event sounds\nthe “event.id” property of the stream is not set to one of the known identifiers of test sounds used in some volume control dialogs\nwhether the “event.mouse.hpos” and “event.mouse.vpos”, or “window.hpos” and “window.vpos” properties are set for the stream\nIf all of the above is true, PulseAudio adjusts the volume balance of the stream:\nthe horizontal position defines the balance between the “left” and “right” channels\nthe vertical position defines the balance between the “front” and “rear” channels\nThus, the GUI events on the screen are virtually mapped to a horizontal plane around the user.\nmatch volumes\ncomponent\nmodule-match\nWhen a new stream appears, automatically update its volume based on its name and a preconfigured match table, by default loaded from \u0026quot;~/.pulse/match\u0026quot;. Each line contains a regular expression to match the stream name and the volume to set.\nRole-based configuration of ALSA devices component\nmodule-alsa-card\nWhen a stream is moved to or from an ALSA source or sink which employs UCM, the stream role may affect ALSA mixer configuration:\nEvery ALSA device port is associated with zero or one UCM modifier.\nEvery UCM modifier is mapped to a PulseAudio role.\nThe UCM modifier of a device port is enabled when there is at least one source output or sink input connected to the source or sink of the device port, which has the “media.role” property equal to the UCM modifier role.\nAutomated setup of Bluetooth devices component\nmodule-bluetooth-policy\nPulseAudio provides two features that automate the setup of Bluetooth devices that support A2DP (usually used for music) and HSP (usually used for phone calls):\nautomatic profile switch\nAutomatically switch between A2DP and HSP profiles (if both are available for a device), depending on the roles of the currently active streams.\nWhen a new stream is created and its “media.role” property is set to “phone” or unset (depending on the “auto_switch” module parameter), switch the card profile to HSP, to make the bluetooth source available when the regular stream routing logic is executed. When all such source outputs disappear, switch the card profile to A2DP.\nautomatic playback\nAutomatically route and play audio from A2DP and HSP/HFP AG sources.\nWhen a new A2DP or HSP/HFP AG source is created for a BLuetooth card, do the following:\ncreate a loopback, i.e. a pair of a source output and sink input connected with a queue connect the loopback source output to the source set the “media.role” property of the loopback sink input to “music” (for A2DP) or “phone” (for HSP/HFP), which may be used for routing let PulseAudio route the loopback sink input to some sink automatic capture\nAutomatically route and record audio from HSP/HFP AG sinks.\nWhen a new HSP/HFP AG sink is created for a Bluetooth card, do the following:\ncreate a loopback, i.e. a pair of a source output and sink input connected with a queue connect the loopback sink input to the sink set the “media.role” property of the loopback source output to “phone”, which may be used for routing let PulseAudio route the loopback source output to some source Desktop integrations This section describes features that integrate PulseAudio into the desktop environment. Some details are also available on the Desktops page on wiki.\nAutospawn and autoexit PulseAudio server usually starts and exits automatically:\nautospawn\ncomponent\nlibpulse\nWhen a client tries to communicate to the server via libpulse, the server is started automatically if it’s not started yet. Due to this feature, when the user kills the server but an active client exists, the server will be automatically started again. This feature can be disabled in the client configuration file.\nautoexit\ncomponent\nlibpulsecore\nWhen there are no connected clients during some period of time, the server automatically exits. However, the automatic exit may be prevented by one of the session management modules.\nSession management There are several session management modules that prevent the server from exiting during the lifetime of a desktop session:\nsystemd-logind\ncomponent\nmodule-systemd-login\nThis module monitors logind events via D-Bus API. It creates a fake PulseAudio client for every new login session of the current user (determined by UID), and removes it when the session ends. These fake clients keep the server opened until the last user session ends.\nConsoleKit\ncomponent\nmodule-console-kit\nThis module is the same but monitors ConsoleKit events.\nXSMP\ncomponent\nmodule-x11-xsmp\nThis module connects to the X session manager via XSMP protocol. It creates a fake client and removes it when the current X11 session ends.\nX11 publishing component\nmodule-x11-publish\nPulseAudio server may publish its address and credentials via the X11 root window properties. These properties may be then read by clients running on the same X display, including remote clients that use SSH X forwarding.\nThe properties can be read using this command:\nxprop -root | grep PULSE Here is the list:\nPULSE_COOKIE\nAn authentication cookie that may be used by clients to connect to the server. All clients that have access to the current X11 display will be able to connect to PulseAudio server too.\nPULSE_ID\nServer ID in form of \u0026quot;server_uid@machine_id/server_pid\u0026quot;.\nPULSE_SERVER\nSpace-separated list of server sockets, e.g. \u0026quot;{machine_id}unix:/socket/path tcp:hostname:port\u0026quot;. The server automatically updates this property when sockets are opened or closed.\nPULSE_SESSION_ID\nThe value of $XDG_SESSION_ID environment variable when the server was started. Omitted if the variable wasn’t set.\nPULSE_SOURCE, PULSE_SINK\nThese properties are optional. They may be set to a source or sink name provided by the user via the module arguments when starting the server. Clients will use these source and sink as defaults.\nX11 events PulseAudio can interact with X11 events in two ways:\nBell\ncomponent\nmodule-x11-bell\nThe core X11 protocol allows clients to ring a bell, imitating analogous feature of the TTY. XKB extends this by supporting multiple named bells and providing an API for controlling bells and handling bell events.\nPulseAudio is able to intercept XKB bell event and play a preconfigured sample from the sample cache.\nCork\ncomponent\nmodule-x11-cork-request\nPulseAudio can automatically cork (mute and request a pause) all active streams when a more important stream appears, and uncork them when it disappears.\nThis is implemented as special cork-request and uncork-request events sent from the server to clients. However, many PulseAudio clients don’t subscribe and handle server events, and their streams become muted but not paused.\nAs a workaround, PulseAudio can artificially synthesize X11 media key events along with the cork or uncork request, as if the pause or play multimedia keyboard button was pressed. Some applications will handle these events and pause/resume playback. This scheme is known to be buggy, however.\nRealtimeKit (rtkit) component\nlibpulsecore\nRealtimeKit provides a D-Bus API that allows user processes to enable the realtime scheduling policy without the root privileges. Basically, it’s a D-Bus, rootless, policy-based replacement for the sched_setscheduler POSIX call, plus a watchdog. Some details are available in this post.\nPulseAudio may use it to enable SCHED_RR policy for some of its threads. Threads under this policy preempt any other threads on the system except the other realtime threads.\nThe realtime policy is enabled for the sink and source threads, including the ALSA sink thread that runs the timer-based scheduler. This helps to handle low latency values because when it’s time to provide samples for the ALSA driver, PulseAudio will not be delayed even if there are other starved processes.\nGNOME registry (GConf) component\nmodule-gconf\ngconf-helper\nGConf is a system used by the GNOME desktop environment for storing configuration settings for the desktop and applications. GConf is currently deprecated and is replaced with GSettings and dconf, but is still available in distros.\nPulseAudio monitors the \u0026quot;/system/pulseaudio/modules\u0026quot; GConf directory that should have the following layout:\nsystem └── pulseaudio └── modules # root directory monitored by PulseAudio └── \u0026lt;foobar\u0026gt; # subdirectory with an arbitrary name ├── enabled # contains a boolean, true if this subdirectory is enabled ├── name0 # contains a string with module name ├── args0 # contains a string with module arguments ├── name1 # contains a string with module name ├── args1 # contains a string with module arguments └── ... For every subdirectory, PulseAudio automatically loads modules when new entries appear (up to ten entries currently) and unloads them when they disappear or “enabled” entry is set to false. The monitoring is implemented in gconf-helper tool which is the part of the PulseAudio package.\nThis feature is used in the paprefs GUI which may be used to configure and enable some non-default modules like RTP sender and receiver. The advantage of this approach is that these settings are stored persistently in the GConf database.\nCompatibility layers There are several compatibility layers with other sound systems, so that existing applications may automatically run on PulseAudio without modification.\nEmulate ALSA component\nalsa-plugins\nALSA implements both kernel drivers and a user space library (libasound) with a high-level API for applications. This library supports plugins, which implement virtual ALSA devices.\nPlugins are usually used for two things:\nto add software sound processing on top of a real device, when it’s missed in hardware (e.g. resampling or mixing) to redirect sound to another sound systems (e.g. PulseAudio and JACK) The user manually enables plugins and sets the default device in ALSA configuration files, regularly \u0026quot;~/.asoundrc\u0026quot; or \u0026quot;/etc/asound.conf\u0026quot;.\nNote that plugins are the feature of libasound. They work entirely in user space inside a process that uses the library and has opened a virtual device. ALSA is not a sound server and doesn’t run a daemon process, unlike PulseAudio and JACK.\nPulseAudio ships with an ALSA plugin that implements the “pulse” virtual device. In Linux distros that use PulseAudio, this device is usually configured as the default device for ALSA applications.\nThis is how it works:\nWhen an ALSA application opens the “pulse” device, a new PulseAudio stream is created, and playback or recording is redirected to PulseAudio server.\nPulseAudio server does its usual business: routing, mixing, adjusting volume, etc.\nIf the stream was routed to a local sound card, and PulseAudio uses ALSA backend, the stream goes to libasound again. This time, however, a hardware device for the appropriate sound card is used instead of the virtual “pulse” device.\nFinally, libasound asks the kernel space ALSA driver to write or read samples from the sound card ring buffer.\nEmulate OSS component\nossp\npadsp\nOSS is an aged interface for making and capturing sound in Unix. OSS creates a character device for every sound card, and applications may open device, write or read samples, and perform control commands with ioctl.\nossp (OSS Proxy) provides a full-featured OSS emulation on top of ALSA or PulseAudio. It has a modular architecture consisting of the main daemon and backends:\nosspd\nosspd daemon creates and serves OSS devices using CUSE (Character Device in User Space). It forwards sound to a backend.\nossp-aslap\nossp-aslap backend forwards sound to ALSA using libasound.\nossp-padsp\nossp-padsp backend forwards sound to PulseAudio using libpulse.\nWell, quite enough forwarding:\nIn addition, PulseAudio provides the padsp wrapper. It intercepts standard library functions using the LD_PRELOAD trick:\nwhen the wrapped application tries to open an OSS device, it gets a fake file descriptor\nwhen the application issues an operation on that fake descriptor (e.g. write, stat, or ioctl), the wrapper handles it and forwards sound or control commands to PulseAudio\nThe same approach is used in aoss (OSS to ALSA), esddsp (OSS to ESound), and artsdsp (OSS to arts sound server). It works for some applications but is known to be incomplete.\nNote that ALSA driver also provides in-kernel OSS emulation. However, it’s not aware of the user space stuff, including libasound virtual devices, and therefore can’t be used to forward sound to PulseAudio. See OSS emulation on ALSA wiki.\nEmulate ESound component\nmodule-esound-protocol-{unix,tcp}\nmodule-esound-compat-{spawnpid,spawnfd}\nLike PulseAudio, Enlightened Sound Daemon is a sound server accessed by applications via a socket. This daemon was being used in GNOME before it switched to PulseAudio.\nFor seamless migration, PulseAudio server provides two features:\nA module that implements the ESound protocol and emulates ESD server. Existing applications may communicate with PulseAudio server as if it were an ESound server.\nTwo modules that implement ESound autospawn conventions. An application may start PulseAudio server as if it were an ESound server, and PulseAudio will notify the application that it was successfully started with a signal of via a file descriptor.\nPartially or completely disable PulseAudio There are several methods of running applications that need PulseAudio on systems where PulseAudio is not the primary sound system or even is not installed.\nConfiguring PulseAudio to use JACK backend\nIt’s possible to configure PulseAudio to use JACK backend (that has limited functionality and larger latency) instead of ALSA backend.\nThe typical use case for this method is to run PulseAudio applications that don’t support JACK (like Skype) on a system that uses JACK, without switching the entire system to PulseAudio.\nConfiguring PulseAudio as a “dumb pipe” for ALSA\ndmix and dsnoop are ALSA virtual devices that implement software mixing and support sharing the same device between multiple applications.\nIt’s possible to configure PulseAudio to provide single sink and source attached to dmix and dsnoop devices without creating sinks and sources for hardware ALSA devices. See instructions here and here.\nThe typical use case for this method is to run PulseAudio applications that don’t support ALSA (like Skype) on a system that uses ALSA, without switching the entire system to PulseAudio.\nEmulating PulseAudio on top of ALSA\nThe apulse wrapper tool uses the LD_PRELOAD trick to implement libpulse and libpulse-simple API directly on top of ALSA (libasound).\nThe typical use case for this method is to run PulseAudio applications that don’t support ALSA on a system that uses ALSA, without even installing PulseAudio.\nBluetooth without PulseAudio\nThe BlueALSA (bluez-alsa) project implements virtual ALSA device that uses Bluez5 as a backend. This allows to play and record audio from Bluetooth devices with any software that supports ALSA.\nTemporary suspend PulseAudio pasuspender is a wrapper tool for applications that need exclusive access to ALSA devices.\nIt uses the device reservation API to ask PulseAudio server to release ownership of devices, runs the wrapped application, and returns the ownership when the application exits.\nThe typical use case for this method is to run JACK applications on a system that uses PulseAudio.\nServer internals This section provides a brief overview of PulseAudio server internals.\nComponents PulseAudio server consists of several logical components:\ndaemon\nDaemon is a top-level component that configures core and modules and starts the core main loop.\ncore\nCore provides building blocks and shared environment for modules. It is implemented in the libpulsecore library, which also uses libpulse and libpulsecommon.\nmodules\nModules are dynamically loaded libraries that extend server and implement many actual features, including network protocols, device drivers, sound processing tools, audio routing, etc. Modules use the libpulsecore library.\nCore The core provides building blocks and shared environment for modules:\nenvironment for modules\nShared environment for modules:\nmodule management name registry main loop hooks fundamental objects\nSkeleton implementation of the fundamental objects:\nmodule client card device port source source output sink sink input common functionality\nReusable parts common for multiple modules:\naudio processing stream management parsers and formatters codecs protocols utility functions\nNumerous general-purpose utility functions:\nmemory management collections message queues event loops threading I/O platform wrappers OOP helpers Modules A module is a dynamically loadable server extension. Usually, it is a shared library loaded at run-time, but it’s also possible to build it as a static library and link into the server at compile time.\nThere are several ways how a module may extend the server:\nregister new objects\nA module can get a skeleton implementation from the core (e.g. source or sink), extend it, create an instance, and register the instance in the name registry. The core and other modules may then use registered objects.\nSuch a module is usually loaded multiple times. The module arguments define the name and parameters of the object to create. For every new object, a new instance of the module is loaded.\nThis approach is used to implement hardware and network devices and sound processing tools.\nregister event loop handlers\nA module can monitor various external events, like a Udev event, a D-Bus signal, or socket I/O. Module registers I/O or timer event handler in the core event loop, and core invokes handlers when the event is fired.\nSuch a module may be loaded either once or multiple times depending on the implementation, e.g. one module instance for every socket address to be monitored.\nThis approach is used to implement device hotplug, network publishing and discovery, and servers for various protocols, including the D-Bus API, the “native” protocol used in the C API, and the CLI protocol used in command line tools.\nregister hooks and subscriptions\nA module can register hooks or subscribe events. The module monitors created, removed, or modified objects or other events, and implements some behavior.\nSuch a module is usually loaded only once.\nThis approach is used to implement various automatic actions like routing new streams, automatically saving and restoring object parameters to the database, autoloading filters, etc.\nregister protocol and API extensions\nA module can register extensions for the “native” protocol and the C API, as well as for the D-Bus API. Clients may then use these extensions to communicate with the module.\nThis approach is used to provide an API to manage the restoration database and setup custom parameters for some sound processing tools.\nObjects There are two base types that are used to implement objects:\nobject\nObject (pa_object) is the base type for reference-countable objects. It implements reference counting, virtual destructor, and dynamic type checks and casts.\nUsage:\nmessage object device port message object\nMessage object (pa_msgobject) is the base type for objects that can receive messages. It extends pa_object, so provides all its features, and adds message handling.\nUsage:\ncore source source output sink sink input network connections streams Registries The core provides two global registries accessible in modules:\nname registry\nThe name registry (pa_namereg) is a global hashmap that contains objects of the following types:\ncard\nsource\nsink\nsample cache entry\nThese objects are always added to the name registry when they’re created, and modules can access them by name.\nshared properties\nThe shared property subsystem (pa_shared) is a global hashmap that contains arbitrary data. Modules use it to register objects that may be accessed by name in related modules or other instances of the same module.\nHooks and subscriptions Hooks are an internal notification mechanism. Hooks may be provided both by core and modules:\nthe core provides hooks for registered objects (e.g. sources and sinks) the core provides hooks for the “native” and D-Bus protocol events Bluetooth backend provides hooks for driver events Object hooks can be roughly divided into the four categories:\nobject is created or removed object is fully initialized object state or properties are changed, e.g. the property list or volume is updated object connections are changed, e.g. a stream is moved to another device Subscription events are an alternative notification mechanism for registered objects (e.g. sources and sinks). They may be used both internally and externally via the C API:\nmodule or client subscribes events by a mask core triggers event when an object is created, removed, or modified Properties PulseAudio defines numerous properties that may be set for objects registered in the core. See ApplicationProperties wiki page and the Doxygen documentation with the full list of properties.\nA property has a textual name and textual or binary value. The value format depends on the property name. Properties are organized into property lists. The following objects have a property list:\nmodule client card device port source source output sink sink input sample cache entry format info The properties are accessible in the core, in modules, and in clients through the C API and D-Bus API. They are set both by the server and applications. When an application that uses libpulse connects to the server or creates a stream, libpulse automatically sets some client and stream properties from environment variables and process attributes.\nApplications typically specify properties when creating a context or stream object. Context properties are used for the corresponding server-side client object, and stream properties are used for the corresponding server-side source output or sink input object.\nThere are also two cases when properties are inherited:\nwhen a sink input or source output is created for a “native” protocol stream, it inherits properties of the client that owns the stream\nwhen a sink input is created for to play a sample cache entry, it inherits properties of the entry\nMost properties are used to provide various auxiliary meta-information to applications, like description and icons that may be displayed in GUI. Some properties are used for automatic actions like filter autoloading and routing.\nProperties are grouped into property classes. The table below summarizes them.\nclass\ndescription\nset for\nset by\nused by\n\u0026ldquo;module.*\u0026rdquo;\nmodule meta-information\nmodule\nmodule loader\nGUIs\n\u0026ldquo;application.*\u0026rdquo;\napplication and process attributes\nclient\napplications, libpulse\nGUIs, autodetecting properties, automatic actions, routing\n\u0026ldquo;window.*\u0026rdquo;\ndesktop window attributes\nclient\napplications, libpulse\nposition event sounds module\n\u0026ldquo;device.*\u0026rdquo;\ncard, device, and device port attributes\ncard, device port, source, sink\ndevice backends, sound processing modules\nGUIs, automatic actions, routing\n\u0026ldquo;media.*\u0026rdquo;\nmultimedia attributes of a stream\nsource output, sink input\napplications, sound processing modules\nGUIs, automatic actions, routing, device backends\n\u0026ldquo;event.*\u0026rdquo;\nevent sound stream attributes\nsource output, sink input\napplications, sample cache\nsample cache, position event sounds module\n\u0026ldquo;filter.*\u0026rdquo;\nsound processing filters for a stream\nsource output, sink input\napplications\nfilter autoloading\n\u0026ldquo;format.*\u0026rdquo;\nsample format attributes\nformat info of source, sink, source output, or sink input\napplications, device backends\napplications, device backends\nThreads There are two types of threads used in PulseAudio server:\nevent loop threads\nThese threads run an event loop (pa_mainloop) which handles I/O and timer events, including asynchronous messages from other threads.\nThe mainloop API hides the execution flow control from the thread implementer, which just registers event handlers and starts the loop.\nIO threads\nThese threads run a select/poll-like loop (pa_rtpoll) which handles I/O and timer events, including asynchronous messages from other threads.\nThe rtpoll API leaves the execution flow control to the thread implementer, who may decide how much and when to sleep, what file descriptors to handle and when, when to handle pending messages, etc.\nPulseAudio server creates the following threads:\ncore event loop\nThe main thread runs the core main loop. Most modules register handlers in the core main loop. For example, the core main loop is used in hotplug and network modules to listen to Udev events (to detect ALSA cards), to listen to D-Bus events (to detect Bluetooth devices or receive events from JACK), to listen to broadcast announcements (to detect RTP receivers), and to handle client connections.\navahi event loop\nPulseAudio creates a separate threaded event loop which runs an Avahi client and handles asynchronous messages from other threads.\nclient event loops\nWhen PulseAudio server acts as a client to another remote PulseAudio server, it runs a separate thread with a client main loop. This is used in the tunnel source and sink implementation.\ndevice IO threads\nEvery source and sink, except filter sources and sinks, have its own IO thread which reads or writes samples, implements clocking and maintains latency.\nThe communication between threads is done via messages. Each thread installs a thread message queue (pa_thead_mq which uses pa_asyncmsgq) to handle asynchronous messages for message objects (pa_msgobject).\nThere are three types of message objects:\nSources and sinks, except filter sources and sinks, are message objects with a dedicated IO thread, which handles both IO and asynchronous messages.\nFilter sources and sinks don’t have a dedicated thread. They are running inside the IO thread of the master source or sink.\nOther message objects, including source outputs and sink inputs, don’t have a dedicated thread as well. They all share the core event loop thread which handles asynchronous messages sent to them.\nMemory The memory management in PulseAudio is based on the following concepts:\nreference counting\nPulseAudio extensively uses reference counting. In particular, all objects (pa_object), message objects (pa_msgobject), memory blocks (pa_memblock), and many other types are reference-counted.\nblocks\nA block (pa_memblock) is a fixed-size reference-counted array in memory, usually, but not necessary, allocated from a memory pool. Usually, blocks are used indirectly via chunks.\nchunks\nA chunk (pa_memchunk) is a variable-size slice of a block. It consists of a pointer to a block, a starting offset in the block, and a length. Chunks are not reference-counted. They are usually allocated on stack or inside other objects. Most of the sample processing and exchange in done using chunks.\nblock queues\nA block queue (ps_memblockq) is a FIFO of chunks. It allows to push and pop chunks and implements various flow control operations. Sample streams are based on the block queues.\npools\nA pool (pa_mempool) is the most common way to allocate blocks.\nPools may use either private memory, POSIX shared memory, or memfd shared memory. The shared memory pools combined with the memory exports and imports are used to implement the zero-copy mode.\nPools may be either global or per-client. The per-client pools guarantee that only the owner client is able to access its memory.\nexports and imports\nMemory exports (pa_memexport) allow to make blocks accessible from other processes. Memory imports (pa_memimport) allow to access the exported blocks.\nThe exporting process allocates a block in a shared memory pool, fills it, and communicates the shared memory id and the block id to the importing process. The importing process opens the shared memory, finds the block, and uses it.\nWhen either the exporting process revokes the block or the importing process releases the block, it is returned to the memory pool.\nI/O PulseAudio server uses the following I/O APIs:\nmainloop\nThe event loop API (pa_mainloop_api) is used to handle I/O and timer events using callbacks. The user registers callbacks and runs the loop.\nThere are three implementations of the mainloop API: a regular event loop (pa_mainloop), a threaded event loop (pa_threaded_mainloop), which runs an event loop in a separate thread, and a Glib event loop (pa_glib_mainloop), which runs an event loop on top of the Glib event loop.\nMost of the I/O, with a notable exception of the device IO threads, is based on the mainloop API.\nrtpoll\nThe rtpoll API (pa_rtpoll) is used to handle I/O and timer events using a select/poll-like loop.\nThe user registers descriptors and manually runs the loop iterations. The user fully controls the flow and decides when to perform I/O and when and how much to sleep.\nThe device threads are based on the rtpoll API.\nsrbchannel\nA shared ring buffer channel (pa_srbchannel) is a bidirectional byte stream on top of two ring buffers in the shared memory, two file descriptor-based semaphores (using POSIX pipe or Linux-specific eventfd), and an event loop.\nThe user registers a callback that is called when the descriptor is ready, and performs non-blocking read or write operations.\nThe “native” protocol streams employ this channel to exchange control commands and chunk identifiers in the zero-copy mode, when the client and server are running on the same host and use a shared memory pool.\niochannel\nAn I/O channel (pa_iochannel) is a bidirectional byte stream on top of a socket file descriptor and an event loop.\nThe user registers a callback that is called when the descriptor is ready, and performs non-blocking read or write operations. In addition to the regular data, the user may also send and receive file descriptors and user credentials, if a Unix domain socket is used.\nBinary client streams, like the “native”, “simple”, and ESound streams, are based on this channel, except when using the zero-copy mode of the “native” protocol.\nioline\nAn I/O line (pa_ioline) is a bidirectional line-oriented text stream on top of the I/O channel (pa_iochannel). The user registers callback that is called for every received line and sends and receives data line-by-line.\nText client streams, like the CLI and HTTP streams, are based on this channel.\nPackets The “native” protocol is implemented on top of packets, packet stream, and packet dispatcher:\npacket\nPacket (pa_packet) is a fixed-size reference-countable blob with data. Packets are usually allocated from a global per-process pool.\npacket stream\nPacket stream (pa_pstream) is a bidirectional message stream on top of an event loop (pa_mainloop), an I/O channel (pa_iochannel), and a memory pool (pa_mempool).\nThe packet stream may optionally employ memory exports (pa_memexport), memory imports (pa_memimport), and a shared ring buffer channel (pa_srbchannel) if the zero-copy mode is enabled.\nThe user may send and receive packets (pa_packet), chunks (pa_memchunk), and control messages (shmrelease, shmrevoke). All these message types are used by the “native” protocol.\npacket dispatcher\nPacket dispatcher (pa_pdispatch) looks up and invokes a callback for a packet. The user first registers callbacks for commands in the dispatcher, and then passes incoming packets to it.\nAudio files Normally, it’s up to the client application to read or write audio files, and both PulseAudio server and client libraries deal only with sample streams. However, there are two cases when the server can read an audio file directly:\nthe client may ask the server to load an audio file to the sample cache the client may ask the server to play an audio file In the latter case, the server creates a new sink input that reads samples from the file. Both features are available through the CLI protocol. The server uses libsndfile to read audio files.\nOptimizations Depending on the target platform, PulseAudio may employ various optimized versions of the sample conversion and software volume functions.\nThey include several functions written in the GCC inline assembly and employing MMX (x86), SSE (x86), or NEON (arm) instructions, and several functions written in the Orc assembly. The latter may be compiled at run-time for the current CPU.\nWatchdog PulseAudio server has a built-in watchdog based on the POSIX rlimit feature. It terminates the server process if it consumes too much CPU load and doesn’t respond in time. The server may be then automatically started by a client if the autospawn feature is enabled.\nThe server configures the RLIMIT_CPU timer which has the soft and hard limits:\nWhen the CPU time of the server process reaches the soft limit, the kernel sends to the process the SIGXCPU signal.\nThe signal handler checks if the server consumed too high percent of the CPU load since the previous signal handler invocation. If so, the signal handler terminates the server. Otherwise, it restarts the RLIMIT_CPU timer.\nWhen the CPU time of the server process reaches the hard limit, the kernel sends to the process the SIGKILL signal.\nThis signal can’t be handled and unconditionally terminates the process. This happens only if the server failed to handle the previous SIGXCPU signal and didn’t restart the RLIMIT_CPU timer before it reached the hard limit.\nModule list The tables below provide a brief summary of modules available out of the box, grouped by categories. Further details may be found on the Modules page on wiki.\nProtocols and networking module\nusage\ndescription\nmodule-cli-protocol-{unix,tcp}\nmodule-cli\nenabled by default\nStarts the CLI protocol server over a Unix domain socket, TCP socket, or the controlling TTY of the daemon.\nmodule-dbus-protocol\nenabled by default\nStarts the D-Bus protocol server.\nmodule-native-protocol-{fd,unix,tcp}\nenabled by default\nStarts the \u0026ldquo;native\u0026rdquo; protocol server over a preopened file descriptor, Unix domain socket, or TCP socket.\nmodule-simple-protocol-{unix,tcp}\nrarely used\nStarts the \u0026ldquo;simple\u0026rdquo; protocol server over a Unix domain socket or TCP socket.\nmodule-esound-protocol-{unix,tcp}\nStarts the ESound protocol server over a Unix domain socket or TCP socket.\nmodule-tunnel-{source,sink}\nloaded by another module\nCreates a source or sink connected to a remote source or sink via the \u0026ldquo;native\u0026rdquo; protocol (implements client from scratch).\nmodule-tunnel-{source,sink}-new\nwork in progress\nCreates a source or sink connected to a remote source or sink via the \u0026ldquo;native\u0026rdquo; protocol (reuses client from libpulse).\nmodule-zeroconf-discover\nenabled in paprefs\nListens to mDNS announcements and automatically loads module-tunnel-{source,sink} for every remote source or sink (uses Avahi).\nmodule-zeroconf-publish\nenabled in paprefs\nSends mDNS announcements for all local sources and sinks (uses Avahi).\nmodule-bonjour-publish\nfor MacOS\nSends mDNS announcements for all local sources and sinks (uses Apple Bonjour).\nmodule-raop-sink\nloaded by another module\nCreates a sink that forwards audio to a remote AirPlay1 device.\nmodule-raop-discover\nenabled in paprefs\nListens to mDNS announcements and automatically loads module-raop-sink for every remote AirPlay1 device (uses Avahi).\nmodule-rtp-recv\nenabled in paprefs\nListens to SDP/SAP announcements and automatically creates an RTP sink input for every detected RTP sender.\nmodule-rtp-send\nenabled in paprefs\nCreates RTP source output that sends samples to a preconfigured address, and broadcasts SDP/SAP announcements for it.\nmodule-http-protocol-{unix,tcp}\nused by other modules\nStarts an HTTP server over a Unix domain socket or TCP socket. Implements a web interface and HTTP streaming for sources and sink monitors.\nmodule-rygel-media-server\nenabled in paprefs\nRegisters a plugin for the Rygel DLNA / UpNP server. Publishes HTTP streams of sources and sink monitors.\nDevice drivers module\nusage\ndescription\nmodule-udev-detect\nenabled if available\nListens to Udev events and automatically loads module-alsa-card for every ALSA card.\nmodule-hal-detect\ndeprecated\nLoads module-udev-detect.\nmodule-detect\nfor systems without Udev\nOn start, detects ALSA, OSS, Solaris, and WaveOut devices and loads appropriate source and sink modules.\nmodule-alsa-card\nloaded by another module\nCreates a card for an ALSA card, and automatically creates sources and sinks for inner ALSA devices.\nmodule-alsa-{source,sink}\nCreates a source or sink for an ALSA device.\nmodule-bluetooth-discover\nenabled by default\nLoads either module-bluez5-discover or module-bluez4-discover.\nmodule-bluetooth-policy\nenabled by default\nAutomatically switches card profiles of Bluetooth cards and loads module-loopback to route and play music from new Bluetooth sources.\nmodule-{bluez5,bluez4}-discover\nloaded by another module\nListens to Bluez and oFono events on D-Bus and automatically loads module-{bluez5,bluez4}-device for every device.\nmodule-{bluez5,bluez4}-device\nloaded by another module\nCreates a card, source, and sink for a Bluetooth device.\nmodule-jackdbus-detect\nListens to JACK events on D-Bus and automatically loads module-jack-{source,sink} when JACK is started.\nmodule-jack-{source,sink}\nloaded by another module\nCreates a source or sink that read and write samples to JACK.\nmodule-oss\nfor systems with OSS\nCreates a source and sink for an OSS device (/dev/dspN).\nmodule-solaris\nfor Solaris and some *BSD\nCreates a source and sink for a Sun audio device (/dev/audio).\nmodule-coreaudio-detect\nfor MacOS\nListens to CoreAudio events and automatically loads module-coreaudio-device for every CoreAudio device.\nmodule-coreaudio-device\nfor MacOS\nCreates a source and sink for a CoreAudio device.\nmodule-waveout\nfor Windows\nCreates a source and sink for Win32 WaveIn/WaveOut devices.\nmodule-esound-sink\nhas latency issues\nCreates a sink connected to the ESound daemon.\nmodule-lirc\nListens to LIRC events from an IR remote control and forward volume up/down and mute requests to a preconfigured sink.\nmodule-mmkbd-evdev\nListens to evdev events from the multimedia keyboard and forward volume up/down and mute requests to a preconfigured sink.\nSound processing module\nusage\ndescription\nmodule-loopback\nused by other modules\nCreates a pair of virtual source output and sink input connected with a queue, may be used as a source-to-sink adapter.\nmodule-null-source\nCreates a virtual source that always produces silence.\nmodule-null-sink\nCreates a virtual sink that silently drops all data. Together with its monitor, may be used as a source-output-to-sink-input adapter.\nmodule-combine-sink\nenabled in paprefs\nCreates a virtual sink that duplicates data to several other sinks.\nmodule-combine\ndeprecated\nLoads module-combine-sink.\nmodule-sine-source\nmodule-sine\nCreates a virtual source or sink input that generates a sine wave with the preconfigured frequency.\nmodule-pipe-{source,sink}\nCreates a virtual source or sink that reads or writes data to a preconfigured file or named pipe.\nmodule-remap-{source,sink}\nautoloadable filter\nCreates a filter source or sink that performs channel remapping on top of the master source or sink.\nmodule-equalizer-sink\nautoloadable filter\nCreates a filter sink that implements a digital equalizer on top of the master source or sink. The equalizer may be controlled via D-Bus.\nmodule-virtual-surround-sink\nautoloadable filter\nCreates a filter sink that performs a convolution with a HRIR WAV file on top of the master source or sink.\nmodule-virtual-{source,sink}\nautoloadable filter\nCreates a filter source or sink that just reads or writes data to the master source or sink.\nmodule-echo-cancel\nautoloadable filter\nCreates a paired filter source and sink that perform acoustic echo cancellation on top of the master source and sink.\nmodule-ladspa-sink\nautoloadable filter\nCreates a filter sink that applies an audio filter from an external LADSPA plugin on top of the master source or sink. The plugin may be controlled via D-Bus.\nPower saving module\nusage\ndescription\nmodule-suspend-on-idle\nenabled by default\nMonitors sources and sinks and automatically suspends them when there are no connected streams for some period of time.\nAutomatic setup and routing module\nusage\ndescription\nmodule-default-device-restore\nenabled by default\nSaves and restores the fallback source and sink.\nmodule-card-restore\nenabled by default\nSaves and restores active profiles of card.\nmodule-device-restore\nenabled by default\nSaves and restores volume/mute settings and active ports of devices.\nmodule-stream-restore\nenabled by default\nSaves and restores volume/mute settings and routing rules of stream groups. Routes new streams according to stored device name.\nmodule-volume-restore\ndeprecated\nLoads module-stream-restore.\nmodule-device-manager\nenabled in KDE\nStores per-role and global priority lists of devices. Routes new streams according to the stored priority lists.\nmodule-intended-roles\nenabled by default\nRoutes new streams according to the stream role and device intended role list.\nmodule-augment-properties\nenabled by default\nComputes client properties from the desktop file of the application.\nmodule-filter-heuristics\nenabled by default\nAdjusts the filters requested via the stream properties.\nmodule-filter-apply\nenabled by default\nLoads the filters requested via the stream properties.\nmodule-switch-on-port-available\nenabled by default\nWhen the availability of device ports changes, automatically switch the active device port and card profile.\nmodule-switch-on-connect\nWhen a new device appears, automatically sets it as the fallback device and move all active streams to it.\nmodule-rescue-streams\nenabled by default\nWhen a device disappears, automatically moves streams connected to it to another working device.\nmodule-always-sink\nenabled by default\nWhen all sinks disappear, automatically loads the null sink.\nmodule-role-{ducking,cork}\nenabled by default\nWhen an important stream is started, automatically ducks (lower the volume) or corks (mute and request a pause) active streams.\nmodule-allow-passthrough\nWhen a new passthrough stream is moved to an existing sink, automatically creates a null sink and moves all other running streams to that null sink.\nmodule-position-event-sounds\nenabled by default\nWhen a stream is created or its properties are changed, adjusts the volume balance of the stream depending on its on-screen 2-D coordinated provided by the application.\nmodule-match\nWhen a new stream appears, automatically updates its volume based on its name and a preconfigured match table.\nDesktop integrations module\nusage\ndescription\nmodule-systemd-login\nenabled by default\nListens to logind events on D-Bus and creates a fake PulseAudio client for every new login session of the current user to prevent server from exiting until the user logouts.\nmodule-console-kit\nfor systems w/o systemd\nListens to ConsoleKit events on D-Bus and creates a fake PulseAudio client for every new login session of the current user to prevent server from exiting the user logouts.\nmodule-x11-xsmp\nenabled by default\nListens to X session manager events and creates a fake PulseAudio client for the current login session to prevent server from exiting the user logouts.\nmodule-x11-publish\nenabled by default\nPublishes server address and credentials via the X11 root window properties, which may be read by clients running on the same X display, including remote clients that use SSH X forwarding.\nmodule-x11-bell\nenabled by default\nIntercepts XKB bell events and plays a preconfigured sample from the sample cache instead of the default X11 bell.\nmodule-x11-cork-request\nenabled by default\nWhen an (un)cork is requested for a stream, synthesizes X11 media key event and sends it to the application. A workaround for applications that don\u0026rsquo;t handle cork requests, but do handle multimedia keys.\nmodule-gconf\nenabled by default\nMonitors GConf directory and automatically loads PulseAudio modules listed there.\nCompatibility layers module\nusage\ndescription\nmodule-esound-compat-spawnpid\nmodule-esound-compat-spawnfd\nloaded by esdcompat\nNotifies the process that started the server that the server was successfully started with a signal or via a file descriptor.\nGUI tools This section provides a brief summary of the three standard GUI tools for PulseAudio. Besides these tools, most desktop environments also provide their own tools or applets.\npavucontrol pavucontrol (PulseAudio Volume Control) provides the following features:\nsetup volumes or mute sources, source outputs, sinks, and sink inputs connect sink inputs to sinks connect source outputs to sources set fallback source and sink set active device port of a source or sink set active card profile of a card configure latency offset and passthrough mode of a source or sink This tool uses the C API to communicate with the server. PulseAudio automatically saves most of these setting to the restoration database, so they are persistent.\npaprefs paprefs (PulseAudio Preferences) provides methods to enable modules or module options that are disabled by default.\nThis tool just writes module names and arguments to the GNOME registry (GConf) and PulseAudio automatically loads the modules. These settings are persistent.\nqpaeq qpaeq (Qt PulseAudio Equalizer) is a frontend for the equalizer sink.\nThis tool communicates with the server through the D-Bus API. It first creates an equalizer sink connected to a selected master sink, and then uses the D-Bus API extension registered by the equalized sink.\nThe number of sliders depends on the window size, so a larger window gives a higher precision.\nCommand line tools PulseAudio package comes with several command line tools.\nServer pulseaudio\npulseaudio tool starts or kills the server.\nThe user may specify what configuration files to use, what modules to load and from where, configure log levels, and some other options.\nClients pacmd\npacmd tool starts an interactive session for server configuration.\nIt connects to the server via the CLI protocol over a Unix domain socket. This text protocol provides a variety of commands to inspect and configure the server. The tool redirects its stdin and stdout to the socket so that the user directly communicates with the server.\nThe pacmd tool doesn’t work over the network or when the server is running in system mode and doesn’t support autospawn. Users are encouraged to use the pactl tool instead.\npactl\npactl tool implements non-interactive commands for server configuration.\nIt communicates with the server via the C API, which uses the “native” protocol internally. The tool understands a number of commands which should be specified via the command line arguments. It supports most of the features available in the CLI protocol.\npacat\npacat tool implements a playback and recording client.\nThe paplay, parecord, parec, and pamon tools are symlinks to the pacat tool. The tool has four operation modes. The mode is determined by the symlink used to invoke the tool:\npaplay\nCreate a playback stream, read and decode samples from an audio file using libsndfile, and send samples to the stream. parecord\nCreate a recording stream, receive samples from the stream, and encode and write them to a file using libsndfile. pacat\nCreate a playback stream, read raw samples from stdin, and send samples to the stream. parec or pamon\nCreate a recording stream, receive samples from the stream, and write raw samples to stdout. Desktop start-pulseaudio-x11\nstart-pulseaudio-x11 tool starts the server for the current X11 session.\nIt relies on the autospawn feature to start the server. The tool loads several desktop-related modules, passing them the $DISPLAY and $SESSION_MANAGER environment variables.\npax11publish\npax11publish tool publishes the server address and credentials via the X11 root window properties.\nThe tool sets X11 properties that may be used by PulseAudio clients to connect to the server. The server address and credentials should be manually provided by the user via command line arguments. The tool is now superseded by the module-x11-publish and start-pulseaudio-x11 tool.\nCompatibility pasuspender\npasuspender tool is a wrapper for applications that require an exclusive access to devices.\nThe tool connects to the server via the C API and asks the server to suspend all sinks and sources. Then it runs a child process, that may freely use any devices, typically ALSA devices. When the child exits, the tool asks the server to resume sinks and sources.\npadsp\npadsp tool is a wrapper for OSS applications.\nThe tool intercepts standard library functions of an OSS application using the LD_PRELOAD trick and redirects sound to PulseAudio. This approach works for some applications but is known to be incomplete. The ossp daemon with the ossp-padsp backend may be a better alternative for this tool.\nesdcompat\nesdcompat emulates ESound autospawn feature.\nThe tool uses PulseAudio autospawn to start PulseAudio server and loads ESound compatibility modules that emulate ESound autospawn. The tool takes the same arguments as the ESound daemon so it can be used as a drop-in replacement for the esd tool.\nConfiguration component\nlibpulse\nlibpulsecore\nPulseAudio configuration is documented on the User and the FAQ pages on wiki.\nThe Arch Linux wiki may be also useful: 1, 2, 3, 4.\nSystem and user modes There are two ways to run the server:\none instance per-user one system-wide instance The system-wide mode is not recommended but is useful in some cases like an embedded system. See the SystemWide and WhatIsWrongWithSystemWide pages on wiki.\nThe major differences between the two modes are the following:\nIn the system-wide mode, the most of the desktop integration features, like session management and X11 publishing, are not necessary and can’t be used since the server is not bound to a login session or X11 display.\nIn the per-user mode, server instances use the device reservation API to acquire exclusive access on ALSA devices. In the system-wide mode, this API is not used.\nIn the per-user mode, PulseAudio system directories are shared between server instances, and PulseAudio user directories belong to the user. In the system-wide mode, PulseAudio user directories belong to the “pulse” user.\nFrom the usability and security points of view, the server is not designed to be shared by multiple users. There is no user separation inside the server. In particular:\nAny connected client can read and control streams or devices used by another client. Any connected client can affect global configuration, like device hotplug.\nAll connected clients share the same persistent state, including the default source and sink, the restoration database, etc.\nSystem directories PulseAudio uses the following system-wide directories:\nconfig directory\nHolds system-wide configuration files.\nUsually set to \u0026quot;/etc/pulse\u0026quot;.\nContains .conf and .pa configuration files.\nmodule directory\nHolds dynamically loadable libraries for server modules.\nUsually set to \u0026quot;/usr/lib/pulse-{VERSION}/modules\u0026quot;.\nContains \u0026quot;module-{NAME}.so\u0026quot; files.\ndata directory\nHolds platform-independent server data.\nUsually set to \u0026quot;/usr/share/pulseaudio\u0026quot;.\nContains \u0026quot;alsa-mixer\u0026quot; directory with ALSA profiles for non-UCM ALSA cards. See Profiles page on wiki.\nUser directories PulseAudio uses the following per-user directories (belonging to the “pulse” user in the system-wide mode):\nhome directory\nDefault or parent directory for the configuration files, persistent state, and runtime state of the user’s server instance.\nUsually set to \u0026quot;$XDG_CONFIG_HOME/pulse\u0026quot; (typically \u0026quot;~/.config/pulse\u0026quot;). However, if the \u0026quot;$HOME/.pulse\u0026quot; directory already exists, it’s used otherwise.\nContains:\na cookie file (for the “native” protocol authentication) usually, contains configuration files, if the config directory is the same as the home directory usually, contains persistent state, if the state directory is the same as the home directory may contain a symlink to the runtime directory config directory\nHolds per-user configuration files that override global configuration.\nUsually, this is the same directory as the home directory. However, another path may be specified via the $PULSE_CONFIG_PATH environment variable.\nContains .conf and .pa configuration files.\nstate directory\nHolds per-user persistent state that is changed dynamically and should be kept across reboots.\nUsually, this is the same directory as the home directory. However, another path may be specified via the $PULSE_STATE_PATH environment variable.\nContains:\ndefault source and sink names restoration database device manager database equalizer sink database runtime directory\nHolds per-user runtime state that should be cleared when the server restarts.\nIf the $PULSE_RUNTIME_PATH environment variable is set, it specifies the path of the runtime directory. Otherwise, if the $XDG_RUNTIME_DIR environment variable is set, the runtime directory path is set to \u0026quot;$XDG_RUNTIME_DIR/pulse\u0026quot; instead (typically somewhere in \u0026quot;/run\u0026quot;).\nIf non of these environment variables are set, the runtime directory is created in \u0026quot;/tmp\u0026quot;, and a symlink to it is created in the home directory. The symlink name includes the machine id.\nContains:\nsockets files pid file lock files Configuration files PulseAudio uses two types of configuration files:\n.conf - client or server options in a simple key-value format .pa - server initialization commands in the CLI protocol format Four files are used by default:\ndaemon.conf - server options client.conf - client options default.pa - server initialization for the per-user mode system.pa - server initialization for the system-wide mode If the user config directory contains .conf files, the system .conf files with the same name are ignored.\nSockets PulseAudio uses two types of sockets:\nUnix domain sockets (for local clients) TCP sockets (for remote clients) The table below lists the non-standard TCP ports used by PulseAudio.\nmodule\nport\nmodule-native-protocol-tcp\n4713\nmodule-simple-protocol-tcp\n4711\nmodule-http-protocol-tcp\n4714\nmodule-cli-protocol-tcp\n4712\nClient startup Every client that uses libpulse performs the following steps at startup:\ninitialize\nFirst, the client reads the server address, the authentication cookie, and other options from:\nenvironment variables X11 root window properties per-user and system-wide client configuration files connect\nWhen the initialization is done, the client tries to connect to the following addresses:\na Unix socket for the “native” protocol in the per-user and system-wide runtime directories a TCP socket on localhost a TCP socket on the host defined by the $DISPLAY environment variable autospawn\nIf the client can’t connect to the server, it automatically starts the server if all of the following is true:\nthe autospawn is not disabled in the client configuration file the client is not running under the root user the server address is not set, or it’s set and belongs to the same login session as the client authenticate\nWhen the client has connected to the server, it tries to read the authentication cookie from:\nenvironment variables X11 root window properties a cookie file explicitly provided by the application per-user and system-wide client configuration files per-user home directory Portability PulseAudio may work on several POSIX-compatible platforms. See About and Ports pages on wiki.\nCurrently supported operating systems are:\nLinux Android *BSD Solaris Mac OSX Windows PulseAudio core implements platform wrappers for low-level stuff like threading, networking, I/O, and shared memory. The rest code is mostly cross-platform.\nHowever, some important features rely on platform-specific external components:\nThe only full-featured hardware backends are ALSA and Bluetooth, which a both Linux-specific.\nThe timer-based scheduler is implemented only for ALSA cards, hence it’s also Linux-specific.\nMost desktop integration features depend on freedesktop and Unix or Linux-specific components which are not used on Android, Mac OSX, and Windows.\nHardware controls handling also depend on Unix or Linux-specific components which are not used on Mac OSX and Windows.\nIn result, Linux is the only platform on which all of the important features are supported. Other Unix desktops are supported but have limited functionality. Some features rely on the Linux and Unix desktop stack, therefore non-Unix desktop support is even more limited.\nExample setups This section demonstrates example PulseAudio configurations for several common and advanced use cases.\nPlayback and recording Connect a recording application to a source\nAn application connects to the server via the “native” protocol and creates a recording stream. The server creates a source output for the stream. The source output is connected to a source.\nConfiguration:\nStart the recording application:\nparecord output.wav Connect the parecord source output to the source using the pavucontrol tool.\nConnect a playback application to a sink\nAn application connects to the server via the “native” protocol and creates a playback stream. The server creates a sink input for the stream. The sink input is connected to a sink.\nConfiguration:\nStart the playback application:\npaplay input.wav Connect the paplay sink input to the sink using the pavucontrol tool.\nConnect a playback application to a filter sink\nAn application connects to the server via the “native” protocol and creates a playback stream. The server creates a sink input for the stream. The sink input is connected to a filter sink, which in turn is connected to the master sink.\nConfiguration:\nCreate an equalizer sink and sink input:\npactl load-module module-dbus-protocol qpaeq Connect the equalizer sink input to the master sink using the pavucontrol tool.\nStart the playback application:\npaplay input.wav Connect the paplay sink input to the equalizer sink using the pavucontrol tool.\nCapturing sound Connect a sink to a file\nThe monitor source of a sink is connected to a loopback, which in turn is connected to a pipe sink (because it’s not possible to connect a source to a sink directly). The pipe sink writes samples to a file.\nConfiguration:\nCreate the pipe sink:\npactl load-module module-pipe-sink file=\u0026quot;$(pwd)/output.pcm\u0026quot; Create the loopback sink input and source output:\npactl load-module module-loopback Connect the loopback sink input to the pipe sink using the pavucontrol tool.\nConnect the loopback source output to the sink monitor of a sink using the pavucontrol tool.\nWhen the capture is done, play the recorded file:\npacat output.pcm Connect a playback application to a file\nAn application connects to the server via the “native” protocol and creates a playback stream. The server creates a sink input for the stream. The sink input is connected to a pipe sink. The pipe sink writes samples to a file.\nConfiguration:\nCreate the pipe sink:\npactl load-module module-pipe-sink file=\u0026quot;$(pwd)/output.pcm\u0026quot; Start the playback application:\npaplay input.wav Connect the paplay sink input to the pipe sink using the pavucontrol tool.\nWhen the capture is done, play the recorded file:\npacat output.pcm Connect a playback application to a recording application\nTwo applications connect to the server via the “native” protocol and create playback and recording streams. The server creates sink input and source output for the streams. The sink input is connected to the source output via a null sink (because it’s not possible to connect a sink input to a source output directly).\nConfiguration:\nCreate a null sink:\npactl load-module module-null-sink Start the playback application:\npaplay input.wav Start the recording application:\nparecord output.wav Connect the paplay sink input to the null sink using the pavucontrol tool.\nConnect the parecord source output to the monitor of the null sink using the pavucontrol tool.\nNative protocol Connect a local playback application to a remote sink\nAn application connects to the local server via the “native” protocol and creates a playback stream. The local server creates a sink input for the stream. The local sink input is connected to a tunnel sink. The tunnel sink connects to the remote server via the “native” protocol and creates a playback stream. The remote server creates a sink input for the stream. The remote sink input is connected to a sink.\nConfiguration:\nEnable Zeroconf publishing on the remote server using the paprefs tool or the following commands:\n(start avahi daemon) pactl load-module module-native-protocol-tcp auth-anonymous=1 pactl load-module module-zeroconf-publish Enable Zeroconf discovery on the local server using the paprefs tool or the following commands:\n(start avahi daemon) pactl load-module module-zeroconf-discover Wait until the tunnel sinks and sources appear on the local server.\nStart the playback application on the local server:\npaplay input.wav Connect the local sink input to the tunnel sink using the pavucontrol tool on the local server.\nConnect the remote sink input to a sink using the pavucontrol tool on the remote server.\nConnect a local playback application to a remote recording application\nThe local application connects to the local server via the “native” protocol and creates a playback stream. The local server creates a sink input for the stream. The remote application connects to the remote server via the “native” protocol and creates a recording stream. The remote server creates a source output for the stream.\nThe local sink input is connected to a tunnel sink. The tunnel sink connects to the remote server via the “native” protocol and creates a playback stream. The remote server creates a sink input for the stream. The remote sink input is connected to the remote source output via a null sink (because it’s not possible to connect a sink input to a source output directly).\nConfiguration:\nEnable Zeroconf publishing on the remote server using the paprefs tool or the following commands:\n(start avahi daemon) pactl load-module module-native-protocol-tcp auth-anonymous=1 pactl load-module module-zeroconf-publish Enable Zeroconf discovery on the local server using the paprefs tool or the following commands:\n(start avahi daemon) pactl load-module module-zeroconf-discover Wait until the tunnel sinks and sources appear on the local server.\nStart the playback application on the local server:\npaplay input.wav Start the recording application on the remote server:\nparecord output.wav Create the null sink on the remote server:\npactl load-module module-null-sink Connect the local sink input to the tunnel sink using the pavucontrol tool on the local server.\nConnect the remote sink input to the null sink using the pavucontrol tool on the remote server.\nConnect the remote source output to the sink monitor of the null sink using the pavucontrol tool on the remote server.\nConnect a local sink to a remote sink\nThe monitor source of a sink is connected to a loopback, which in turn is connected to a tunnel sink (because it’s not possible to connect a source to a sink directly). The tunnel sink connects to the remote server via the “native” protocol and creates a playback stream. The remote server creates a sink input for the stream. The remote sink input is connected to a sink.\nConfiguration:\nEnable Zeroconf publishing on the remote server using the paprefs tool or the following commands:\n(start avahi daemon) pactl load-module module-native-protocol-tcp auth-anonymous=1 pactl load-module module-zeroconf-publish Enable Zeroconf discovery on the local server using the paprefs tool or the following commands:\n(start avahi daemon) pactl load-module module-zeroconf-discover Wait until the tunnel sinks and sources appear on the local server.\nCreate the loopback on the local server:\npactl load-module module-loopback Connect the loopback sink input to the tunnel sink using the pavucontrol tool.\nConnect the loopback source output to the sink monitor of a sink using the pavucontrol tool.\nConnect the remote sink input to the sink using the pavucontrol tool on the remote server.\nRTP Connect a local playback application to a remote sink\nAn application connects to the local server via the “native” protocol and creates a playback stream. The local server creates a sink input for the stream. The local sink input is connected to the RTP source output via a null sink (because it’s not possible to connect a sink input to a source output directly). The local RTP source output is connected to the remote RTP sink input via RTP. The remote RTP sink input is connected to a sink.\nConfiguration:\nEnable RTP receiver on the remote server using the paprefs tool or the following command:\npactl load-module module-rtp-recv sap_address=\u0026lt;IP\u0026gt; Enable RTP sender on the local server using the paprefs tool or the following command:\npactl load-module module-rtp-send destionation_ip=\u0026lt;IP\u0026gt; Create the null sink on the local server:\npactl load-module module-null-sink Connect the local sink input to the null sink using the pavucontrol tool on the local server.\nConnect the sink monitor of the null sink to the local RTP source output using the pavucontrol tool on the local server.\nConnect the remote RTP sink input to a sink using the pavucontrol tool on the remote server.\nConnect a local playback application to a remote recording application\nThe local application connects to the local server via the “native” protocol and creates a playback stream. The local server creates a sink input for the stream. The remote application connects to the remote server via the “native” protocol and creates a recording stream. The remote server creates a source output for the stream.\nThe local sink input is connected to the RTP source output via a null sink (because it’s not possible to connect a sink input to a source output directly). The local RTP source output is connected to the remote RTP sink input via RTP. The remote RTP sink input is connected to the remote source output via a null sink (because it’s not possible to connect a sink input to a source output directly).\nConfiguration:\nEnable RTP receiver on the remote server using the paprefs tool or the following command:\npactl load-module module-rtp-recv sap_address=\u0026lt;IP\u0026gt; Enable RTP sender on the local server using the paprefs tool or the following command:\npactl load-module module-rtp-send destionation_ip=\u0026lt;IP\u0026gt; Create the null sink on the local server:\npactl load-module module-null-sink Create the null sink on the remote server:\npactl load-module module-null-sink Connect the local sink input to the null sink using the pavucontrol tool on the local server.\nConnect the sink monitor of the null sink to the local RTP source output using the pavucontrol tool on the local server.\nConnect the remote RTP sink input to the null sink using the pavucontrol tool on the remote server.\nConnect the remote source output to the sink monitor of the null sink using the pavucontrol tool on the remote server.\nConnect a local sink to a remote sink\nThe sink monitor of a local sink is connected to the local RTP source output. The local RTP source output is connected to the remote RTP sink input via RTP. The remote RTP sink input is connected to a sink.\nConfiguration:\nEnable RTP receiver on the remote server using the paprefs tool or the following command:\npactl load-module module-rtp-recv sap_address=\u0026lt;IP\u0026gt; Enable RTP sender on the local server using the paprefs tool or the following command:\npactl load-module module-rtp-send destionation_ip=\u0026lt;IP\u0026gt; Connect the sink monitor of a sink to the local RTP source output using the pavucontrol tool on the local server.\nConnect the remote RTP sink input to a sink using the pavucontrol tool on the remote server.\nExample clients and modules This section provides several examples of client applications and server modules. The source code and usage instructions are available on GitHub.\nAlso, some analysis of the client examples is available in this post.\nDocumentation The following official documentation is available:\nOverview:\nClients - application developer documentation Developer - module developer documentation API documentation:\nC API D-Bus API Client examples:\nDoxygen Wiki Module examples:\nmodule-virtual-source module-virtual-sink D-Bus API This example is quite straightforward, so just look at the code.\npa_dbus_print\nPython3 script that prints various server-side objects using the D-Bus API.\nC API These examples are described in details in another article.\npa_play_simple\nMinimal playback client using the Simple API.\npa_play_async_cb\nPlayback client using the Asynchronous API, based on callbacks.\npa_play_async_poll\nPlayback client using the Asynchronous API, based on polling.\nModules These examples have comments in the source code.\npa_module_source\nThis module implements a source. The module runs a thread that takes care of the timing and periodically generates samples and writes them to the connected source outputs.\npa_module_source_output\nThis module implements a source output. The module provides callbacks invoked when a source generates more samples.\npa_module_sink\nThis module implements a sink. The module runs a thread that takes care of the timing and periodically requests samples from the connected sink inputs.\npa_module_sink_input\nThis module implements a sink input. The module provides callbacks invoked when a sink needs more samples.\nSee also:\nWriting a simple PulseAudio module. Critique Finally, I’d like to discuss some problems in the PulseAudio design and implementation that I’ve gathered while writing this document.\nWe’ll discuss only problems that are essential but yet solvable and could be avoided while still providing the same functionality to the user.\nWe won’t discuss several kinds of issues:\nThe rationale for implementing or not implementing some features in a sound server. An ideal feature set of a sound server deserves a deeper analysis than I can provide here.\nFundamental costs of the provided features that are unavoidable. It’s up to the user to decide whether to pay them or not, depending on the requirements and available alternatives.\nBugs and limitations of the existing code that can be just fixed at some point. There is a bug tracker for such things.\nDocumentation A comprehensive documentation is a starting point for detecting problems and improving things. PulseAudio has a good documentation for public interfaces and troubleshooting. However, the following official documentation is partially or completely missing:\na detailed feature overview a design overview a detailed description of the key abstractions and their relations a high-level description of the key algorithms a high-level description of the protocols a detailed documentation for internal APIs a rationale Abstraction level PulseAudio is built around the four fundamental object types: sources, sinks, source outputs, and sink inputs.\nThese object types are low-level enough. They operate with streams of samples. It’s up to the implementation how to handle the samples: write them to a sound card, send them over the network, perform sound processing or whatever else.\nThe good thing with this approach is the flexibility it gives. All of the numerous PulseAudio features are implemented in terms of the four objects. The problem, however, is that the features implemented on top of them are much higher level.\nBecause of this distance, modules often need to implement some intermediate layers. Similar modules need similar intermediate layers. However, the lack of the appropriate core abstractions causes problems with code reuse and consistency.\nHere are some candidates of the high-level abstractions that are currently missing:\npublishing and discovery\nPulseAudio implements Zeroconf publishing and discovery for PulseAudio servers, SAP/SDP publishing and discovery for RTP, Zeroconf discovery for AirPlay, Udev hotplugging for ALSA cards, and BlueZ hotplugging for Bluetooth devices.\nAll of these mechanisms are implemented independently and are bound to a concrete type of transport or device. It’s not possible to reuse existing publishing and discovery alone. For example, it’s not possible to write a module that improves the RTP support without reimplementing SAP/SDP support or reuse the Zeroconf discovery for an alternative transport.\ntransport\nPulseAudio implements various transport protocols, including the “native” protocol, RTP, RAOP, HTTP, and several Bluetooth transports.\nConceptually, a transport could implement just encoding and I/O. Bluetooth transports are close to this. RTP transport is implemented as a source output and sink input, which is a bit more complicated. The “native” protocol and RAOP transports are implemented as a source and sink, which is the most complicated.\nEvery network transport is implemented from scratch. A network source or sink should start an IO thread, run the rtpoll loop, implement clocking, handle asynchronous events, etc. It’s not possible to reuse this code for a new transport.\nprotocol extension\nPulseAudio is an extensible server, and it supports several extensible protocols: the D-Bus API, the “native” protocol, and the RTP.\nThe D-Bus API is a good example. It provides an abstraction of the protocol extension which may be registered by a module.\nModules can also implement custom commands for the “native” protocol. However, the core does not provide an explicit abstraction of the “native” protocol extension. All modules that implement custom commands are hardcoded in the protocol implementation. It’s not possible to add a new extension without modifying the core.\nThe RTP is designed to be an extremely extensible protocol. However, PulseAudio doesn’t support RTP extensions. It’s not possible to add support for a new payload type or add forward error correction support to the RTP transport.\nfilter\nPulseAudio supports several filters, like channel remapping and echo cancellation.\nAll filter modules follow the same convention. They accept the same arguments and create a pair of virtual device and stream.\nEvery filter module is implemented from scratch. It should parse the module arguments, start an IO thread, run the rtpoll loop, implement clocking, handle asynchronous events, etc. It’s not possible to reuse this code for a new filter.\nAlthough it’s possible to implement filters as LADSPA plugins, all filters available out of the box don’t use this possibility.\npriority list\nPulseAudio implements an extensible routing algorithm, which is spread across several modules.\nEvery routing step is implemented in a separate module. These modules are quite isolated because the core doesn’t provide a generic routing abstraction. Every module just installs a hook that tries to route a stream in its own way if it wasn’t routed by another module yet. The routing rules are implemented independently in every module, which may lead to inconsistency.\nThis problem is addressed by the PriorityRouting proposal which is currently not implemented.\nscheduler\nPulseAudio implements an advanced timer-based scheduler for ALSA device. The scheduler is implemented inside the ALSA source and sink.\nThere are two problems. First, the implementation is duplicated in the source and in the sink. Second, the implementation is pretty complicated, and mixing it with the source or sink housekeeping makes things even more complicated. It would be much simpler to understand and improve it if it was a standalone component.\nMechanism and policy Lack of the appropriate high-level abstractions leads to violation of the separation of mechanism and policy principle.\nIf a mechanism is not encapsulated by an abstraction that is generic enough, its implementation tends to be merged with the concrete policy or a set of policies. It makes it hard to modify, replace, or reuse mechanism and policy independently.\nWhen this happens, and several modules need the same mechanism, two scenarios are possible:\nThe implementation of the mechanism may be reimplemented in every module, but a bit differently, fitted for the concrete policy that the module needs.\nIn this case, we get increased code duplication. We also get an increased coupling between modules, because cooperating modules rely on the concrete policies implemented in other modules, instead of a generic mechanism implemented in core.\nThis happened with network transports, sound processing filters, and routing modules.\nThe implementation of the mechanism together with the implementation of all necessary policies may be moved to the core.\nIn this case, we get defective modularity. We also get an increased coupling between modules and core, because the modules rely on the concrete policies implemented in core, instead of a generic mechanism.\nThis happened with the network protocols and protocol extensions. Actually, the modularity is only an illusion in this particular case, because the “native”, the “simple”, the CLI, the HTTP, and the ESound protocol modules are just thin wrappers that use the functionality implemented completely in the core.\nCode quality There are some usual problems with the code quality, that could be resolved by introducing stricter code style guidelines:\nComplicated sharing of responsibilities between the “base” part of an object from the core and the “derived” part from a module, which recursively calls each other.\nComplicated sharing of an object state between threads and non-obvious jumps from one thread to another. Different methods of the same object are called on different threads and use different subsets of the object fields.\nWide and coupled internal interfaces. Maybe it’s just me, but it took me about a week to figure it out what’s going on in the ALSA Mixer and UCM related code.\nMixing low-level code like memory management and string manipulation with the high-level logic, which becomes hard to extract and understand.\nMixing generic utilities and domain-specific components in core. Utilities are boring and vast, and it would be helpful to separate them from the really important code.\nHandmade serialization for the network protocols and restoration database, handmade formatters and parsers.\nCustom implementation of the collections, event loops, threading primitives, and platform wrappers, instead of using a general purpose library like Glib, which is an acceptable dependency on the desktop.\nLong function bodies, short variable names, reusing the same variable for several purposes, #ifdef madness in some modules.\nService quality Two PulseAudio servers can be connected either using the “native” protocol or using RTP. Both implementations are not suited for unreliable networks like WiFi:\nthe “native” protocol is based on TCP, and packet losses cause playback delays\nthe implementation of RTP sender and receiver in PulseAudio doesn’t employ any RTP extensions for error correction or retransmission, and packet losses cause playback holes\nThis problem is addressed by the Roc Toolkit project that I’m currently working on. A tutorial is available here.\nUsability There are several sources of confusion for users:\nBuilding transport chains is non-intuitive. The user just wants to select the source, destination, and transport. However, things are not that simple.\nOn the one hand, different transports are implemented using different object types. For example, the “native” protocol uses tunnel sources and sinks, but the RTP uses RTP source outputs and sink inputs. On the other hand, only certain object types may be connected directly.\nHence, in some cases, the user can just connect a device or a stream to a transport, while in other cases the user has to configure tricky adapters like the loopback and null sink.\nBuilding sound processing chains is non-intuitive. The user just wants to apply one or several filters to a stream or device. But things are not that simple.\nEvery filter module creates a pair of a virtual device and stream. The user has to find both in the list of available devices and streams and connect the filter device to some stream and the filter stream to some device. When multiple filter chains are employed, the configuration becomes totally confusing.\nRouting is non-intuitive. The user wants to select a device either for a stream or for a category of streams. Again, things are not that simple.\nThere are two independent routing rule databases. The fist one (module-stream-restore) is used when the user moves a single stream, but the server may move either a single stream or a category of streams, depending on the stream properties provided by the application.\nThe second one (module-device-manager) is used when the user configures preferred devices for a category of streams. However, this routing rules may be overridden by conflicting rules from the first database. Only KDE provides a GUI for this second database.\nThe autospawn feature is just weird. The user wants to kill the server, but the server is magically restarted by any background client. By the way, if the user disables autospawn, the server will not be automatically restarted after a crash, which still happens from time to time.\nFinal thoughts This section lists some downsides, but there are upsides too:\nWith a few exceptions mentioned above, the module system is done well.\nAt the high level, the inter-thread communication is done well. Mutexes are rare, threads use event loops and exchange asynchronous messages.\nThe memory management is done well. It is based on pools and chunks and is flexible enough to support the zero-copy mode.\nThe function contracts are carefully covered with assertions.\nThere is a plenty of advanced and well-engineered features, including the timer-based scheduler, buffer rewinding, clocking, and latency management. There is a lot to learn from the implementation.\nFinally, PulseAudio just works in many cases.\nMost of the problems listed in this section are not trivial but may be resolved with a thoughtful refactoring. In this regard, it’s worth mentioning some reasons to contribute to PulseAudio:\nThe project solves real problems that are complex and interesting, so it offers a challenge.\nThe project employs or implements many different technologies, from various device backends to sound processing tools and scheduling, so there is something to learn from.\nThe project is already used on many Linux desktops, so it’s practical.\nThanks for reading, and happy hacking!\n$(\u0026lsquo;h2, h3, h4\u0026rsquo;).each(function() { var id = $(this).attr(\u0026lsquo;id\u0026rsquo;); $(this).prepend( \u0026lsquo;\u0026rsquo; ); })\n(function() { var d = document, s = d.createElement(\u0026lsquo;script\u0026rsquo;); s.src = \u0026lsquo;//gavv.disqus.com/embed.js\u0026rsquo;; s.setAttribute(\u0026lsquo;data-timestamp\u0026rsquo;, +new Date()); (d.head || d.body).appendChild(s); })();\nEnable JavaScript to view the comments powered by Disqus.\n","permalink":"https://fanyxok.github.io/blog/pulseaudio/","summary":"Diagram of PulseAudio Pipewire is an alternative. Preface I’m working on the Roc Toolkit open-source project, a development kit for realtime audio streaming over the network. You can read more about the project in these two articles: 1, 2.\nWe decided to implement a set of PulseAudio modules that will allow PulseAudio to use Roc as a network transport. Many Linux distros employ PulseAudio, and their users will be able to improve network service quality without changing the workflow.","title":"PulseAudio Under the Hood"},{"content":"Neovim Beginner init.lua Install.sh plugins.lua is where we start to configure Neovim.\nAll Lua configuration files are under the lua folder (:h lua-require).\nStartup Screen Use alpha.nvim as the startup screen.\nThe configuration file is lua/config/alpha.lua.\nalpha.lua:\npcall is used to prevent errors requiring a non-existent module or a module with syntax errors. We specify an ASCII art header, some convenient shortcuts, and a footer to indicate the number of plugins, date and time, and a quote. Git use Neogit for version control.\nThe configuration file is lua/config/neogit.lua.\nlocal M = {}function M.setup() local status_ok, neogit = pcall(require, \u0026#34;neogit\u0026#34;) if not status_ok then return end neogit.setup {} endreturn M pcall is used to prevent errors requiring a non-existent module or a module with syntax errors. For now, we use the default configuration for Neogit. options Besides Vimscript files, Lua files can be loaded automatically from special folders in your runtimepath (:h load-plugins). All *.vim files are sourced before *.lua files. We configure a few default settings by using after/plugin/defaults.lua.\ndefaults.lua\nWe configure as the Leader key (:h ).\nFor each setting, you can check out the help documentation for more details. We shall see how to further customize Neovim with sensible defaults later. We configure an autocmd to highlight the text when yanking or copying. We also configure a file type plugin (:h ftplugin) for Lua files with buffer-specific settings.\nA filetype plugin is like a global plugin, except it only sets options and defines mappings for the current buffer.\nvim.bo.shiftwidth = 2 vim.bo.tabstop = 2 vim.bo.softtabstop = 2 vim.bo.textwidth = 120 To remove the configuration, simply delete the ~/.config/nvim-beginner folder.\nIf we want to start nvb in a new terminal without installing all the plugins again, put the following lines in the RC files, e.g. .zshrc, or .bashrc.\nKey Mappings ans WhichKey Plugin Management Status Line Fuzzy File Search File Explorer Buffer Motion Built-in Completion Completion Plugin Auto Pairs LSP LSP using null-ls.nvim LSP Plugin Debuging using DAP Testing Debugging using vimspector Performance Snippets Lua Autocmd and Keymap Functions Color Scheme Remote Plugins Session Snippets using Lua Refactoring Code Annotation and Documentation Note Taking, Writing, Diagramming, and Presentation Source Code Control Window Bar User Interface Python Remote Debugging Python Code Refactoring Plugins for Key Mapping Code Folding Terminal Debugger GUI Cheatsheet and Coding Assistant Conventional Commits Database Explorer Code Context ","permalink":"https://fanyxok.github.io/blog/neovim-beginner/","summary":"Neovim Beginner init.lua Install.sh plugins.lua is where we start to configure Neovim.\nAll Lua configuration files are under the lua folder (:h lua-require).\nStartup Screen Use alpha.nvim as the startup screen.\nThe configuration file is lua/config/alpha.lua.\nalpha.lua:\npcall is used to prevent errors requiring a non-existent module or a module with syntax errors. We specify an ASCII art header, some convenient shortcuts, and a footer to indicate the number of plugins, date and time, and a quote.","title":"Neovim Beginner"},{"content":"","permalink":"https://fanyxok.github.io/about/","summary":"","title":"About"},{"content":"How to debug a HardFault on an ARM Cortex-M MCU 20 Nov 2019 by Chris Coleman\nFaults happen on embedded devices all the time for a variety of reasons – ranging from something as simple as a NULL pointer dereference to something more unexpected like running a faulty code path only when in a zero-g environment on the Tower of Terror in Disneyland1. It’s important for any embedded engineer to understand how to debug and resolve this class of issue quickly.\nIn this article, we explain how to debug faults on ARM Cortex-M based devices. In the process, we learn about fault registers, how to automate fault analysis, and figure out ways to recover from some faults without rebooting the MCU. We include practical examples, with a step by step walk-through on how to investigate them.\nIf you’d rather listen to me present this information and see some demos in action, watch this webinar recording.\nLike Interrupt? Subscribe to get our latest posts straight to your mailbox.\nTable of Contents Determining What Caused The Fault Relevant Status Registers Configurable Fault Status Registers (CFSR) - 0xE000ED28 HardFault Status Register (HFSR) - 0xE000ED2C Recovering the Call Stack Automating the Analysis Halting \u0026amp; Determining Core Register State Fault Register Analyzers Postmortem Analysis Recovering From A Fault Examples eXecute Never Fault Bad Address Read Coprocessor Fault Imprecise Fault Fault Entry Exception Recovering from a UsageFault without a SYSRESET Determining What Caused The Fault All MCUs in the Cortex-M series have several different pieces of state which can be analyzed when a fault takes place to trace down what went wrong.\nFirst we will explore the dedicated fault status registers that are present on all Cortex-M MCUs except the Cortex-M0.\nIf you are trying to debug a Cortex-M0, you can skip ahead to the next section where we discuss how to recover the core register state and instruction being executed at the time of the exception.\nNOTE: If you already know the state to inspect when a fault occurs, you may want to skip ahead to the section about how to automate the analysis.\nRelevant Status Registers Configurable Fault Status Registers (CFSR) - 0xE000ED28 This 32 bit register contains a summary of the fault(s) which took place and resulted in the exception. The register is comprised of three different status registers – UsageFault, BusFault \u0026amp; MemManage Fault Status Registers:\nThe register can be accessed via a 32 bit read at 0xE000ED28 or each register can be read individually. For example, in GDB it would look something like this:\nEntire CFSR - print/x *(uint32_t *) 0xE000ED28 UsageFault Status Register (UFSR) - print/x *(uint16_t *)0xE000ED2A BusFault Status Register (BFSR) - print/x *(uint8_t *)0xE000ED29 MemManage Status Register (MMFSR) - print/x *(uint8_t *)0xE000ED28 NOTE: If multiple faults have occurred, bits related to several faults may be set. Fields are only cleared by a system reset or by writing a 1 to them.\nUsageFault Status Register (UFSR) - 0xE000ED2A This register is a 2 byte register which summarizes any faults that are not related to memory access failures, such as executing invalid instructions or trying to enter invalid states.\nwhere,\nDIVBYZERO - Indicates a divide instruction was executed where the denominator was zero. This fault is configurable. UNALIGNED - Indicates an unaligned access operation occurred. Unaligned multiple word accesses, such as accessing a uint64_t that is not 8-byte aligned, will always generate this fault. With the exception of Cortex-M0 MCUs, whether or not unaligned accesses below 4 bytes generate a fault is also configurable. NOCP - Indicates that a Cortex-M coprocessor instruction was issued but the coprocessor was disabled or not present. One common case where this fault happens is when code is compiled to use the Floating Point extension (-mfloat-abi=hard -mfpu=fpv4-sp-d16) but the coprocessor was not enabled on boot. INVPC - Indicates an integrity check failure on EXC_RETURN. We’ll explore an example below. EXC_RETURN is the value branched to upon return from an exception. If this fault flag is set, it means a reserved EXC_RETURN value was used on exception exit. INVSTATE - Indicates the processor has tried to execute an instruction with an invalid Execution Program Status Register (EPSR) value. Among other things the ESPR tracks whether or not the processor is in thumb mode state. Instructions which use “interworking addresses”2 (bx \u0026amp; blx or ldr \u0026amp; ldm when loading a pc-relative value) must set bit[0] of the instruction to 1 as this is used to update ESPR.T. If this rule is violated, a INVSTATE exception will be generated. When writing C code, the compiler will take care of this automatically, but this is a common bug which can arise when hand-writing assembly. UNDEFINSTR - Indicates an undefined instruction was executed. This can happen on exception exit if the stack got corrupted. A compiler may emit undefined instructions as well for code paths that should be unreachable. Configurable UsageFault It is worth noting that some classes of UsageFaults are configurable via the Configuration and Control Register (CCR) located at address 0xE000ED14.\nBit 4 (DIV_0_TRP) - Controls whether or not divide by zeros will trigger a fault. Bit 3 (UNALIGN_TRP) - Controls whether or not unaligned accesses will always generate a fault. NOTE: On reset both of these optional faults are disabled. It is generally a good idea to enable DIV_0_TRP to catch mathematical errors in your code.\nBusFault Status Register (BFSR) - 0xE000ED29 This register is a 1 byte register which summarizes faults related to instruction prefetch or memory access failures.\nBFARVALID - Indicates that the Bus Fault Address Register (BFAR), a 32 bit register located at 0xE000ED38, holds the address which triggered the fault. We’ll walk through an example using this info below. LSPERR \u0026amp; STKERR - Indicates that a fault occurred during lazy state preservation or during exception entry, respectively. Both are situations where the hardware is automatically saving state on the stack. One way this error may occur is if the stack in use overflows off the valid RAM address range while trying to service an exception. We’ll go over an example below. UNSTKERR - Indicates that a fault occurred trying to return from an exception. This typically arises if the stack was corrupted while the exception was running or the stack pointer was changed and its contents were not initialized correctly. IMPRECISERR - This flag is very important. It tells us whether or not the hardware was able to determine the exact location of the fault. We will explore some debug strategies when this flag is set in the next section and walk through a code exampe below. PRECISERR - Indicates that the instruction which was executing prior to exception entry triggered the fault. Imprecise Bus Error Debug Tips Imprecise errors are one of the hardest classes of faults to debug. They result asynchronously to instruction execution flow. This means the registers stacked on exception entry will not point to the code that caused the exception.\nInstruction fetches and data loads should always generate synchronous faults for Cortex-M devices and be precise. Conversely, store operations can generate asynchronous faults. This is because writes will sometimes be buffered prior to being flushed to prevent pipeline stalls so the program counter will advance before the actual data store completes.\nWhen debugging an imprecise error, you will want to inspect the code around the area reported by the exception for a store that looks suspicious. If the MCU has support for the ARM Embedded Trace Macrocell (ETM), the history of recently executed instructions can be viewed by some debuggers3.\nAuxiliary Control Register (ACTLR) - 0xE000E008 This register allows for some hardware optimizations or features to be disabled typically at the cost of overall performance or interrupt latency. The exact configuration options available are specific to the Cortex-M implementation being used.\nFor the Cortex M3 \u0026amp; Cortex M4 only, there is a trick to make all IMPRECISE accesses PRECISE by disabling any write buffering. This can be done by setting bit 1 (DISDEFWBUF) of the register to 1.\nFor the Cortex M7, there is no way to force all stores to be synchronous / precise.\nAuxiliary Bus Fault Status Register (ABFSR) - 0xE000EFA8 This register only exists for Cortex-M7 devices. When an IMPRECISE error occurs it will at least give us an indication of what memory bus the fault occurred on4:\nA full discussion of memory interfaces is outside the scope of this article but more details can be found in the reference manual 4.\nMemManage Status Register (MMFSR) - 0xE000ED28 This register reports Memory Protection Unit faults.\nTypically MPU faults will only trigger if the MPU has been configured and enabled by the firmware. However, there are a few memory access errors that will always result in a MemManage fault – such as trying to execute code from the system address range (0xExxx.xxxx).\nThe layout of the register looks like this:\nwhere,\nMMARVALID - Indicates that the MemManage Fault Address Register (MMFAR), a 32 bit register located at 0xE000ED34, holds the address which triggered the MemManage fault. MLSPERR \u0026amp; MSTKERR - Indicates that a MemManage fault occurred during lazy state preservation or exception entry, respectively. For example, this could happen if an MPU region is being used to detect stack overflows. MUNSTKERR - Indicates that a fault occurred while returning from an exception DACCVIOL - Indicates that a data access triggered the MemManage fault. IACCVIOL - Indicates that an attempt to execute an instruction triggered an MPU or Execute Never (XN) fault. We’ll explore an example below. HardFault Status Register (HFSR) - 0xE000ED2C This registers explains the reason a HardFault exception was triggered.\nThere’s not too much information in this register but we will go over the fields real quickly\nDEBUGEVT - Indicates that a debug event (i.e executing a breakpoint instruction) occurred while the debug subsystem was not enabled FORCED - This means a configurable fault (i.e. the fault types we discussed in previous sections) was escalated to a HardFault, either because the configurable fault handler was not enabled or a fault occurred within the handler. VECTTBL - Indicates a fault occurred because of an issue reading from an address in the vector table. This is pretty atypical but could happen if there is a bad address in the vector table and an unexpected interrupt fires. Recovering the Call Stack To fix a fault, we will want to determine what code was running when the fault occurred. To accomplish this, we need to recover the register state at the time of exception entry.\nIf the fault is readily reproducible and we have a debugger attached to the board, we can manually add a breakpoint for the function which handles the exception. In GDB this will look something like\n(gdb) break HardFault_Handler Upon exception entry some registers will always be automatically saved on the stack. Depending on whether or not an FPU is in use, either a basic or extended stack frame will be pushed by hardware.\nRegardless, the hardware will always push the same core set of registers to the very top of the stack which was active prior to entering the exception. ARM Cortex-M devices have two stack pointers, msp \u0026amp; psp. Upon exception entry, the active stack pointer is encoded in bit 2 of the EXC_RETURN value pushed to the link register. If the bit is set, the psp was active prior to exception entry, else the msp was active.\nLet’s look at the state when we break in HardFault_Handler for a pathological example:\nint illegal_instruction_execution(void) { int (*bad_instruction)(void) = (void *)0xE0000000; return bad_instruction(); } (gdb) p/x $lr $4 = 0xfffffffd # psp was active prior to exception if bit 2 is set # otherwise, the msp was active (gdb) p/x $lr\u0026amp;(1\u0026lt;\u0026lt;2) $5 = 0x4 # First eight values on stack will always be: # r0, r1, r2, r3, r12, LR, pc, xPSR (gdb) p/a *(uint32_t[8] *)$psp $16 = { 0x0 \u0026lt;g_pfnVectors\u0026gt;, 0x200003c4 \u0026lt;ucHeap+604\u0026gt;, 0x10000000, 0xe0000000, 0x200001b8 \u0026lt;ucHeap+80\u0026gt;, 0x61 \u0026lt;illegal_instruction_execution+16\u0026gt;, 0xe0000000, 0x80000000 } Offset 6 and 7 in the array dumped hold the LR (illegal_instruction_execution) \u0026amp; PC (0xe0000000) so we now can see exactly where the fault originated!\nFaults from Faults The astute observer might wonder what happens when a new fault occurs in the code dealing with a fault. If you have enabled configurable fault handlers (i.e MemManage, BusFault, or UsageFault), a fault generated in these handlers will trigger a HardFault.\nOnce in the HardFault Handler, the ARM Core is operating at a non-configurable priority level, -1. At this level or above, a fault will put the processor in an unrecoverable state where a reset is expected. This state is known as Lockup.\nTypically, the processor will automatically reset upon entering lockup but this is not a requirement per the specification. For example, you may have to enable a hardware watchdog for a reset to take place. It’s worth double checking the reference manual for the MCU being used for clarification.\nWhen a debugger is attached, lockup often has a different behavior. For example, on the NRF52840, “Reset from CPU lockup is disabled if the device is in debug interface mode”5.\nWhen a lockup happens, the processor will repeatedly fetch the same fixed instruction, 0xFFFFFFFE or the instruction which triggered the lockup, in a loop until a reset occurs.\nFun Fact: Whether or not some classes of MemManage or BusFaults trigger a fault from an exception is actually configurable via the MPU_CTRL.HFNMIENA \u0026amp; CCR.BFHFNMIGN register fields, respectively.\nAutomating the Analysis At this point we have gone over all the pieces of information which can be manually examined to determine what caused a fault. While this might be fun the first couple times, it can become a tiresome and error prone process if you wind up doing it often. In the following sections we’ll explore how we can automate this analysis!\nHalting \u0026amp; Determining Core Register State What if we are trying to debug an issue that is not easy to reproduce? Even if we have a debugger attached, useful state may be overwritten before we have a chance to halt the debugger and take a look.\nThe first thing we can do is to programmatically trigger a breakpoint when the system faults:\n// NOTE: If you are using CMSIS, the registers can also be // accessed through CoreDebug-\u0026gt;DHCSR \u0026amp; CoreDebug_DHCSR_C_DEBUGEN_Msk #define HALT_IF_DEBUGGING() \\ do { \\ if ((*(volatile uint32_t *)0xE000EDF0) \u0026amp; (1 \u0026lt;\u0026lt; 0)) { \\ __asm(\u0026quot;bkpt 1\u0026quot;); \\ } \\ } while (0) Above, we discussed how to hand unroll the register state prior to the exception taking place. Let’s explore how we can instrument the code to make this a less painful process.\nFirst, we can easily define a C struct to represent the register stacking:\ntypedef struct __attribute__((packed)) ContextStateFrame { uint32_t r0; uint32_t r1; uint32_t r2; uint32_t r3; uint32_t r12; uint32_t lr; uint32_t return_address; uint32_t xpsr; } sContextStateFrame; We can determine the stack pointer that was active prior to the exception using a small assembly shim that applies the logic discussed above and passes the active stack pointer as an argument into my_fault_handler_c:\n#define HARDFAULT_HANDLING_ASM(_x) \\ __asm volatile( \\ \u0026quot;tst lr, #4 \\n\u0026quot; \\ \u0026quot;ite eq \\n\u0026quot; \\ \u0026quot;mrseq r0, msp \\n\u0026quot; \\ \u0026quot;mrsne r0, psp \\n\u0026quot; \\ \u0026quot;b my_fault_handler_c \\n\u0026quot; \\ ) Finally, we can put together my_fault_handler_c that looks something like:\n// Disable optimizations for this function so \u0026quot;frame\u0026quot; argument // does not get optimized away __attribute__((optimize(\u0026quot;O0\u0026quot;))) void my_fault_handler_c(sContextStateFrame *frame) { // If and only if a debugger is attached, execute a breakpoint // instruction so we can take a look at what triggered the fault HALT_IF_DEBUGGING(); // Logic for dealing with the exception. Typically: // - log the fault which occurred for postmortem analysis // - If the fault is recoverable, // - clear errors and return back to Thread Mode // - else // - reboot system } Now when a fault occurs and a debugger is attached, we will automatically hit a breakpoint and be able to look at the register state! Re-examining our illegal_instruction_execution example we have:\n0x00000244 in my_fault_handler_c (frame=0x200005d8 \u0026lt;ucHeap+1136\u0026gt;) at ./cortex-m-fault-debug/startup.c:94 94\tHALT_IF_DEBUGGING(); (gdb) p/a *frame $18 = { r0 = 0x0 \u0026lt;g_pfnVectors\u0026gt;, r1 = 0x200003c4 \u0026lt;ucHeap+604\u0026gt;, r2 = 0x10000000, r3 = 0xe0000000, r12 = 0x200001b8 \u0026lt;ucHeap+80\u0026gt;, lr = 0x61 \u0026lt;illegal_instruction_execution+16\u0026gt;, return_address = 0xe0000000, xpsr = 0x80000000 } Furthermore, we now have a variable we can read stack info from and a C function we can easily extend for postportem analysis!\nFault Register Analyzers Instrumenting the code Many Real Time Operating Systems (RTOS) targetting Cortex-M devices will add options to dump verbose fault register information to the console upon crash. Some examples include Arm Mbed OS6 and Zephyr7. For example, with Zephyr, the illegal_instruction_execution() crash looks like:\n***** MPU FAULT ***** Instruction Access Violation ***** Hardware exception ***** Current thread ID = 0x20000074 Faulting instruction address = 0xe0000000 Fatal fault in thread 0x20000074! Aborting. This approach has a couple notable limitations:\nIt bloats the code \u0026amp; data size of the binary image and consequently often gets turned off. It can increase the stack size requirements for the fault handler (due to printf calls) It requires a firmware update to improve or fix issues with the analyzers It requires a console session be active to see what fault occurred. Furthermore, this can be flaky if the system is in a crashed state. Debugger Plugins Many embedded IDEs expose a system view that can be used to look at registers. The registers will often be decoded into human readable descriptions. These implementations typically leverage the CMSIS System View Description (SVD) format8, a standardized XML file format for describing the memory mapped registers in an ARM MCU. Most silicon vendors expose this information on their own website, ARM’s website9, or provide the files upon request.\nYou can even load these files in GDB using PyCortexMDebug10, a GDB python script .\nTo use the utility, all you need to do is update your .gdbinit to use PyPi packages from your environment (instructions here) and then run:\n$ git clone git@github.com:bnahill/PyCortexMDebug.git # Check out Python 2 compatible code $ git checkout 77af54e $ cd PyCortexMDebug $ python setup.py install When you next start gdb, you can source the svd_gdb.py script and use it to start inspecting registers. Here’s some output for the svd plugin we will use in the examples below:\n(gdb) source cmdebug/svd_gdb.py (gdb) svd_load cortex-m4-scb.svd (gdb) svd Available Peripherals: ... SCB: System control block ... (gdb) svd SCB Registers in SCB: ... CFSR_UFSR_BFSR_MMFSR: 524288 Configurable fault status register ... (gdb) svd SCB CFSR_UFSR_BFSR_MMFSR Fields in SCB CFSR_UFSR_BFSR_MMFSR: IACCVIOL: 0 Instruction access violation flag DACCVIOL: 0 Data access violation flag MUNSTKERR: 0 Memory manager fault on unstacking for a return from exception MSTKERR: 0 Memory manager fault on stacking for exception entry. MLSPERR: 0 MMARVALID: 0 Memory Management Fault Address Register (MMAR) valid flag IBUSERR: 1 Instruction bus error PRECISERR: 0 Precise data bus error IMPRECISERR: 0 Imprecise data bus error UNSTKERR: 0 Bus fault on unstacking for a return from exception STKERR: 0 Bus fault on stacking for exception entry LSPERR: 0 Bus fault on floating-point lazy state preservation BFARVALID: 0 Bus Fault Address Register (BFAR) valid flag UNDEFINSTR: 0 Undefined instruction usage fault INVSTATE: 1 Invalid state usage fault INVPC: 0 Invalid PC load usage fault NOCP: 0 No coprocessor usage fault. UNALIGNED: 0 Unaligned access usage fault DIVBYZERO: 0 Divide by zero usage fault Postmortem Analysis The previous two approaches are only helpful if we have a debug or physical connection to the device. Once the product has shipped and is out in the field these strategies will not help to triage what went wrong on devices.\nOne approach is to simply try and reproduce the issue on site. This is a guessing game (are you actually reproducing the same issue the customer hit?), can be a huge time sink and in some cases is not even particularly feasible1.\nAnother strategy is to log the fault register and stack values to persistent storage and periocially collect or push the error logs. On the server side, the register values can be decoded and addresses can be symbolicated to try to root cause the crash.\nAlternatively, an end-to-end firmware error analysis system, such as Memfault, can be used to automatically collect, transport, deduplicate and surface the faults and crashes happening in the field. Here is some example output from Memfault for the bad memory read example we will walk through below:\nRecovering From A Fault DISCLAIMER: Typically when a fault occurs, the best thing to do is reset the MCU since it’s hard to be certain what parts of the MCU were corrupted as part of the fault (embedded MCUs don’t offer a MMU like you would find on a bigger processors).\nOccasionally, you may want to recover the system from a fault without rebooting it. For example, maybe you have one RTOS task isolated by the MPU that just needs to be restarted.\nLet’s quickly explore how we could implement a recovery mechanism that puts a RTOS task which experience a UsageFault into an idle loop and reboots the system otherwise.\nWe will use the Application Interrupt and Reset Control Register to reset the device if the fault is unrecoverable. We can easily extend my_fault_handler_c from above:\nvoid my_fault_handler_c(sContextStateFrame *frame) { [...] volatile uint32_t *cfsr = (volatile uint32_t *)0xE000ED28; const uint32_t usage_fault_mask = 0xffff0000; const bool non_usage_fault_occurred = (*cfsr \u0026amp; ~usage_fault_mask) != 0; // the bottom 8 bits of the xpsr hold the exception number of the // executing exception or 0 if the processor is in Thread mode const bool faulted_from_exception = ((frame-\u0026gt;xpsr \u0026amp; 0xFF) != 0); if (faulted_from_exception || non_usage_fault_occurred) { // For any fault within an ISR or non-usage faults // let's reboot the system volatile uint32_t *aircr = (volatile uint32_t *)0xE000ED0C; *aircr = (0x05FA \u0026lt;\u0026lt; 16) | 0x1 \u0026lt;\u0026lt; 2; while (1) { } // should be unreachable } [...] } Now, the interesting part, how do we clean up our state and return to normal code from the HardFault handler?!\nThere’s a few things we will need to do:\nClear any logged faults from the CFSR by writing 1 to each bit which is set. Change the function we return to so we idle the task. In the example case it’s recover_from_task_fault. Scribble a known pattern over the lr. The function we are returning to will need to take special action (i.e like deleting the task or entering a while(1) loop). It can’t just exit and branch to where we were before so we want to fault if this is attempted. Reset the xpsr. Among other things the xpsr tracks the state of previous comparison instructions which were run and whether or not we are in the middle of a “If-Then” instruction block. The only bit that needs to remain set is the “T” field (bit 24) indicating the processor is in thumb mode11. This winds up looking like:\n// Clear any logged faults from the CFSR *cfsr |= *cfsr; // the instruction we will return to when we exit from the exception frame-\u0026gt;return_address = (uint32_t)recover_from_task_fault; // the function we are returning to should never branch // so set lr to a pattern that would fault if it did frame-\u0026gt;lr = 0xdeadbeef; // reset the psr state and only leave the // \u0026quot;thumb instruction interworking\u0026quot; bit set frame-\u0026gt;xpsr = (1 \u0026lt;\u0026lt; 24); You may recall from the RTOS Context Switching post that fault handlers can work just like regular C functions so after these changes we will exit from my_fault_handler_c and start executing whatever is in recover_from_task_fault function. We will walk through an example of this below.\nExamples In the sections below we will walk through the analysis of a couple faults.\nFor this setup we will use:\na nRF52840-DK12 (ARM Cortex-M4F) as our development board SEGGER JLinkGDBServer13 as our GDB Server. GCC 8.3.1 / GNU Arm Embedded Toolchain as our compiler14 GNU make as our build system All the code can be found on the Interrupt Github page with more details in the README in the directory linked.\nSetup Start a GDB Server:\nJLinkGDBServer -if swd -device nRF52840_xxAA Follow the instructions above to setup support for reading SVD files from GDB, build, and flash the example app:\n$ make [...] Linking library Generated build/nrf52.elf $ arm-none-eabi-gdb-py --eval-command=\u0026quot;target remote localhost:2331\u0026quot; --ex=\u0026quot;mon reset\u0026quot; --ex=\u0026quot;load\u0026quot; --ex=\u0026quot;mon reset\u0026quot; --se=build/nrf52.elf $ source PyCortexMDebug/cmdebug/svd_gdb.py $ (gdb) svd_load cortex-m4-scb.svd Loading SVD file cortex-m4-scb.svd... (gdb) The app has eight different crashes you can configure by changing FAULT_EXAMPLE_CONFIG at compile time or by editing the value at runtime:\n(gdb) break main (gdb) continue (gdb) set g_crash_config=1 (gdb) continue eXecute Never Fault Code int illegal_instruction_execution(void) { int (*bad_instruction)(void) = (void *)0xE0000000; return bad_instruction(); } Analysis (gdb) break main (gdb) continue Breakpoint 1, main () at ./cortex-m-fault-debug/main.c:180 180\txQueue = xQueueCreate(mainQUEUE_LENGTH, sizeof(unsigned long)); (gdb) set g_crash_config=0 (gdb) c Continuing. Program received signal SIGTRAP, Trace/breakpoint trap. 0x00000218 in my_fault_handler_c (frame=0x200005e8 \u0026lt;ucHeap+1152\u0026gt;) at ./cortex-m-fault-debug/startup.c:91 91\tHALT_IF_DEBUGGING(); (gdb) bt #0 0x00000218 in my_fault_handler_c (frame=0x200005e8 \u0026lt;ucHeap+1152\u0026gt;) at ./cortex-m-fault-debug/startup.c:91 #1 \u0026lt;signal handler called\u0026gt; #2 0x00001468 in prvPortStartFirstTask () at ./cortex-m-fault-debug/freertos_kernel/portable/GCC/ARM_CM4F/port.c:267 #3 0x000016e6 in xPortStartScheduler () at ./cortex-m-fault-debug/freertos_kernel/portable/GCC/ARM_CM4F/port.c:379 #4 0x1058e476 in ?? () We can check the CFSR to see if there is any information about the fault which occurred.\n(gdb) p/x *(uint32_t*)0xE000ED28 $3 = 0x1 (gdb) svd SCB CFSR_UFSR_BFSR_MMFSR Fields in SCB CFSR_UFSR_BFSR_MMFSR: IACCVIOL: 1 Instruction access violation flag [...] That’s interesting! We hit a Memory Management instruction access violation fault even though we haven’t enabled any MPU regions. From the CFSR, we know that the stacked frame is valid so we can take a look at that to see what it reveals:\n(gdb) p/a *frame $1 = { r0 = 0x0 \u0026lt;g_pfnVectors\u0026gt;, r1 = 0x200003c4 \u0026lt;ucHeap+604\u0026gt;, r2 = 0x10000000, r3 = 0xe0000000, r12 = 0x200001b8 \u0026lt;ucHeap+80\u0026gt;, lr = 0x195 \u0026lt;prvQueuePingTask+52\u0026gt;, return_address = 0xe0000000, xpsr = 0x80000000 } We can clearly see that the executing instruction was 0xe0000000 and that the calling function was prvQueuePingTask.\nFrom the ARMv7-M reference manual15 we find:\nThe MPU is restricted in how it can change the default memory map attributes associated with System space, that is, for addresses 0xE0000000 and higher. System space is always marked as XN, Execute Never.\nSo the fault registers didn’t lie to us, and it does make sense that we hit a memory management fault!\nBad Address Read Code uint32_t read_from_bad_address(void) { return *(volatile uint32_t *)0xbadcafe; } Analysis (gdb) break main (gdb) continue Breakpoint 1, main () at ./cortex-m-fault-debug/main.c:189 189\txQueue = xQueueCreate(mainQUEUE_LENGTH, sizeof(unsigned long)); (gdb) set g_crash_config=1 (gdb) c Continuing. Program received signal SIGTRAP, Trace/breakpoint trap. 0x00000218 in my_fault_handler_c (frame=0x200005e8 \u0026lt;ucHeap+1152\u0026gt;) at ./cortex-m-fault-debug/startup.c:91 91\tHALT_IF_DEBUGGING(); Again, let’s take a look at the CFSR and see if it tells us anything useful.\n(gdb) p/x *(uint32_t*)0xE000ED28 $13 = 0x8200 (gdb) svd SCB CFSR_UFSR_BFSR_MMFSR Fields in SCB CFSR_UFSR_BFSR_MMFSR: [...] PRECISERR: 1 Precise data bus error [...] BFARVALID: 1 Bus Fault Address Register (BFAR) valid flag Great, we have a precise bus fault which means the return address in the stack frame holds the instruction which triggered the fault and that we can read BFAR to determine what memory access triggered the fault!\n(gdb) svd/x SCB BFAR Fields in SCB BFAR: BFAR: 0x0BADCAFE Bus fault address (gdb) p/a *frame $16 = { r0 = 0x1 \u0026lt;g_pfnVectors+1\u0026gt;, r1 = 0x200003c4 \u0026lt;ucHeap+604\u0026gt;, r2 = 0x10000000, r3 = 0xbadcafe, r12 = 0x200001b8 \u0026lt;ucHeap+80\u0026gt;, lr = 0x195 \u0026lt;prvQueuePingTask+52\u0026gt;, return_address = 0x13a \u0026lt;trigger_crash+22\u0026gt;, xpsr = 0x81000000 } (gdb) info line *0x13a Line 123 of \u0026quot;./cortex-m-fault-debug/main.c\u0026quot; starts at address 0x138 \u0026lt;trigger_crash+20\u0026gt; and ends at 0x13e \u0026lt;trigger_crash+26\u0026gt;. (gdb) list *0x13a 0x13a is in trigger_crash (./cortex-m-fault-debug/main.c:123). 118\tswitch (crash_id) { 119\tcase 0: 120\tillegal_instruction_execution(); 121\tbreak; 122\tcase 1: ===\u0026gt; FAULT HERE 123\tread_from_bad_address(); 124\tbreak; 125\tcase 2: 126\taccess_disabled_coprocessor(); 127\tbreak; Great, so we have pinpointed the exact code which triggered the issue and can now fix it!\nCoprocessor Fault Code void access_disabled_coprocessor(void) { // FreeRTOS will automatically enable the FPU co-processor. // Let's disable it for the purposes of this example __asm volatile( \u0026quot;ldr r0, =0xE000ED88 \\n\u0026quot; \u0026quot;mov r1, #0 \\n\u0026quot; \u0026quot;str r1, [r0]\t\\n\u0026quot; \u0026quot;dsb \\n\u0026quot; \u0026quot;vmov r0, s0 \\n\u0026quot; ); } Analysis (gdb) break main (gdb) continue Breakpoint 4, main () at ./cortex-m-fault-debug/main.c:180 180\txQueue = xQueueCreate(mainQUEUE_LENGTH, sizeof(unsigned long)); (gdb) set g_crash_config=2 (gdb) c Continuing. Program received signal SIGTRAP, Trace/breakpoint trap. 0x00000218 in my_fault_handler_c (frame=0x20002d80) at ./cortex-m-fault-debug/startup.c:91 91\tHALT_IF_DEBUGGING(); We can inspect CFSR to get a clue about the crash which took place\n(gdb) p/x *(uint32_t*)0xE000ED28 $13 = 0x8200 (gdb) svd SCB CFSR_UFSR_BFSR_MMFSR Fields in SCB CFSR_UFSR_BFSR_MMFSR: [...] NOCP: 1 No coprocessor usage fault. [...] We see it was a coprocessor UsageFault which tells us we either issued an instruction to a non-existent or disabled Cortex-M coprocessor. We know the frame contents are valid so we can inspect that to figure out where the fault originated:\n(gdb) p/a *frame $27 = { r0 = 0xe000ed88, r1 = 0x0 \u0026lt;g_pfnVectors\u0026gt;, r2 = 0x10000000, r3 = 0x0 \u0026lt;g_pfnVectors\u0026gt;, r12 = 0x200001b8 \u0026lt;ucHeap+80\u0026gt;, lr = 0x199 \u0026lt;prvQueuePingTask+52\u0026gt;, return_address = 0x114 \u0026lt;access_disabled_coprocessor+12\u0026gt;, xpsr = 0x81000000 } (gdb) disassemble 0x114 Dump of assembler code for function access_disabled_coprocessor: 0x00000108 \u0026lt;+0\u0026gt;:\tldr\tr0, [pc, #16]\t; (0x11c) 0x0000010a \u0026lt;+2\u0026gt;:\tmov.w\tr1, #0 0x0000010e \u0026lt;+6\u0026gt;:\tstr\tr1, [r0, #0] 0x00000110 \u0026lt;+8\u0026gt;:\tdsb\tsy ===\u0026gt; FAULT HERE on a Floating Point instruction 0x00000114 \u0026lt;+12\u0026gt;:\tvmov\tr0, s0 0x00000118 \u0026lt;+16\u0026gt;:\tbx\tlr vmov is a floating point instruction so we now know what coprocessor the NOCP was caused by. The FPU is enabled using bits 20-23 of the CPACR register located at 0xE000ED88. A value of 0 indicates the extension is disabled. Let’s check it:\n(gdb) p/x (*(uint32_t*)0xE000ED88 \u0026gt;\u0026gt; 20) \u0026amp; 0xf $29 = 0x0 We can clearly see the FP Extension is disabled. We will have to enable the FPU to fix our bug.\nImprecise Fault Code void bad_addr_double_word_write(void) { volatile uint64_t *buf = (volatile uint64_t *)0x30000000; *buf = 0x1122334455667788; } Analysis (gdb) break main (gdb) continue Breakpoint 4, main () at ./cortex-m-fault-debug/main.c:182 182\txQueue = xQueueCreate(mainQUEUE_LENGTH, sizeof(unsigned long)); (gdb) set g_crash_config=3 (gdb) c Continuing. Program received signal SIGTRAP, Trace/breakpoint trap. 0x0000021c in my_fault_handler_c (frame=0x200005e8 \u0026lt;ucHeap+1152\u0026gt;) at ./cortex-m-fault-debug/startup.c:91 91\tHALT_IF_DEBUGGING(); Let’s inspect CFSR:\n(gdb) p/x *(uint32_t*)0xE000ED28 $31 = 0x400 (gdb) svd SCB CFSR_UFSR_BFSR_MMFSR Fields in SCB CFSR_UFSR_BFSR_MMFSR: [...] IMPRECISERR: 1 Imprecise data bus error [...] Yikes, the error is imprecise. This means the stack frame will point to the general area where the fault occurred but not the exact instruction!\n(gdb) p/a *frame $32 = { r0 = 0x55667788, r1 = 0x11223344, r2 = 0x10000000, r3 = 0x30000000, r12 = 0x200001b8 \u0026lt;ucHeap+80\u0026gt;, lr = 0x199 \u0026lt;prvQueuePingTask+52\u0026gt;, return_address = 0x198 \u0026lt;prvQueuePingTask+52\u0026gt;, xpsr = 0x81000000 } (gdb) list *0x198 0x198 is in prvQueuePingTask (./cortex-m-fault-debug/main.c:162). 157 158\twhile (1) { 159\tvTaskDelayUntil(\u0026amp;xNextWakeTime, mainQUEUE_SEND_FREQUENCY_MS); 160\txQueueSend(xQueue, \u0026amp;ulValueToSend, 0U); 161 ==\u0026gt; Crash somewhere around here 162\ttrigger_crash(g_crash_config); 163\t} 164\t} 165 166\tstatic void prvQueuePongTask(void *pvParameters) { Analysis after making the Imprecise Error Precise If the crash was not readily reproducible we would have to inspect the code around this region and hypothesize what looks suspicious. However, recall that there is a trick we can use for the Cortex-M4 to make all memory stores precise. Let’s enable that and re-examine:\n(gdb) mon reset Resetting target (gdb) c Continuing. Breakpoint 4, main () at ./cortex-m-fault-debug/main.c:182 182\txQueue = xQueueCreate(mainQUEUE_LENGTH, sizeof(unsigned long)); (gdb) set g_crash_config=3 ==\u0026gt; Make all memory stores precise at the cost of performance ==\u0026gt; by setting DISDEFWBUF in the Cortex M3/M4 ACTLR reg (gdb) set *(uint32_t*)0xE000E008=(*(uint32_t*)0xE000E008 | 1\u0026lt;\u0026lt;1) (gdb) c Continuing. Program received signal SIGTRAP, Trace/breakpoint trap. 0x0000021c in my_fault_handler_c (frame=0x200005e8 \u0026lt;ucHeap+1152\u0026gt;) at ./cortex-m-fault-debug/startup.c:91 91\tHALT_IF_DEBUGGING(); (gdb) p/a *frame $33 = { r0 = 0x55667788, r1 = 0x11223344, r2 = 0x10000000, r3 = 0x30000000, r12 = 0x200001b8 \u0026lt;ucHeap+80\u0026gt;, lr = 0x199 \u0026lt;prvQueuePingTask+52\u0026gt;, return_address = 0xfa \u0026lt;bad_addr_double_word_write+10\u0026gt;, xpsr = 0x81000000 } (gdb) list *0xfa 0xfa is in bad_addr_double_word_write (./cortex-m-fault-debug/main.c:92). 90\tvoid bad_addr_double_word_write(void) { 91\tvolatile uint64_t *buf = (volatile uint64_t *)0x30000000; ==\u0026gt; FAULT HERE 92\t*buf = 0x1122334455667788; 93\t} (gdb) Awesome, that saved us some time … we were able to determine the exact line that caused the crash!\nFault Entry Exception Code void stkerr_from_psp(void) { extern uint32_t _start_of_ram[]; uint8_t dummy_variable; const size_t distance_to_ram_bottom = (uint32_t)\u0026amp;dummy_variable - (uint32_t)_start_of_ram; volatile uint8_t big_buf[distance_to_ram_bottom - 8]; for (size_t i = 0; i \u0026lt; sizeof(big_buf); i++) { big_buf[i] = i; } trigger_irq(); } Analysis (gdb) break main (gdb) continue Breakpoint 4, main () at ./cortex-m-fault-debug/main.c:182 182\txQueue = xQueueCreate(mainQUEUE_LENGTH, sizeof(unsigned long)); (gdb) set g_crash_config=4 (gdb) c Continuing. Program received signal SIGTRAP, Trace/breakpoint trap. 0x0000021c in my_fault_handler_c (frame=0x1fffffe0) at ./cortex-m-fault-debug/startup.c:91 91\tHALT_IF_DEBUGGING(); Let’s take a look at CFSR again to get a clue about what happened:\n(gdb) p/x *(uint32_t*)0xE000ED28 $39 = 0x1000 (gdb) svd SCB CFSR_UFSR_BFSR_MMFSR Fields in SCB CFSR_UFSR_BFSR_MMFSR: [...] STKERR: 1 Bus fault on stacking for exception entry Debug Tips when dealing with a STKERR There are two really important things to note when a stacking exception occurs:\nThe stack pointer will always reflect the correct adjusted position as if the hardware successfully stacked the registers. This means you can find the stack pointer prior to exception entry by adding the adjustment value. Depending on what access triggers the exception, the stacked frame may be partially valid. For example, the very last store of the hardware stacking could trigger the fault and all the other stores could be valid. However, the order the hardware pushes register state on the stack is implementation specific. So when inspecting the frame assume the values being looked at may be invalid! Taking this knowledge into account, let’s examine the stack frame:\n(gdb) p frame $40 = (sContextStateFrame *) 0x1fffffe0 Interestingly, if we look up the memory map of the NRF5216, we will find that RAM starts at 0x20000000. Our stack pointer location, 0x1fffffe0 is right below that in an undefined memory region. This must be why we faulted! We see that the stack pointer is 32 bytes below RAM, which matches the size of sContextStateFrame. This unfortunately means none of the values stacked will be valid since all stores were issued to a non-existent address space!\nWe can manually walk up the stack to get some clues:\n(gdb) x/a 0x20000000 0x20000000 \u0026lt;uxCriticalNesting\u0026gt;:\t0x3020100 (gdb) 0x20000004 \u0026lt;g_crash_config\u0026gt;:\t0x7060504 (gdb) 0x20000008 \u0026lt;xQueue\u0026gt;:\t0xb0a0908 (gdb) 0x2000000c \u0026lt;s_buffer\u0026gt;:\t0xf0e0d0c (gdb) 0x20000010 \u0026lt;s_buffer+4\u0026gt;:\t0x13121110 (gdb) 0x20000014 \u0026lt;s_buffer+8\u0026gt;:\t0x17161514 (gdb) 0x20000018 \u0026lt;pxCurrentTCB\u0026gt;:\t0x1b1a1918 (gdb) 0x2000001c \u0026lt;pxDelayedTaskList\u0026gt;:\t0x1f1e1d1c (gdb) 0x20000020 \u0026lt;pxOverflowDelayedTaskList\u0026gt;:\t0x23222120 It looks like the RAM has a pattern of sequentially increasing values and that the RAM addresses map to different variables in our code (i.e pxCurrentTCB). This suggests we overflowed the stack we were using and started to clobber RAM in the system until we ran off the end of RAM!\nTIP: To catch this type of failure sooner consider using an MPU Region\nSince the crash is reproducible, let’s leverage a watchpoint and see if we can capture the stack corruption in action! Let’s add a watchpoint for any access near the bottom of RAM, 0x2000000c:\n(gdb) mon reset (gdb) continue Breakpoint 4, main () at ./cortex-m-fault-debug/main.c:182 182\txQueue = xQueueCreate(mainQUEUE_LENGTH, sizeof(unsigned long)); (gdb) set g_crash_config=4 (gdb) watch *(uint32_t*)0x2000000c Hardware watchpoint 9: *(uint32_t*)0x2000000c TIP: Sometimes it will take a couple tries to choose the right RAM range to watch. It’s possible an area of the stack never gets written to and the watchpoint never fires or that the memory address being watched gets updated many many times before the actual failure. In this example, I intentionally opted not to watch 0x20000000 because that is the address of a FreeRTOS variable, uxCriticalNesting which is updated a lot.\nLet’s continue and see what happens:\n(gdb) continue Hardware watchpoint 9: *(uint32_t*)0x2000000c Old value = 0 New value = 12 0x000000c0 in stkerr_from_psp () at ./cortex-m-fault-debug/main.c:68 68\tbig_buf[i] = i; (gdb) bt #0 0x000000c0 in stkerr_from_psp () at ./cortex-m-fault-debug/main.c:68 #1 0x00000198 in prvQueuePingTask (pvParameters=\u0026lt;optimized out\u0026gt;) at ./cortex-m-fault-debug/main.c:162 #2 0x00001488 in ?? () at ./cortex-m-fault-debug/freertos_kernel/portable/GCC/ARM_CM4F/port.c:703 Backtrace stopped: previous frame identical to this frame (corrupt stack?) (gdb) list *0xc0 0xc0 is in stkerr_from_psp (./cortex-m-fault-debug/main.c:68). 63\textern uint32_t _start_of_ram[]; 64\tuint8_t dummy_variable; 65\tconst size_t distance_to_ram_bottom = (uint32_t)\u0026amp;dummy_variable - (uint32_t)_start_of_ram; 66\tvolatile uint8_t big_buf[distance_to_ram_bottom - 8]; 67\tfor (size_t i = 0; i \u0026lt; sizeof(big_buf); i++) { 68\tbig_buf[i] = i; 69\t} 70 71\ttrigger_irq(); 72\t} Great, we’ve found a variable located on the stack big_buf being updated. It must be this function call path which is leading to a stack overflow. We can now inspect the call chain and remove big stack allocations!\nRecovering from a UsageFault without a SYSRESET In this example we’ll just step through the code we developed above and confirm we don’t reset when a UsageFault occurs.\nCode void unaligned_double_word_read(void) { extern void *g_unaligned_buffer; uint64_t *buf = g_unaligned_buffer; *buf = 0x1122334455667788; } Analysis (gdb) break main (gdb) continue Breakpoint 4, main () at ./cortex-m-fault-debug/main.c:188 188\txQueue = xQueueCreate(mainQUEUE_LENGTH, sizeof(unsigned long)); (gdb) set g_crash_config=5 (gdb) c Continuing. Program received signal SIGTRAP, Trace/breakpoint trap. 0x00000228 in my_fault_handler_c (frame=0x200005e8 \u0026lt;ucHeap+1152\u0026gt;) at ./cortex-m-fault-debug/startup.c:94 94\tHALT_IF_DEBUGGING(); We have entered the breakpoint in the fault handler. We can step over it and confirm we fall through to the recover_from_task_fault function.\n(gdb) break recover_from_task_fault Breakpoint 12 at 0x1a8: file ./cortex-m-fault-debug/main.c, line 181. (gdb) n 108\tvolatile uint32_t *cfsr = (volatile uint32_t *)0xE000ED28; (gdb) c Continuing. Breakpoint 12, recover_from_task_fault () at ./cortex-m-fault-debug/main.c:181 181\tvoid recover_from_task_fault(void) { (gdb) list *recover_from_task_fault 0x1a8 is in recover_from_task_fault (./cortex-m-fault-debug/main.c:181). 181\tvoid recover_from_task_fault(void) { 182\twhile (1) { 183\tvTaskDelay(1); 184\t} 185\t} If we continue from here we will see the system happily keeps running because the thread which was calling the problematic trigger_crash function is now parked in a while loop. The the while loop could be extended in the future to delete and/or restart the FreeRTOS task if we wanted as well.\nClosing I hope this post gave you a useful overview of how to debug a HardFault on a Cortex-M MCU and that maybe you even learned something new!\nAre there tricks you like to use that I didn’t mention or other topics about faults you’d like to learn more about? Let us know in the discussion area below!\nInterested in learning more about debugging HardFaults? Watch this webinar recording..\nSee anything you\u0026rsquo;d like to change? Submit a pull request or open an issue at GitHub\nReferences The Tower of Terror: A Bug Mystery ↩ ↩2\nSee “A4.1.1 ARMv7-M and interworking support” ↩\nSegger JTrace \u0026amp; Lauterbach Trace32 are both capable of analyzing the ETM ↩\nSee “3.3.9 Auxiliary Bus Fault Status Register” ↩ ↩2\nSee “5.3.6.8 Reset behavior” ↩\nMBed OS fault handler ↩\nZephyr ARM fault handler ↩\nCMSIS-SVD ↩\nCMSIS Software Packs ↩\nPyCortexMDebug ↩\nSee “B1.5.5 Reset behavior” \u0026amp; “B1.4.2 The special-purpose program status registers, xPSR” ↩\nnRF52840 Development Kit ↩\nJLinkGDBServer ↩\nGNU ARM Embedded toolchain for download ↩\nSee B3.5.1 “Relation of the MPU to the system memory map” ↩\nSee “4.2.3 Memory map” ↩\nChris Coleman is a founder and CTO at Memfault. Prior to founding Memfault, Chris worked on the embedded software teams at Sun, Pebble, and Fitbit.\nDiscourseEmbed = { discourseUrl: \u0026lsquo;https://community.memfault.com/', discourseEmbedUrl: \u0026lsquo;https://interrupt.memfault.com/blog/cortex-m-hardfault-debug' }; (function() { var d = document.createElement(\u0026lsquo;script\u0026rsquo;); d.type = \u0026rsquo;text/javascript\u0026rsquo;; d.async = true; d.src = DiscourseEmbed.discourseUrl + \u0026lsquo;javascripts/embed.js\u0026rsquo;; (document.getElementsByTagName(\u0026lsquo;head\u0026rsquo;)[0] || document.getElementsByTagName(\u0026lsquo;body\u0026rsquo;)[0]).appendChild(d); })(); anchors.options.visible = \u0026lsquo;hover\u0026rsquo;; anchors.add(\u0026rsquo;.post-content \u0026gt; h1, h2, h3, h4, h5, h6\u0026rsquo;);\n","permalink":"https://fanyxok.github.io/blog/p01-cortex-m-fault/","summary":"How to debug a HardFault on an ARM Cortex-M MCU 20 Nov 2019 by Chris Coleman\nFaults happen on embedded devices all the time for a variety of reasons – ranging from something as simple as a NULL pointer dereference to something more unexpected like running a faulty code path only when in a zero-g environment on the Tower of Terror in Disneyland1. It’s important for any embedded engineer to understand how to debug and resolve this class of issue quickly.","title":"How to debug a HardFault on an ARM Cortex-M MCU"},{"content":"32-bit 表示一个单精度浮点数 float\n$V=(-1)^s \\times M \\times 2^E$ Bit Segments 32-bit 分为3部分： S, E, M\nS: 符号位，1bit，0 表示正数， -1 表示负数 E: 指数位，8bit，取值[-127, 128]. M: 尾数位，23bit，取值[0, $2^{23}-1$]. 指数的底固定为2，指数有正数也有负数，但指数部分没有符号位，取一个中间值，小于中间值的表示负数，等于中间值的表示0，大于中间值的表示正数，中间值的定义如下：Bias=$2^{k-1}$-1, k表示E的bit数。实际的E为E-Bias.\n尾数部分隐藏了小数点前的1，实际为1.M。M转换为十进制从高位开始计算： $1+b_{22}\\times\\frac{1}{2}+b_{21}\\times\\frac{1}{4}+\u0026hellip;+b_{0}\\times\\frac{1}{2^{23}}$\n规格化的表示，E不是全0，也不是全1，M任意取值\nE全为0时，表示+0.0, -0.0和接近0.0的值\nE全为1，M全为0时，表示无穷，符号位区分正无穷和负无穷\nE全为1，M不全为0时，表示NaN，不是一个数。\n","permalink":"https://fanyxok.github.io/blog/ieee745/","summary":"32-bit 表示一个单精度浮点数 float\n$V=(-1)^s \\times M \\times 2^E$ Bit Segments 32-bit 分为3部分： S, E, M\nS: 符号位，1bit，0 表示正数， -1 表示负数 E: 指数位，8bit，取值[-127, 128]. M: 尾数位，23bit，取值[0, $2^{23}-1$]. 指数的底固定为2，指数有正数也有负数，但指数部分没有符号位，取一个中间值，小于中间值的表示负数，等于中间值的表示0，大于中间值的表示正数，中间值的定义如下：Bias=$2^{k-1}$-1, k表示E的bit数。实际的E为E-Bias.\n尾数部分隐藏了小数点前的1，实际为1.M。M转换为十进制从高位开始计算： $1+b_{22}\\times\\frac{1}{2}+b_{21}\\times\\frac{1}{4}+\u0026hellip;+b_{0}\\times\\frac{1}{2^{23}}$\n规格化的表示，E不是全0，也不是全1，M任意取值\nE全为0时，表示+0.0, -0.0和接近0.0的值\nE全为1，M全为0时，表示无穷，符号位区分正无穷和负无穷\nE全为1，M不全为0时，表示NaN，不是一个数。","title":"IEEE 745 floating point representation"},{"content":"AES 对称密钥加密中最流行的算法之一, 除加密以外还可以用于构建消息认证码, 伪随机函数等密码工具\nAES的区块长度固定为128比特，密钥长度则可以是128，192或256比特\n分组密码工作模式 使用同一个区块密码密钥对多于一块的数据进行加密\n区块密码自身只能加密长度等于密码区块长度的单块数据，若要加密变长数据，则数据必须先被划分为一些单独的密码块\n最后一块数据也需要使用合适填充方式将数据扩展到符合密码块大小的长度\n一种工作模式描述了加密每一数据块的过程，并常常使用基于一个通常称为初始化向量的附加输入值以进行随机化，以保证安全。\n初始化向量IV 许多工作模式中用于将加密随机化的一个位块，由此即使同样的明文被多次加密也会产生不同的密文，避免了较慢的重新产生密钥的过程。\n初始化向量与密钥相比有不同的安全性需求，因此IV通常无须保密，然而在大多数情况中，不应当在使用同一密钥的情况下两次使用同一个IV. 对于CBC和CFB，重用IV会导致泄露明文首个块的某些信息，亦包括两个不同消息中相同的前缀。 对于OFB和CTR而言，重用IV会导致完全失去安全性。\n在CBC模式中，IV在加密时必须是无法预测的. SSL2.0使用的采用上一个消息的最后一块密文作为下一个消息的IV，是不安全的\nPadding ECB和CBC需要最后一块在加密前进行填充\n在明文的最后填充空字符以使其长度为块长度的整数倍 在数据后添加一个1位，再添加足够的0位直到满足块长度的要求 添加一个值为128的字节（十六进制的80），再以0字节填满最后一个块 向最后一个块填充n个值均为n的字节 密文窃取 等等 常用模式 电子密码本（ECB） 需要加密的消息按照块密码的块大小被分为数个块，并对每个块进行独立加密。\n缺点在于同样的明文块会被加密成相同的密文块；它不能很好的隐藏数据模式。在某些场合，这种方法不能提供严格的数据保密性，因此并不推荐用于密码协议中。\n易受到重放攻击的影响，因为每个块是以完全相同的方式解密的.\n密码块链接（CBC） 每个明文块先与前一个密文块进行异或后，再进行加密. 为了保证每条消息的唯一性，在第一个块中需要使用初始化向量。\n主要缺点在于加密过程是串行的，无法被并行化，而且消息必须被填充到块大小的整数倍。在解密时，从两个邻接的密文块中即可得到一个明文块。因此，解密过程可以被并行化.\n填充密码块链接（PCBC） 每个明文块先与前一个明文块和密文块进行异或后再进行加密, 互换两个邻接的密文块不会对后续块的解密造成影响\n密文反馈（CFB，Cipher feedback） 类似于CBC，可以将块密码变为自同步的流密码；工作过程亦非常相似，CFB的解密过程几乎就是颠倒的CBC的加密过程\n输出反馈模式（Output feedback, OFB） 将块密码变成同步的流密码。它产生密钥流的块，然后将其与明文块进行异或，得到密文。 与其它流密码一样，密文中一个位的翻转会使明文中同样位置的位也产生翻转。这种特性使得许多错误校正码，例如奇偶校验位，即使在加密前计算，而在加密后进行校验也可以得出正确结果。 每个使用OFB的输出块与其前面所有的输出块相关，因此不能并行化处理。然而，由于明文和密文只在最终的异或过程中使用，因此可以事先对IV进行加密，最后并行的将明文或密文进行并行的异或处理。\n计数器模式（CTR） CTR模式的特征类似于OFB，但它允许在解密时进行随机存取。由于加密和解密过程均可以进行并行处理，CTR适合运用于多处理器的硬件上。\n其他 消息认证码（MAC）通常由块密码得到，例如CBC-MAC（英语：CBC-MAC），OMAC（英语：One-key_MAC）和PMAC（英语：PMAC_(cryptography)）。\n认证加密也采用块密码作为其中的一部，其同时使用加密和MAC以提供保密性和数据完整性，例如IAPM（英语：IAPM_(mode)），CCM（英语：CCM_mode），CWC（英语：CWC_mode），EAX（英语：EAX_mode），GCM（英语：Galois/Counter_Mode）和OCB（英语：OCB_mode）。\n","permalink":"https://fanyxok.github.io/blog/aes/","summary":"AES 对称密钥加密中最流行的算法之一, 除加密以外还可以用于构建消息认证码, 伪随机函数等密码工具\nAES的区块长度固定为128比特，密钥长度则可以是128，192或256比特\n分组密码工作模式 使用同一个区块密码密钥对多于一块的数据进行加密\n区块密码自身只能加密长度等于密码区块长度的单块数据，若要加密变长数据，则数据必须先被划分为一些单独的密码块\n最后一块数据也需要使用合适填充方式将数据扩展到符合密码块大小的长度\n一种工作模式描述了加密每一数据块的过程，并常常使用基于一个通常称为初始化向量的附加输入值以进行随机化，以保证安全。\n初始化向量IV 许多工作模式中用于将加密随机化的一个位块，由此即使同样的明文被多次加密也会产生不同的密文，避免了较慢的重新产生密钥的过程。\n初始化向量与密钥相比有不同的安全性需求，因此IV通常无须保密，然而在大多数情况中，不应当在使用同一密钥的情况下两次使用同一个IV. 对于CBC和CFB，重用IV会导致泄露明文首个块的某些信息，亦包括两个不同消息中相同的前缀。 对于OFB和CTR而言，重用IV会导致完全失去安全性。\n在CBC模式中，IV在加密时必须是无法预测的. SSL2.0使用的采用上一个消息的最后一块密文作为下一个消息的IV，是不安全的\nPadding ECB和CBC需要最后一块在加密前进行填充\n在明文的最后填充空字符以使其长度为块长度的整数倍 在数据后添加一个1位，再添加足够的0位直到满足块长度的要求 添加一个值为128的字节（十六进制的80），再以0字节填满最后一个块 向最后一个块填充n个值均为n的字节 密文窃取 等等 常用模式 电子密码本（ECB） 需要加密的消息按照块密码的块大小被分为数个块，并对每个块进行独立加密。\n缺点在于同样的明文块会被加密成相同的密文块；它不能很好的隐藏数据模式。在某些场合，这种方法不能提供严格的数据保密性，因此并不推荐用于密码协议中。\n易受到重放攻击的影响，因为每个块是以完全相同的方式解密的.\n密码块链接（CBC） 每个明文块先与前一个密文块进行异或后，再进行加密. 为了保证每条消息的唯一性，在第一个块中需要使用初始化向量。\n主要缺点在于加密过程是串行的，无法被并行化，而且消息必须被填充到块大小的整数倍。在解密时，从两个邻接的密文块中即可得到一个明文块。因此，解密过程可以被并行化.\n填充密码块链接（PCBC） 每个明文块先与前一个明文块和密文块进行异或后再进行加密, 互换两个邻接的密文块不会对后续块的解密造成影响\n密文反馈（CFB，Cipher feedback） 类似于CBC，可以将块密码变为自同步的流密码；工作过程亦非常相似，CFB的解密过程几乎就是颠倒的CBC的加密过程\n输出反馈模式（Output feedback, OFB） 将块密码变成同步的流密码。它产生密钥流的块，然后将其与明文块进行异或，得到密文。 与其它流密码一样，密文中一个位的翻转会使明文中同样位置的位也产生翻转。这种特性使得许多错误校正码，例如奇偶校验位，即使在加密前计算，而在加密后进行校验也可以得出正确结果。 每个使用OFB的输出块与其前面所有的输出块相关，因此不能并行化处理。然而，由于明文和密文只在最终的异或过程中使用，因此可以事先对IV进行加密，最后并行的将明文或密文进行并行的异或处理。\n计数器模式（CTR） CTR模式的特征类似于OFB，但它允许在解密时进行随机存取。由于加密和解密过程均可以进行并行处理，CTR适合运用于多处理器的硬件上。\n其他 消息认证码（MAC）通常由块密码得到，例如CBC-MAC（英语：CBC-MAC），OMAC（英语：One-key_MAC）和PMAC（英语：PMAC_(cryptography)）。\n认证加密也采用块密码作为其中的一部，其同时使用加密和MAC以提供保密性和数据完整性，例如IAPM（英语：IAPM_(mode)），CCM（英语：CCM_mode），CWC（英语：CWC_mode），EAX（英语：EAX_mode），GCM（英语：Galois/Counter_Mode）和OCB（英语：OCB_mode）。","title":"AES对称加密"},{"content":"Welcome The first post of hugo\nThis is a second header The third The fouth Fifty Six? amazing\nA math equation: \\(c^2 = a^2 + b^2\\)\nfull line equation: $$ e^e = \\pi $$\nA Table a a b b A complex latex equation $$\\begin{array}{cc} a \u0026amp; b \\\\ c \u0026amp; d \\end{array}$$\n$$\\begin{bmatrix} a \u0026amp; b \\newline c \u0026amp; d \\end{bmatrix}$$\n","permalink":"https://fanyxok.github.io/blog/my-first-post/","summary":"Welcome The first post of hugo\nThis is a second header The third The fouth Fifty Six? amazing\nA math equation: \\(c^2 = a^2 + b^2\\)\nfull line equation: $$ e^e = \\pi $$\nA Table a a b b A complex latex equation $$\\begin{array}{cc} a \u0026amp; b \\\\ c \u0026amp; d \\end{array}$$\n$$\\begin{bmatrix} a \u0026amp; b \\newline c \u0026amp; d \\end{bmatrix}$$","title":"My First Post"},{"content":"Cyclic Group 定义 $$\\langle\\mathbb{G},q,g \\rangle$$\n\\(\\mathbb{G}=\\langle \\mathbb{Z}_{n}, \\cdot \\rangle\\)\n\\(\\mathbb{Z}_{n} \\)是一个集合, {$0$\u0026hellip;$n-1$}.\n\\(\\cdot\\)是集合中的运算符.\n对\\(\\mathbb{G}\\)中的元素$a$进行$k$次幂运算表示为\\(a^{k}=a \\cdot a \u0026hellip;\\cdot a\\), 即$k$个$a$进行$\\cdot$运算.\n若群$\\mathbb{G}$的每一个元素都是$\\mathbb{G}$的某一个固定元素$a$的幂，则称$\\mathbb{G}$为循环群.\n$q$是$\\mathbb{G}$的order, $q$的值等于$\\mathbb{G}$中元素的个数, 也记为$|\\mathbb{G}|$.\n$g$是$\\mathbb{G}$的generator(生成元), $\\mathbb{G}$中的所有元素都能由$g$通过幂运算生成.\n由群$\\mathbb{G}$, 阶$q$和生成元$g$, 即可定义一个循环群.\n如何寻找一个生成元 如果$\\mathbb{G}$有素数阶$p$, 则$\\mathbb{G}$中除了identity之外的所有元素都是$\\mathbb{G}$的生成元.\n如果$p$是素数, 则$\\mathbb{Z}_{p}^{*}$是阶为$p-1$的循环群.\n假设$\\mathbb{G}$阶非素数$p$, 可以均匀的从$\\mathbb{G}$中采样元素, 直到这个元素是一个生成元.\n$\\mathbb{G}$的阶$q$有素数因数$\\{p_{i}\\}^k_{i=1}$, 检查元素$h$是否为生成元\nfor $i = 1$ to $k$ : if $h^{q/p_{i}}=1$ return \u0026ldquo;$h$ is not a generator\u0026rdquo; return \u0026ldquo;$h$ is a generator\u0026rdquo; $\\mathbb{G}$有阶$q$,生成元$g$, 则任意一个元素可以表示为$h=g^{x}$.\n当gcd($x,q$)=1时, $h$也是$\\mathbb{G}$的一个生成元. Oblivious Transfer from Cyclic Group Reference:\nMore efficient oblivious transfer and extensions for faster secure computation\nAsharov G, Lindell Y, Schneider T, et al.\nProceedings of the 2013 ACM SIGSAC conference on Computer \u0026amp; communications security.\n以下方案基于DDH假设.\nFast Notes Input Sender and Receiver agree on $\\langle\\mathbb{G},q,g \\rangle$. Sender and Receiver agree on Hash. Sender: n个 ($x_{i}^{0}, x_{i}^{1}$) pair Receiver: n个 bit ($\\sigma_{1},\u0026hellip;,\\sigma_{n}$) Receiver do $\\alpha_{i}\\in_{R}\\mathbb{Z}_{q}$ $h_{i}\\in_{R}\\mathbb{G}$ $h_{i}^{0}= (\\sigma_{i} == 0) \\ ?\\ g^{\\alpha_{i}} : h_{i}$ $h_{i}^{1}= (\\sigma_{i} == 0) \\ ?\\ h_{i} : g^{\\alpha_{i}}$ send $n$ pair $(h_{i}^{0}, h_{i}^{1})$ to Sender. Sender do $r\\in_{R}\\mathbb{Z}_{q}$ $u=g^{r}$ $k_{i}^{0}, k_{i}^{1} = (h_{i}^{0})^{r}, (h_{i}^{1})^{r}$ $v_{i}^{0}, v_{i}^{1} = x_{i}^{0} \\oplus \\text{Hash}(k_{i}^{0}), x_{i}^{1} \\oplus \\text{Hash}(k_{i}^{1})$ send $u$ to Receiver. send n pair $(v_{i}^{0}, v_{i}^{1})$. Receiver do $k_{i}^{\\sigma_{i}} = u^{\\alpha_{i}}$ $x_{i}^{\\sigma_{i}} = v_{i}^{\\sigma_{i}} \\oplus \\text{Hash}(k_{i}^{\\sigma_{i}})$ Receiver outputs $\\{x_{i}^{\\sigma_i}\\}$, Sender has no output. Discussion $x$应当和Hash的长度保持一致, 或者小于Hash的长度, 否则$v$的头几位会泄露$x$的信息. 如果Hash的长度与$k$的长度一致, 由于$k$是$\\mathbb{G}$中的元素, $x$也应是$\\mathbb{G}$中的元素, 否则会泄露$x$的信息. 最好的情况是, S和R约定好Hash, 可以从较短的$k$中产生较长的Hash$(k)$, 那么此时也应约定好$x$的bit长度$l$. OT传输的信息长度可能会变, 也可能太长, 如果导致计算的开销过高, 或许可以通过与对称密码结合的方法, 用OT传输秘钥, 用秘钥加密$x$, 从而让Receiver获得秘钥来解密密文. 此时对称加密的秘钥长度就可以固定下来. 不过对称加密如AES本身秘钥的长度就达到了129,192和256, 或许已经超过了$\\mathbb{G}$的长度. MPC中最用的两中有限域是$prime$域和$2k$域$\\mathbb{G}_{p}$和$\\mathbb{G}_{2^k}$, 理论上来说素数域的效率会更高一些, 而2k域正好对应计算时32bit, 64bi的数据范围. ","permalink":"https://fanyxok.github.io/blog/otandcylicgroup/","summary":"Cyclic Group 定义 $$\\langle\\mathbb{G},q,g \\rangle$$\n\\(\\mathbb{G}=\\langle \\mathbb{Z}_{n}, \\cdot \\rangle\\)\n\\(\\mathbb{Z}_{n} \\)是一个集合, {$0$\u0026hellip;$n-1$}.\n\\(\\cdot\\)是集合中的运算符.\n对\\(\\mathbb{G}\\)中的元素$a$进行$k$次幂运算表示为\\(a^{k}=a \\cdot a \u0026hellip;\\cdot a\\), 即$k$个$a$进行$\\cdot$运算.\n若群$\\mathbb{G}$的每一个元素都是$\\mathbb{G}$的某一个固定元素$a$的幂，则称$\\mathbb{G}$为循环群.\n$q$是$\\mathbb{G}$的order, $q$的值等于$\\mathbb{G}$中元素的个数, 也记为$|\\mathbb{G}|$.\n$g$是$\\mathbb{G}$的generator(生成元), $\\mathbb{G}$中的所有元素都能由$g$通过幂运算生成.\n由群$\\mathbb{G}$, 阶$q$和生成元$g$, 即可定义一个循环群.\n如何寻找一个生成元 如果$\\mathbb{G}$有素数阶$p$, 则$\\mathbb{G}$中除了identity之外的所有元素都是$\\mathbb{G}$的生成元.\n如果$p$是素数, 则$\\mathbb{Z}_{p}^{*}$是阶为$p-1$的循环群.\n假设$\\mathbb{G}$阶非素数$p$, 可以均匀的从$\\mathbb{G}$中采样元素, 直到这个元素是一个生成元.\n$\\mathbb{G}$的阶$q$有素数因数$\\{p_{i}\\}^k_{i=1}$, 检查元素$h$是否为生成元\nfor $i = 1$ to $k$ : if $h^{q/p_{i}}=1$ return \u0026ldquo;$h$ is not a generator\u0026rdquo; return \u0026ldquo;$h$ is a generator\u0026rdquo; $\\mathbb{G}$有阶$q$,生成元$g$, 则任意一个元素可以表示为$h=g^{x}$.\n当gcd($x,q$)=1时, $h$也是$\\mathbb{G}$的一个生成元. Oblivious Transfer from Cyclic Group Reference:\nMore efficient oblivious transfer and extensions for faster secure computation","title":"Oblivious Transfer and Cyclic Group"},{"content":"SSH to RPI (Ethernet Cable) ssh fanyx@pi.local where pi is the hostname of rpi.\nThis instruction doesn\u0026rsquo;t need a address.\n","permalink":"https://fanyxok.github.io/blog/device-rpi-4b/","summary":"SSH to RPI (Ethernet Cable) ssh fanyx@pi.local where pi is the hostname of rpi.\nThis instruction doesn\u0026rsquo;t need a address.","title":""},{"content":"Linux Kernel Module Programming Guide [TOC]\nEasy Begin 内核模块包 编程内核模块首先要安装内核模块包kmod.\nsudo apt-get install build-essential kmod 命令行的内核模块相关命令 sudo lsmod # 查看所有加载进内核的模块 sudo cat /proc/modules # 和lsmod有什么不同 sudo modinfo ABC_x.ko # 查看模块ABC的信息 sudo insmod ABC_x.ko # 向内核中加载模块ABC sudo rmmod ABC-x # 从内核中卸载模块ABC, 注意ABC_x.ko名称中的\u0026#39;_\u0026#39;字符会被内核自动替换成\u0026#39;-\u0026#39;字符 sudo journalctl --since \u0026#34;1 hour ago\u0026#34; | grep kernel # 查看过去一小时内的内核日志 编码前须知 版本. 如果编译模块A时include的内核头文件版本与加载模块A的内核版本不一致, 会导致加载失败. 如果必须要这样做, 例如模块A兼容多个内核版本, 需要启用CONFIG_MODVERSIONS编译制导. 命令行. 内核模块无法打印输出到屏幕,但可以记录日志信息然后读取日志信息到屏幕. SecureBoot. SecureBoot是UEFI的一个设置, 要求只会引导安全密钥签名过的模块. [Shell]ERROR: could not insert module. [dmesg]Lockdown: insmod: unsigned module loading is restricted; see man kernel lockdown.7. SecureBoot - Debian Wiki 内核头文件 编译模块必需内核头文件, 编译时Makefile也需要include内核头文件所在文件夹.\n// 查看系统内核版本 \u0026gt; uname -r 6.1.21-v8+ sudo apt-get update // 查看对于内核头文件是否在源仓库中 sudo apt-cache search linux-headers-`uname -r` // 下载(到/usr/src文件夹下) sudo apt-get install linux-headers-`uname -r` rpi的rpi-os中的Debian源里没有对应版本的内核头文件, 但是rpi自己提供了一份.\n// 不需要指定版本 sudo apt-get install raspberrypi-kernel-headers 编码规范 缩进使用tabs而非spaces. 提交patch到上游时必须遵守. pr_info和pr_debugin include/linux/printk.h kbuild, Documentation/kbuild/modules.rst Makefile, Documentation/kbuild/makefiles.rst 一个极简内核模块示例 #include \u0026lt;linux/module.h\u0026gt; /* needed by all modules */ #include \u0026lt;linux/init.h\u0026gt; /* needed for macros */ #include \u0026lt;linux/printk.h\u0026gt; /* needed for pr_info */ /* __initdata macro修饰下的变量在init完成时会被丢弃并释放内存(适用于built-in driver, 对loadable driver没有影响) */ static int hello_data __initdata = 3; /* 模块接受一个基本类型参数的声明方法 */ static int input_data = 1; module_param(input_data, int, S_IRUSR|S_IWUSR|S_IRGRP|S_IWGRP); MODULE_PARM_DESC(input_data, \u0026#34;a int input\u0026#34;); /* 模块接收一个字符串参数的声明方法 */ static char* input_string = \u0026#34;???\u0026#34;; // 字符串的最大长度不需要定义 module_param(input_string, charp, 0000); MODULE_PARM_DESC(input_string, \u0026#34;a string input\u0026#34;); /* 模块接受一个数组类型参数的声明方法 */ static int input_array[2] = {0, 1}; // 预先定义数组的最大长度, 输入数组不足最大长度的位置依然保持这里静态声明的默认值 static int input_array_argc = 0; // number of array elements module_param_array(input_array, int, \u0026amp;input_array_argc, 0000); MODULE_PARM_DESC(input_array, \u0026#34;a int array input\u0026#34;); /* __init macro 修饰下的init函数在完成时会被丢弃并释放内存(适用于built-in driver, 对loadable driver没有影响) 因为built-in driver只加载进内核一次 */ static int __init hello_init(void) { pr_info(\u0026#34;hello %d %d\\n\u0026#34;, hello_data, input_data); return 0; } /* __exit macro 修饰下的exit函数会被忽略(适用于built-in driver, 对loadable driver没有影响) 因为built-in driver不会被卸载出内核 */ static int __exit hello_exit(void) { pr_info(\u0026#34;bey\\n\u0026#34;); return 0; } module_init(hello_init); module_exit(hello_exit); MODULE_LICENSE(\u0026#34;GPL\u0026#34;); MODULE_AUTHOR(\u0026#34;fanyx\u0026#34;); MODULE_DESCRIPTION(\u0026#34;A simple driver\u0026#34;); # 加载内核时, 按照名称赋参数值 sudo insmod hello.ko input_int=1 input_string=\u0026#34;hehe\u0026#34; input_array=-1,-1 编译多个文件组成的模块 \u0026gt; ls start.c stop.c obj-m += hello.o # 增加一个object hello hello-objs := start.o stop.o # 声明hello object包含的文件 PWD := %(CURDIR) all: make -C /lib/modules/$(shell uname -r)/build M=$(PWD) modules clean: make -C /lib/modules/$(shell uname -r)/build M=$(PWD) clean 设备驱动程序视角下的内核模块 Linux Kernel Module的一个重要类型是Device Driver. 物理设备抽象为文件，存放在/dev目录下面。通过 cat /proc/devices 可以查看所有设备。\nDevice分为两种，Char Device和Block Device\n模块如何开始和结束 模块可用的函数 dmesg读取模块日志 Message logging with printk — The Linux Kernel documentation\nName String Alias Function KERN_EMERG \u0026ldquo;0\u0026rdquo; pr_emerg() KERN_ALERT \u0026ldquo;1\u0026rdquo; pr_alert() KERN_CRIT \u0026ldquo;2\u0026rdquo; pr_crit() KERN_ERR \u0026ldquo;3\u0026rdquo; pr_err() KERN_WARNING \u0026ldquo;4\u0026rdquo; pr_warn() KERN_NOTICE \u0026ldquo;5\u0026rdquo; pr_notice() KERN_INFO \u0026ldquo;6\u0026rdquo; pr_info() KERN_DEBUG \u0026ldquo;7\u0026rdquo; pr_debug() and pr_devel() if DEBUG is defined # 查看当前console loglevel $ cat /proc/sys/kernel/printk 4 4 1 7 #current,default,minimun,boot-time-default # 修改当前console loglevel $ dmesg -n 5 优先级高于(越小越高)当前console_loglevel的日志会立刻出现在console上.\nUser Space vs Kernel Space Name Space Code Space Device Driver 字符设备驱动程序 Charactor Device Driver\nstruct file_operation struct file 注册一个设备 注销一个设备 示例程序 兼容多个内核版本 /proc: Module向Process发送信息 struct proc_ops 读、写/proc 文件 通过标准文件系统管理/proc文件 通过seq_file管理/proc文件 /sys：读写Module内部的变量 与设备文件对话 using device file to write things to the physical device write device\u0026rsquo;s commands write data to be sent through the device using device file to read things from the physical device read responses for device\u0026rsquo;s commands read data received through the device Unix ioctl机制实现以上功能\nSystem Calls System Call是真正的process to kernel communication mechanism 一般来说，process不能访问kernel(既不能访问kernel memory也不能调用kernel function)，这是CPU从硬件上保证的(称为protected mode 或者 page protection) system call是一个例外。process向特定的寄存器中填充参数，然后调用一个特别的指令跳转到kernel中预先定义好的位置。当程序跳转到这个位置的时候，硬件就知道程序不再以受限模式运行，而是处于kernel中。这个可以让process跳转到kernel当中的特别的指令就是system_call. system_call的过程是这样的:\n检查system call number, 从而告诉kernel是哪个服务被请求 查看sys_call_table获得对应kernel function的地址 调用查找到的kernel function 等待调用的kernel function返回 做一些system check,然后返回到process当中 如果process时间用完了,会返回到别的process. 这一部分代码在arch/$(architecture)/kernel/entry.S中ENTRY(system_call)的位置. 如果我想改变一个system call的行为, 假设这个system call的符号是system_call_1, 对应的kernel function符号是system_call_1_cb. 我需要自己实现一个函数my_system_call_1_cb(通常只用在my_system_call_1_cb里写一点点定制的代码,然后在末尾调用system_call_1_cb), 然后在sys_call_table中存的system_call_1_cb的指针换成my_system_call_1_cb就可以了. 注意:如果我在module里覆盖了一个system call, 当这个module加载进kernel然后再卸载出kernel之后, sys_call_table中被修改的指针没有改回去, 务必记得在cleanup_module中将sys_call_table恢复到原来的状态.\n如何修改sys_call_table的内容?\n# 不是su权限查看不到kernel中的symbol和address信息, address会返回全零 $\u0026gt; cat /proc/kallsyms | grep sys_call_table 0000000000000000 R sys_call_table 0000000000000000 R compat_sys_call_table $\u0026gt; sudo cat /proc/kallsyms | grep sys_call_table ffffffebcd190938 R sys_call_table ffffffebcd196a38 R compat_sys_call_table 注意:不要在生产用途中篡改任何system call. 举一个导致系统崩溃的例子, 假设module A实现了A_openat用于篡改打开文件的system call openat, module B实现了B_openat用于篡改openat. 如果module A先加载进kernel, module B再加载进kernel, 然后module A先从kernel中卸载, module B再从kernel中卸载. 最终这个system call对应的kernel function pointer指向A_openat, 但是module A已经从kernel中卸载, 就变成了一个野指针, 再次调用这个system call就会导致kernel crash.\n阻塞Processes和Threads Sleep 当module当前无法回应process的请求的时候,可以将这个process sleep, 直到可以回应的时候再唤醒.\nstatic DECLARE_WAIT_QUEUE_HEAD(waitq); wait_event_interruptible(waitq,); wake_up(\u0026amp;waitq); module_put(THIS_MODULE); Completions 在一个module内部,有时需要面对多线程执行的先后顺序的问题. Kernel有一种允许timeout和interrupt的实现方式, 就是Completion.\nstatic struct { struct completion a_comp; struct completion b_comp; } machine; // 声明machine变量(具有匿名类型) static int machine_thread_a(void *arg) { completion_all(\u0026amp;machine.a_comp); kthread_complete_and_exit(\u0026amp;machine.a_comp, 0); } static int machine_thread_b(void *arg) { // 等待completion a_comp完成 wait_for_completion(\u0026amp;machine.a_comp); completion_all(\u0026amp;machine.b_comp); kthread_complete_and_exit(\u0026amp;machine.b_comp, 0); } static int mod_use_completion_init(void) { // 声明两个task struct task_struct *thread_a; struct task_struct *thread_b; pr_info(\u0026#34;example of completion\\n\u0026#34;); // 初始化completion init_completion(\u0026amp;machine.a_comp); init_completion(\u0026amp;machine.b_comp); // 创建thread_a thread_a = kthread_create(machine_thread_a, NULL, \u0026#34;KThread a\u0026#34;); if (IS_ERR(thread_a)) { goto ERROR_THREAD_A; } // 创建thread_b thread_b = kthread_create(machine_thread_b, NULL, \u0026#34;KThread b\u0026#34;); if (IS_ERR(thread_b)) { goto ERROR_THREAD_B; } // wake_up_process(thread_a); wake_up_process(thread_b); return 0; ERROR_THREAD_B: kthread_stop(thread_a); ERROR_THREAD_A: return -1; } static void mod_use_completion_exit(void) { wait_for_completion(\u0026amp;machine.a_comp); wait_for_completion(\u0026amp;machine.b_comp); } module_init(mod_use_completion_init); module_exit(mod_use_completion_exit); 此外还有一些基于wait_for_completion函数而来的变体函数(可以设置timeout或者可以设置interrupt). 一般来说wait_for_completion足以满足绝大部分场景.\n避免Collision和Deadlock Mutex Spinlocks Read and write locks Atomic operations Mutex static DEFINE_MUTEX(my_mutex); mutex_trylock(\u0026amp;my_mutex); mutex_is_locked(\u0026amp;my_mutex); mutex_unlock(\u0026amp;my_mutex); Spinlocks 自旋锁会锁住代码正在运行的CPU, 并占用其100%的资源. 最好只在预期运行时间不超过几毫秒, 并且用户看不出任何事物明显变慢的代码上使用.\nRead and write locks 读写锁是一种特化的自旋锁.\nstatic DEFINE_RWLOCK(my_rwlock); unsigned long flags; read_lock_irqsave(\u0026amp;my_rwlock, flags); read_unlock_irqrestore(\u0026amp;my_rwlock, flags); write_lock_irqsave(\u0026amp;my_rwlock, flags); write_unlock_irqrestore(\u0026amp;my_rwlock, flags); Atomic operations 练习: Replacing Print Macros printk.h中定义的Print Macros将字符串发送到日志当中, 当是有时, 我想让模块将字符串发送到执行了加载这个模块的命令的tty(teletype, text stream abstraction of )当中.\nstatic void print_string(char *str) { /* The tty for the current task */ struct tty_struct *my_tty = get_current_tty(); /* If my_tty is NULL, the current task has no tty you can print to (i.e., * if it is a daemon). If so, there is nothing we can do. */ if (my_tty) { const struct tty_operations *ttyops = my_tty-\u0026gt;driver-\u0026gt;ops; /* my_tty-\u0026gt;driver is a struct which holds the tty\u0026#39;s functions, * one of which (write) is used to write strings to the tty. * It can be used to take a string either from the user\u0026#39;s or * kernel\u0026#39;s memory segment. * * The function\u0026#39;s 1st parameter is the tty to write to, because the * same function would normally be used for all tty\u0026#39;s of a certain * type. * The 2nd parameter is a pointer to a string. * The 3rd parameter is the length of the string. * * As you will see below, sometimes it\u0026#39;s necessary to use * preprocessor stuff to create code that works for different * kernel versions. The (naive) approach we\u0026#39;ve taken here does not * scale well. The right way to deal with this is described in * section 2 of * linux/Documentation/SubmittingPatches */ (ttyops-\u0026gt;write)(my_tty, str, strlen(str)); /* ttys were originally hardware devices, which (usually) strictly * followed the ASCII standard. In ASCII, to move to a new line you * need two characters, a carriage return(回车) and a line feed(换行). * On Unix, the ASCII line feed is used for both purposes - so we can not * just use \\n, because it would not have a carriage return and the * next line will start at the column right after the line feed. * * This is why text files are different between Unix and MS Windows. * In CP/M and derivatives, like MS-DOS and MS Windows, the ASCII * standard was strictly adhered to, and therefore a newline requires * both a LF and a CR. */ (ttyops-\u0026gt;write)(my_tty, \u0026#34;\\015\\012\u0026#34;, 2); } } 练习: Flashing keyboard LEDs #include \u0026lt;linux/init.h\u0026gt; #include \u0026lt;linux/kd.h\u0026gt; /* For KDSETLED, */ #include \u0026lt;linux/module.h\u0026gt; #include \u0026lt;linux/tty.h\u0026gt; /* For tty_struct */ #include \u0026lt;linux/vt.h\u0026gt; /* For MAX_NR_CONSOLES, the max number of virtual console supported by kerntl */ #include \u0026lt;linux/vt_kern.h\u0026gt; /* for fg_console, fg_console is the current virtual console */ #include \u0026lt;linux/console_struct.h\u0026gt; /* For vc_cons, the list of virtual consoles */ static struct tty_driver *this_tty_driver; static struct timer_list this_timer; #define BLINK_DELAY (HZ / 5) #define ALL_LED_ON 0x07 #define RESTORE_LEDS 0xFF static void timer_fn(struct timer_list *unused) { struct tty_struct *t = vc_cons[fg_console].d-\u0026gt;port.tty; if (kbledstatus == ALL_LEDS_ON){ kbledstatus = RESTORE_LEDS; }else{ kbledstatus = ALL_LEDS_ON; } (this_tty_driver-\u0026gt;ops-\u0026gt;ioctl)(t, KDSETLED, kbledstatus); this_timer.expires = jiffies + BLINK_DELAY; add_timer(\u0026amp;this_timer); } static int __init kbleds_init(void) { int i; pr_info(\u0026#34;kbleds: scanning consoles...\\n\u0026#34;); pr_info(\u0026#34;kbleds: fg_console %x\\n\u0026#34;, fg_console); for (int i = 0; i \u0026lt; MAX_NR_CONSOLES; i++) { if (NULL == vc_cons[i].d){ break; } pr_info(\u0026#34;poet_atkm: console[%i/%i] #%i, tty %p\\n\u0026#34;, i, MAX_NR_CONSOLES, vc_cons[i].d-\u0026gt;vc_num, (void *)vc_cons[i].d-\u0026gt;port.tty); } pr_info(\u0026#34;kbleds: scan consoles ends\u0026#34;); this_tty_driver = vc_cons[fg_console].d-\u0026gt;port.tty-\u0026gt;driver; pr_info(\u0026#34;kbleds: tty driver magic %x\\n\u0026#34;, this_tty_driver-\u0026gt;magic); timer_setup(\u0026amp;this_timer, timer_fn, 0); this_timer.expires = jiffies + BLINK_DELAY; add_timer(\u0026amp;this_timer); return 0; } static int __exit kbleds_exit(void) { pr_info(\u0026#34;kbleds: exit\\n\u0026#34;); del_timer(\u0026amp;this_timer); (this_tty_driver-\u0026gt;ops-\u0026gt;ioctl)(vc_cons[fg_console].d-\u0026gt;port.tty, KDSETLED, RESTORE_LEDS); } Task Schedule Tasklets Work queues Tasklets static void my_task_fn(unsigned long data) { pr_info(\u0026#34;example tasklet starts\\n\u0026#34;); mdelay(5000); pr_info(\u0026#34;example tasklet ends\u0026#34;); } static DECLEARE_TASKLET(my_task, my_task_fn, 0L); static int example_tasklet_init(void) { pr_info(\u0026#34;tasklet example init\\n\u0026#34;); tasklet_schedule(\u0026amp;my_task); mdelay(2000); pr_info(\u0026#34;example tasklet continues...\\n\u0026#34;); return 0; } static void example_tasklet_exit(void) { pr_info(\u0026#34;tasklet example exit\\n\u0026#34;); tasklet_kell(\u0026amp;my_task); } tasklet的callback不能sleep, 并且不能访问user space data. 因为tasklet的callback运行atomic context中运行,在software interrupt里.\n此外, kernel只允许任意时间给定tasklet只有一个instance在运行,多个不同tasklet的callback可以并行运行.\ntasklet可以替换成workqueue、timer、threaded interrupt.\nkernel正在移除tasklet, 但目前保留了tasklet兼容.\nWork queues 一个Work queue中的work由Completely Fair Scheduler(CFS)方式调度.\nstatic struct workqueue_struct *queue = NULL; static struct work_struct work; static void work_handler(struct work_struct *data) { pr_info(\u0026#34;work handler function.\\n\u0026#34;) } static int __init sched_init(void) { queue = alloc_workqueue(\u0026#34;Hello World\u0026#34;, WQ_UNBOUND, 1); INIT_WORK(\u0026amp;work, work_handler); schedule_work(\u0026amp;work); return 0; } static void __exit sched_exit(void) { destory_workqueue(queue); } 大而全的万金油多线程实现.\nInterrupt Handler CPU和其他硬件的交互有两个类型,一种是CPU主动发送命令给硬件,另一种是硬件需要告诉CPU一些东西. 第二种称为interrupt. Interrupt类型的交互更难实现,因为硬件通常RAM数量很少, 必须在硬件方便而非CPU方便的情况下完成处理, 否则当一条信息可访问的时候CPU没有读进来, 这条信息就丢失了.\nLinux的硬件interrupt称为IRQ. IRQ有short和long两种. short IRQ预计花费非常短的时间, 在此期间机器的其余部分将被阻塞,并且不会处理其他中断. long IRQ预计花费更长的时间, 在此期间其他中断可能发生(除了来自同一设备的中断). 如何可能的话, 尽量将中断处理程序声明为long IRQ.\n当CPU收到一个中断, 它停止正在进行的程序(除非正在处理优先级更高的中断), 将某些参数保存在堆栈上, 然后调用中断处理程序. 这意味着在中断处理程序内部, 某些事情是不被允许的, 因为当前系统处于一种未知的状态. Linux通过将中断处理过程分为两部分来解决这个问题. 第一部分立刻执行并屏蔽中断线. 硬件中断必须快速处理, 第二部分处理被推迟的繁重工作. 例如Softirq.\n有关IRQ的更详细内容请参考\u0026quot;APCI\u0026quot;.\n练习: Detecting Button Presses Crypto SHA256 #define SHA256_LENGTH 256 char *plaintext=\u0026#34;this is a test\u0026#34;; char hash_sha256[SHA256_LENGTH]; struct crypto_shash *sha256; struct shash_desc *shash; sha256=crypto_alloc_shash(\u0026#34;sha256\u0026#34;,0,0); shash=kmalloc(sizeof(struct shash_desc)+crypto_shash_descsize(sha256), GFP_KERNEL); shash-\u0026gt;tfm=sha256; crypto_shash_init(shash); crypto_shash_update(shash, plaintex, strlen(plaintext)); crypto_shash_final(shash,hash_sha256); // print plaintext \u0026#34;plaintex\u0026#34; and hash result shash. kfree(shash); crypto_free_shash(sha256); AES256 这个更复杂一些, AES变种太多, 初始化过程很漫长.\n#define SYMMETRIC_KEY_LENGTH 32 #define CIPHER_BLOCK_SIZE 16 Virtual Input Output Device Driver 通过Event的方式与Device交互\nint send(struct vinput *, char *, int); int read(struct vinput *, char *, int); The Device Model 以上介绍了各种各样的模块做各种各样的事, 但是缺少一致性. 一致性要由我自己定义的, 控制设备启动挂起结束等等功能的一致性模型称为设备模型. 以下是一个样例:\n#include \u0026lt;linux/kernel\u0026gt; #include \u0026lt;linux/module.h\u0026gt; #include \u0026lt;linux/platform_device.h\u0026gt; struct dm_data{ char *greeting; int number; }; static int dm_probe(struct platform_driver *dev) { struct dm_da *pa = (struct dm_data*)(dev-\u0026gt;dev.platform_data); pr_info(\u0026#34;dm probe\\n\u0026#34;); pr_info(\u0026#34;dm greeting: %s; %d\\n\u0026#34;, pd-\u0026gt;greeting, pd-\u0026gt;number); /* Your device initialization code */ return 0; } static int dm_remove(struct platform_driver *dev) { pr_info(\u0026#34;dm example removed\\n\u0026#34;); /* Your device removal code */ return 0; } static int dm_suspend(struct device *dev) { pr_info(\u0026#34;dm example suspend\\n\u0026#34;); /* Your device suspend code */ return 0; } static int dm_resume(struct device *dev) { pr_info(\u0026#34;dm example resume\\n\u0026#34;); /* Your device resume code */ return 0; } static const struct dev_pm_ops dm_pm_ops = { .suspend = dm_suspend, .resume = dm_resume, .poweroff = dm_suspend, .freeze = dm_suspend, .thaw = dm_resume, .restore = dm_resume, }; static struct platform_driver dm_driver = { .driver = { .name = \u0026#34;dm_example\u0026#34;, .pm = \u0026amp;dm_pm_ops, }, .probe = dm_probe, .remove = dm_remove, }; static int dm_init(void) { int ret; pr_info(\u0026#34;device model init\\n\u0026#34;); ret = platform_driver_register(\u0026amp;dm_driver); if (ret){ pr_err(\u0026#34;Unable to register driver\\n\u0026#34;); return ret; } return 0; } static int dm_exit(void) { pr_info(\u0026#34;dm exit\\n\u0026#34;); platform_driver_unregister(\u0026amp;dm_driver); } module_init(dm_init); module_exit(dm_exit); Optimizations 注意:如果计算性能不是瓶颈, 就不要优化.\nLikely and Unlikely conditions // I know allocating memory always expecting succeed. bvl = bvec_alloc(gfp_mask, nr_iovecs, \u0026amp;idx); if (unlikely(!bvl)){ mempool_free(bio, bio_pool); bio = NULL; goto out; } 编译器会改变生成的机器码, 当使用unlikely的时候, 机器会直接执行false分支, 当condition是true的时候, 才会发生一次跳转. 显然, 正确的情况下避免了一次分支跳转, 错误的情况下延迟也会比分支跳转高.\nStatic Keys How to enable:\ngcc support asm goto inline assembly. CONFIG_JUMP_LABEL=y CONFIG_HAVE_ARCH_JUMP_LABEL=y CONFIG_HAVE_ARCH_JUMP_LABEL_RELATIVE=y // API DEFINE_STATIC_KEY_FALSE(x); DEFINE_STATIC_KEY_TRUE(x); DEFINE_STATIC_KEY_FALSE_RO(x);// read only DEFINE_STATIC_KEY_TRUE_RO(x); // read only // 在一些情况下, 一个static key在module init阶段disable(或enable)之后就不会再改变, 这种情况可以声明为只读. 这时, 这个static key就只能在init阶段改值. 在运行时修改一个只读的static key的值会导致一个page fault. DEFINE_STATIC_KEY_FALSE(fkey); pr_info(\u0026#34;fastpath 1\\n\u0026#34;); if (static_branch_unlikely(\u0026amp;fkey)){ pr_info(\u0026#34;slowpath\\n\u0026#34;); } pr_info(\u0026#34;fastpath 2\\n\u0026#34;); 以这个例子, fkey是static key, 并且值为false. 那么上述代码在fastpath 1之后会直接进入fastpath 2, 不会再做分支检查(应该是通过asm goto来实现的). 当fkey被修改为true时, 上述代码则一定会做分支检查.\nCommon Pitfalls Using standard libraries You can not do that.\n在kernel module中, 我只能使用kernel function. 所有的kernel function都可以在/proc/kallsyms中看到.\nDisabling interrupts 我可以禁用一小小会儿, 问题不大. 但是在禁用之后必须记得启用, 否则系统会卡死哦, 那就只能重启了.\nWhere To Go From Here? Kernel newbies.org Documentation subdirectory within the kernel source code. (It is not always easy to understand, but is a good starting point for further investigation) Linus said, the best way to learn the kernel is to read the source code yourself. ","permalink":"https://fanyxok.github.io/blog/linux-kernel-module-programming-guide/","summary":"Linux Kernel Module Programming Guide [TOC]\nEasy Begin 内核模块包 编程内核模块首先要安装内核模块包kmod.\nsudo apt-get install build-essential kmod 命令行的内核模块相关命令 sudo lsmod # 查看所有加载进内核的模块 sudo cat /proc/modules # 和lsmod有什么不同 sudo modinfo ABC_x.ko # 查看模块ABC的信息 sudo insmod ABC_x.ko # 向内核中加载模块ABC sudo rmmod ABC-x # 从内核中卸载模块ABC, 注意ABC_x.ko名称中的\u0026#39;_\u0026#39;字符会被内核自动替换成\u0026#39;-\u0026#39;字符 sudo journalctl --since \u0026#34;1 hour ago\u0026#34; | grep kernel # 查看过去一小时内的内核日志 编码前须知 版本. 如果编译模块A时include的内核头文件版本与加载模块A的内核版本不一致, 会导致加载失败. 如果必须要这样做, 例如模块A兼容多个内核版本, 需要启用CONFIG_MODVERSIONS编译制导. 命令行. 内核模块无法打印输出到屏幕,但可以记录日志信息然后读取日志信息到屏幕. SecureBoot. SecureBoot是UEFI的一个设置, 要求只会引导安全密钥签名过的模块. [Shell]ERROR: could not insert module. [dmesg]Lockdown: insmod: unsigned module loading is restricted; see man kernel lockdown.","title":""},{"content":"几乎所有嵌入式系统都在某种程度上依赖于处理异步事件的能力。 例如，它从加速度计读取外部传感器数据，以便计算步数 或处理周期性计时器事件以触发RTOS的上下文切换。\n在本文中，我们将深入研究ARM Cortex-M 的Exception Model如何支持异步事件的处理。 本文将介绍支持的不同异常类型、术语(NVIC、ISR、Priority)、configuration registers及其常用设置、与Exception有关的Advanced Topic以及一些用C编写的examples。\n注意:在大多数情况下，所有Cortex-M处理器(ARMv6-M、ARMv7-M和ARMv8-M体系结构)的异常处理机制是相同的。本文将在下面的相关章节中指出它们的差异。\nARM Exception Model Overview ARM specification将Exception定义为“a condition that changes the normal flow of control in a program” 1.\n你会经常看到\u0026quot;Exception\u0026quot;和\u0026quot;Interrupt\u0026quot;交替出现. 在ARM文档中, \u0026ldquo;Interrupt\u0026quot;用来描述\u0026quot;Exception\u0026quot;的一种. Exception包含以下3部分标识信息:\nException Number - 表示特定类型Exception的唯一编号(从1开始). 这个编号也用作Vector Table的offset, 这种类型的Exception对应routine的address可以在这里找到. 这样的routine通常被称为Exception Handler或者Interrupt Service Routine (ISR),它是在Exception触发时会被执行的函数. 当Exception被触发时,ARM硬件会自动的从Vector Table中寻找对应的函数指针并执行. Priority Level / Priority Number - 每种Exception都有一个优先级. 大部分exception的Priority Number是可配置的. 反直觉的地方是, 更小的priority number意味着更高的优先级. 所以如果priority level 1的exception和priority level 2的exception同时发生, priority level 1的exception会先被处理. 当本文说一个exception有最高优先级时, 意味着这个exception有最小的Priority Number. 如果两个exception有相同的Priority Number, Exception Number更小的那个会先被处理. Synchronous 或 Asynchronous - 有一些exception触发时被立即被处理(例如 SVCall), 称为 synchronous. 另一些exception触发时不会被立刻处理, 称为 asynchronous. 一个exception可以处于以下4种状态中的一种:\nPending - MCU检测到了exception并进行了调度,但是还没有调用handler. Active - MCU已经开始执行handler但是还没执行结束. handler处于这种状态下的原因可能是因为被更高优先级的handler抢占. Pending \u0026amp; Active - 只有asynchronous exceptions可能处于这种状态, 这种状态基本上意味着MCU正在处理一个相同类型的exception. Inactive - exception既不处于Pending也不处于Active. 某些exception可以选择性的Enabled或Disabled.\n注意: 即使一个exception已经disabled, 它仍然可以处于pending状态. 一旦enabled, 它就会转移到active状态. 所以, 在启用一个interrupt时, 最好清空所有pending状态下的exception.\n接下来探讨ARM Cortex-M MCU上不同类型的exception.\nBuilt in Exceptions These are exceptions that are part of every ARM Cortex-M core. The ARM Cortex-M specifications reserve Exception Numbers 1-15, inclusive, for these.\nNOTE: Recall that the Exception Number maps to an offset within the Vector Table. Index 0 of the Vector Table holds the reset value of the Main stack pointer. The rest of the Vector Table, starting at Index 1, holds Exception Handler pointers.\nSix exceptions are always supported and depending on the Cortex-M variant, additional handlers will be implemented as well. The minimum set is:\nReset - This is the routine executed when a chip comes out of reset. More details can be found within the Zero to main() series of posts. Non Maskable Interrupt (NMI) - As the name implies, this interrupt cannot be disabled. If errors happen in other exception handlers, a NMI will be triggered. Aside from the Reset exception, it has the highest priority of all exceptions. HardFault - The catchall for assorted system failures that can take place such as accesses to bad memory, divide-by-zero errors and illegal unaligned accesses. It’s the only handler for faults on the ARMv6-M architecture but for ARMv7-M \u0026amp; ARMv8-M, finer granularity fault handlers can be enabled for specific error classes (i.e MemManage, BusFault, UsageFault). 2 SVCall - Exception handler invoked when an Supervisor Call (svc) instruction is executed. PendSV \u0026amp; SysTick - System level interrupts triggered by software. They are typically used when running a RTOS to manage when the scheduler runs and when context switches take place. External Interrupts ARM cores also support interrupt lines which are “external” to the core itself. These interrupt lines are usually routed to vendor-specific peripherals on the MCU such as Direct Memory Access (DMA) engines or General Purpose Input/Output Pins (GPIOs). All of these interrupts are configured via a peripheral known as the Nested Vectored Interrupt Controller (NVIC).\nThe Exception Number for external interrupts starts at 16. The ARMv7-M reference manual has a good graphic which displays the Exception number mappings:\nRegisters used to configure Cortex-M Exceptions Exceptions are configured on Cortex-M devices using a small set of registers within the System Control Space (SCS). An in-depth list of all the registers involved in exception handling can be found in the ARMv7-M reference manual 3. A great way to build out an understanding of how the exception subsystem works is to walk through the registers used to configure it. In the sections below we will explore the highlights.\nIf you already have a good feel for Cortex-M exception configuration, I’d recommend skipping to the advanced topics section which covers a few of the more subtle details about Cortex-M exceptions worth noting or to test your knowledge with a more complex configuration example!\nInterrupt Control and State Register (ICSR) - 0xE000ED04 ICSR bit assignments: This register lets one control the NMI, PendSV, and SysTick exceptions and view a summary of the current interrupt state of the system.\nThe most useful status fields are:\nVECTACTIVE - The Exception Number of the currently running interrupt or 0 if none are active. This number is also stored in the IPSR field of the Program Status Register (xPSR). RETTOBASE - A value of 0 means another interrupt is active aside from the currently executing one. This basically reveals whether or not pre-emption took place. This field is not implemented in ARMv6-M devices. VECTPENDING - The Exception Number of the highest outstanding pending interrupt or 0 if there is None. Application Interrupt and Reset Control Register (AIRCR) - 0xE000ED0C AIRCR bit assignments: The highlights with respect to exceptions are:\nSYSRESETREQ - Writing 1 will trigger a system reset resulting in the System Reset Handler getting invoked. PRIGROUP - This field lets you split exception priorities into two parts known as the group priority and subpriority. The setting here indicates how many bits make up the subpriority. The group priority is used to control which interrupts can preempt one another. The subpriority controls the order in which exceptions in the same group will be processed. This field is not implemented in ARMv6-M based devices. This can be helpful if you only want certain groups of interrupts to be able to preempt one another. NOTE: In order to issue a write to this register, the VECTKEY field must be set to 0x05FA.\nSystem Handler Priority Register (SHPR1-SHPR3) - 0xE000ED18 - 0xE000ED20 This bank of registers allows for the priority of system faults with configurable priority to be updated. Note that the register bank index starts at 1 instead of 0. This is because the first exception numbers (corresponding to Reset, NMI, and Hardfault, respectively) do not have a configurable priority so writing to anything in SHPR0 has no effect.\nEach priority configuration occupies 8 bits of a register bank. That means the configuration for Exception Number 11, SVCall, would be in bits 24-32 of SHPR2. The default priority value for all System Exceptions is 0, the highest configurable priority level. For most applications, it’s not typical to need and change these values.\nSystem Handler Control and State Register (SHCSR) - 0xE000ED24 This register lets you view the status of or enable various built in exception handlers:\nNOTE: For ARMv6-M devices the only value which is implemented is SVCALLPENDED\nInterrupt Controller Type Register (ICTR) - 0xE000E004 This register allows you to determine the total number of external interrupt lines supported by an implementation. For ARMv6-M devices (Cortex-M0, Cortex-M0+), this register is not implemented because the number is always 32. For other Cortex-M MCUs, up to 496 lines may be supported! The layout of the register looks like this:\nThe exact number of interrupts supported is easily computed as 32 * (INTLINESNUM + 1)\nNVIC Registers The NVIC has sets of registers for configuring the “external” interrupt lines. The address ranges are allocated to support the maximum number of external interrupts which can be implemented, 496, but usually a smaller set of the registers will be implemented.\nFour of the register types have a single bit allocated per external interrupt. Each type is in a contiguous bank of 32-bit registers. So if we want to configure external interrupt 65, the configuration will be bit 1 of the 3rd 32-bit register in the bank. Recall external interrupts start at offset 16 in the vector table so the Exception Number (index in the vector table) for this interrupt will be 16 + 65 = 81.\nNOTE 1: Utilizing an external interrupt is usually a little bit more involved than it first appear to be. In addition to configuring the NVIC registers for the interrupt, you usually need to configure the MCU specific peripheral to generate the interrupt as well.4\nNOTE 2: While less common in real-world applications, it’s also possible to re-purpose any NVIC interrupt and trigger it via software. We’ll walk through an example of this in the code examples later in the article.\nInterrupt Set-Enable (NVIC_ISER) and Clear-Enable (NVIC_ICER) Registers NVIC_ISER0-NVIC_ISER15: 0xE000E100-0xE000E13C NVIC_ICER0-NVIC_ICER15: 0xE000E180-0xE000E1BC Writing a 1 to the correct bit offset of the register pair will enable or disable the interrupt and a read will return 1 if the interrupt is enabled.\nInterrupt Set-Pending (NVIC_ISPR) and Clear-Pending (NVIC_ICPR) Registers NVIC_ISPR0-NVIC_ISPR15: 0xE000E200-0xE000E23C NVIC_ICPR0-NVIC_ICPR15: 0xE000E280-0xE000E2BC Writing a 1 to the correct bit offset of the register pair will set or clear the pending state of the interrupt and a read will return 1 if the interrupt is already pending.\nInterrupt Active Bit Registers (NVIC_IABR) NVIC_IABR0-NVIC_IABR15: 0xE000E300-0xE000E33C A read only bank of registers which return whether or not the interrupt is active. One thing to note is this register is not implemented in the ARMv6-M architecture (Cortex-M0 \u0026amp; Cortex-M0+).\nInterrupt Priority Registers (NVIC_IPR) NVIC_IPR0-NVIC_IPR123: 0xE000E400-0xE000E5EC The final NVIC configuration register is used to configure the priority of the interrupt. 8 bits are used to configure the priority of each interrupt. The number of supported priority levels is implementation defined and is in the range of 4-256. When less than 256 priority levels are implemented, the lower bits in the field read-as-zero. So, somewhat confusingly, if only 2 bits are implemented, the valid values from highest priority to lowest priority would be 0b000.0000 (0x0), 0b0100.0000 (0x40), 0b1000.0000 (0x80) and 0b1100.0000 (0xC0).\nSoftware Triggered Interrupt Register (STIR) - 0xE000EF00 This register can be used to set an NVIC interrupt to pending. It’s equivalent to setting the appropriate bit in the NVIC_ISPR to 1. The value that needs to be written to the register is the External Interrupt Number (Exception Number - 16). This register is not implemented for the ARMv6-M architecture\nAdvanced Exception Topics Exception Entry \u0026amp; Exit One of my favorite parts about ARM exception entry is that the hardware itself implements the ARM Architecture Procedure Calling Standard (AAPCS). 5 The AAPCS specification defines a set of conventions that must be followed by compilers. One of these requirements is around the registers which must be saved by a C function when calling another function. When an exception is invoked, the hardware will automatically stack the registers that are caller-saved. The hardware will then encode in the link register ($lr) a value known as the EXC_RETURN value. This value tells the ARM core that a return from an exception is taking place and the core can then unwind the stack and return correctly to the code which was running before the exception took place\nBy leveraging these features, exceptions and thread mode code can share the same set of registers and exception entries can be regular C functions! For other architectures, exception handlers often have to be written in assembly.\nTail-Chaining Usually when exiting an exception, the hardware needs to pop and restore at least eight caller-saved registers. However, when exiting an ISR while a new exception is already pended, this pop and subsequent push can be skipped since it would be popping and then pushing the exact same registers! This optimization is known as “Tail-Chaining”.\nFor example, on a Cortex-M3, when using zero wait state memory, it takes 12 clock cycles to start executing an ISR after it has been asserted and 12 cycles to return from the ISR upon its completion. When the register pop and push is skipped, it only takes 6 cycles to exit from one exception and start another one, saving 18 cycles in total!\nLate-arriving Preemption The ARM core can detect a higher priority exception while in the “exception entry phase” (stacking caller registers \u0026amp; fetching the ISR routine to execute). A “late arriving” interrupt is detected during this period. The optimization is that the higher priority ISR can be fetched and executed but the register state saving that has already taken place can be skipped. This reduces the latency for the higher priority interrupt and, conveniently, upon completion of the late arriving exception handler, the processor can then tail-chain into the initial exception that was going to be serviced.\nLazy State Preservation ARMv7 \u0026amp; ARMv8 devices can implement an optional Floating Point Unit (FPU) for native floating point support. This comes with the addition of 33 four-byte registers (s0-s31 \u0026amp; fpscr). 17 of these are “caller” saved and need to be dealt with by the ARM exception entry handler. Since FPU registers are not often used in ISRs, there is an optimization (“lazy context save”)6 that can be enabled which defers the actual saving of the FPU registers on the stack until a floating point instruction is used in the exception. By deferring the actual push of the registers, interrupt latency can usually be reduced by the push and pop of these 17 registers!\nA full discussion of FPU stacking optimizations is outside the scope of this article but better managing how the registers are stacked can also be a very helpful tool for reducing stack overflows and memory usage in embedded environments … Preserving FPU state across RTOS context switches requires an additional 132 bytes (33 registers) of data to be tracked for each thread! For further reading, ARM wrote a great application note highlighting the lazy preservation FPU features.7\nExecution Priority \u0026amp; Priority Boosting If no exception is active, the current “execution priority” of the system can be thought of as being the “highest configurable priority level” + 1 – essentially meaning if any exception is pended, the currently running code will be interrupted and the ISR will run.\nThere are a few ways in software that the “execution priority” can be manipulated to be above the default priority of thread mode or the exception that is active. This is known as “priority boosting”. This can be useful to do in software when running code that cannot be interrupted such as the logic dealing with context switching in a RTOS.\nPriority boosting is usually controlled via three register fields:\nPRIMASK - Typically configured in code using the CMSIS __disable_irq() and __enable_irq() routines or the cpsid i and cpsie i assembly instructions directly. Setting the PRIMASK to 1 disables all exceptions of configurable priority. This means, only NMI, Hardfault, \u0026amp; Reset exceptions can still occur. FAULTMASK - Typically configured in code using the CMSIS __disable_fault_irq() and __enable_fault_irq() routines or the cpsid f and cpsie f assembly instructions directly. Setting the FAULTMASK disables all exceptions except the NMI exception. This register is not available for ARMv6-M devices. BASEPRI- Typically configured using the CMSIS __set_BASEPRI() routine. The register can be used to prevent exceptions up to a certain priority from being activated. It has no effect when set to 0 and can be set anywhere from the highest priority level, N, to 1. It’s also not available for ARMv6-M based MCUs. Interruptible-continuable instructions Most ARM instructions run to completion before an interrupt is executed and are atomic. For example, any aligned 32-bit memory access is an atomic operation. However, to minimize interrupt latency, some of the longer multi-cycle instructions can be aborted and re-started after the exception completes. These include divide instructions (udiv \u0026amp; sdiv) and double word load/store instructions (ldrd \u0026amp; strd).\nSome instructions are also “interruptible-continuable” which means they can be interrupted but will resume from where they left off on exception return. These include the Load and Store Multiple registers instructions (ldm and stm). This feature is not supported for ARMv6-M and instead the instructions will just be aborted and restarted.\nNOTE: It’s generally a good idea to refrain from using load-multiple or store-multiple instructions to memory regions or variables where repeated reads or writes could cause issues or to guard these accesses in a critical section by disabling interrupts.\nCode Examples For this setup we will use a nRF52840-DK8 running the blinky demo application from the v15.2 SDK9 with a modified main.c that can be found here. However, you should be able to run similar code snippets on pretty much any Cortex-M MCU.\nWe’ll use SEGGER’s JLinkGDBServer10 as our debugger to step through these examples.\nSetup Prep Most SDKs have a pre-defined vector table with default Exception Handlers. The definitions for the Handlers are usually defined as “weak” so they can be overridden.\nCAUTION: The default exception handler provided in most vendor SDKs is usually defined as a while(1){} loop – even for fault handlers! This means when a fault occurs the MCU will just sit in an infinite loop. The device will only recover in this situation if it’s manually reset or runs out of power. It’s generally a good idea to make sure all exception handlers at least reboot the device when a fault occurs to give the device a chance to recover\nWhen building the blinky app for the NRF52840 with gcc, this vector table definition can be found at modules/nrfx/mdk/gcc_startup_nrf52840.S:\n.section .isr_vector .align 2 .globl __isr_vector __isr_vector: .long __StackTop /* Top of Stack */ .long Reset_Handler .long NMI_Handler .long HardFault_Handler .long MemoryManagement_Handler .long BusFault_Handler .long UsageFault_Handler .long 0 /*Reserved */ .long 0 /*Reserved */ .long 0 /*Reserved */ .long 0 /*Reserved */ .long SVC_Handler .long DebugMon_Handler .long 0 /*Reserved */ .long PendSV_Handler .long SysTick_Handler /* External Interrupts */ .long POWER_CLOCK_IRQHandler .long RADIO_IRQHandler .long UARTE0_UART0_IRQHandler .long SPIM0_SPIS0_TWIM0_TWIS0_SPI0_TWI0_IRQHandler .long SPIM1_SPIS1_TWIM1_TWIS1_SPI1_TWI1_IRQHandler .long NFCT_IRQHandler .long GPIOTE_IRQHandler .long SAADC_IRQHandler .long TIMER0_IRQHandler .long TIMER1_IRQHandler .long TIMER2_IRQHandler .long RTC0_IRQHandler .long TEMP_IRQHandler .long RNG_IRQHandler [...] We can pretty easily compute the Exception Number by counting the offset within this table. __StackTop is 0, Reset_Handler is 1, POWER_CLOCK_IRQHandler is 16.\nMost vendors also provide a CMSIS compatible IRQn_Type define which gives you the enumerated list of External Interrupt Numbers (Exception Number - 16). We will want this when we go to configure external interrupts that are part of the NVIC. For the NRF52840, this can be found at modules/nrfx/mdk/nrf52840.h and looks something like this:\ntypedef enum { [...] POWER_CLOCK_IRQn = 0, /*!\u0026lt; 0 POWER_CLOCK */ RADIO_IRQn = 1, /*!\u0026lt; 1 RADIO */ UARTE0_UART0_IRQn = 2, /*!\u0026lt; 2 UARTE0_UART0 */ SPIM0_SPIS0_TWIM0_TWIS0_SPI0_TWI0_IRQn= 3, /*!\u0026lt; 3 SPIM0_SPIS0_TWIM0_TWIS0_SPI0_TWI0 */ SPIM1_SPIS1_TWIM1_TWIS1_SPI1_TWI1_IRQn= 4, /*!\u0026lt; 4 SPIM1_SPIS1_TWIM1_TWIS1_SPI1_TWI1 */ NFCT_IRQn = 5, /*!\u0026lt; 5 NFCT */ GPIOTE_IRQn = 6, /*!\u0026lt; 6 GPIOTE */ SAADC_IRQn = 7, /*!\u0026lt; 7 SAADC */ TIMER0_IRQn = 8, /*!\u0026lt; 8 TIMER0 */ TIMER1_IRQn = 9, /*!\u0026lt; 9 TIMER1 */ TIMER2_IRQn = 10, /*!\u0026lt; 10 TIMER2 */ RTC0_IRQn = 11, /*!\u0026lt; 11 RTC0 */ [...] } IRQn_Type; As discussed above, the actual number of interrupt priority levels is implementation specific. You can find the number of levels implemented in the vendors data sheet for the MCU being used or determine it dynamically with gdb. Unimplemented bits are Read-as-Zero (RAZ) in the NVIC_IPR registers so if we write 0xff and read it back we can figure out the number of levels. Let’s give it a try in GDB:\n(gdb) p/x *(uint32_t*)0xE000E400 $1 = 0x0 (gdb) set *(uint32_t*)0xE000E400=0xff (gdb) p/x *(uint32_t*)0xE000E400 $2 = 0xe0 Great! We see the top 3 bits “stuck” which means the NRF52840 MCU supports 8 priority levels (0-7).\nTriggering a Built In Exception (PendSV) Let’s first start by generating a common built in exception, often used for RTOS context switching, the PendSV exception handler. To make it easier to step through the code with a debugger and examine register state, let’s utilize breakpoint instructions.\nvoid PendSV_Handler(void) { __asm(\u0026quot;bkpt 1\u0026quot;); } __attribute__((optimize(\u0026quot;O0\u0026quot;))) static void trigger_pendsv(void) { volatile uint32_t *icsr = (void *)0xE000ED04; // Pend a PendSV exception using by writing 1 to PENDSVSET at bit 28 *icsr = 0x1 \u0026lt;\u0026lt; 28; // flush pipeline to ensure exception takes effect before we // return from this routine __asm(\u0026quot;isb\u0026quot;); } Let’s call trigger_pendsv() from our main loop and see what happens!\n(gdb) c Continuing. Program received signal SIGTRAP, Trace/breakpoint trap. PendSV_Handler () at ../../../main.c:48 48 __asm(\u0026quot;bkpt 1\u0026quot;); (gdb) Great we see the PendSV_Handler was invoked. We can read the ICSR register (specifically VECTACTIVE, RETTOBASE, \u0026amp; VECTPENDING) described above for additional context:\n(gdb) p/x (*(uint32_t*)0xE000ED04)\u0026amp;0xff $2 = 0xe (gdb) p/x (*(uint32_t*)0xE000ED04)\u0026gt;\u0026gt;11\u0026amp;0x1 $3 = 0x1 (gdb) p/x (*(uint32_t*)0xE000ED04)\u0026gt;\u0026gt;12\u0026amp;0xff $4 = 0x0 The first 8 bits (VECTACTIVE) tell us that Exception Number 0xe is active. This is the PendSV Exception so that matches what we expect! We see RETTOBASE is 1 so no other exceptions are active. And bits 12-20 (VECTPENDING) are zero so we also know no other exceptions are pended.\nPre-emption of an NVIC Interrupt Now let’s configure one interrupt in the NVIC and then call trigger_pendsv() from that interrupt to check out pre-emption!\n__attribute__((optimize(\u0026quot;O0\u0026quot;))) void POWER_CLOCK_IRQHandler(void) { __asm(\u0026quot;bkpt 2\u0026quot;); trigger_pendsv(); __asm(\u0026quot;bkpt 3\u0026quot;); } static void trigger_nvic_int0(void) { // Let's set the interrupt priority to be the // lowest possible for the NRF52. Note the default // NVIC priority is zero which would match our current pendsv // config so no pre-emption would take place if we didn't change this volatile uint32_t *nvic_ipr = (void *)0xE000E400; *nvic_ipr = 0xe0; // Enable the POWER_CLOCK_IRQ (External Interrupt 0) volatile uint32_t *nvic_iser = (void *)0xE000E100; *nvic_iser |= 0x1; // Pend an interrupt volatile uint32_t *nvic_ispr = (void *)0xE000E200; *nvic_ispr |= 0x1; // flush pipeline to ensure exception takes effect before we // return from this routine __asm(\u0026quot;isb\u0026quot;); } Let’s call trigger_nvic_int0 from our main loop and explore what happens!\nProgram received signal SIGTRAP, Trace/breakpoint trap. POWER_CLOCK_IRQHandler () at ../../../main.c:53 53 __asm(\u0026quot;bkpt 2\u0026quot;); (gdb) p/x *(uint32_t*)0xE000ED04 $1 = 0x810 Reading the ICSR register again, we see the active exception number is 0x10 corresponding to external interrupt 0 and that no other exceptions are pended or active. Let’s continue!\n(gdb) next 54 trigger_pendsv(); (gdb) c Continuing. Program received signal SIGTRAP, Trace/breakpoint trap. PendSV_Handler () at ../../../main.c:38 38 __asm(\u0026quot;bkpt 1\u0026quot;); (gdb) bt #0 PendSV_Handler () at ../../../main.c:38 #1 \u0026lt;signal handler called\u0026gt; #2 0x00000306 in trigger_pendsv () at ../../../main.c:48 #3 0x00000322 in POWER_CLOCK_IRQHandler () at ../../../main.c:54 #4 \u0026lt;signal handler called\u0026gt; #5 0x000003a8 in trigger_nvic_int0 () at ../../../main.c:76 #6 main (a=\u0026lt;optimized out\u0026gt;, argv=\u0026lt;optimized out\u0026gt;) at ../../../main.c:129 (gdb) p/x *(uint32_t*)0xE000ED04 $2 = 0xe The active exception is 0xe, PendSV – just like we saw in the first example. We see the RETTOBASE bit is clear meaning another exception is active (NVIC Interrupt 0). We can also check this by looking at the NVIC_IABR registers described above and confirming bit 1 is set:\n(gdb) p/x *(uint32_t[16] *)0xE000E300 $3 = {0x1, 0x0 \u0026lt;repeats 15 times\u0026gt;} We can continue from here and confirm we drop back to the first exception:\n(gdb) next POWER_CLOCK_IRQHandler () at ../../../main.c:55 55 __asm(\u0026quot;bkpt 3\u0026quot;); (gdb) p/x *(uint16_t[16] *)0xE000E300 $4 = {0x1, 0x0 \u0026lt;repeats 15 times\u0026gt;} (gdb) p/x *(uint32_t*)0xE000ED04 $5 = 0x810 Three NVIC Interrupts Pended At Once For our final example, let’s pend a couple exceptions at the same time so we can inspect hands on how the ARM core executes them in priority order.\nCan you tell from the example code the order the breakpoints will be hit in?\n// External Interrupt 9 void TIMER1_IRQHandler(void) { __asm(\u0026quot;bkpt 4\u0026quot;); } // External Interrupt 10 void TIMER2_IRQHandler(void) { __asm(\u0026quot;bkpt 5\u0026quot;); } // External Interrupt 11 void RTC0_IRQHandler(void) { __asm(\u0026quot;bkpt 6\u0026quot;); } static void trigger_nvic_int9_int10_int11(void) { // Let's prioritize the interrupts with 9 having the lowest priority // and 10 \u0026amp; 11 having the same higher priority. // Each interrupt has 8 config bits allocated so // 4 interrupts can be configured per 32-bit register. This // means 9, 10, 11 are next to each other in IPR[2] volatile uint32_t *nvic_ipr2 = (void *)(0xE000E400 + 8); // Only 3 priority bits are implemented so we need to program // the upper 3 bits of each mask *nvic_ipr2 |= (0x7 \u0026lt;\u0026lt; 5) \u0026lt;\u0026lt; 8; *nvic_ipr2 |= (0x6 \u0026lt;\u0026lt; 5) \u0026lt;\u0026lt; 16; *nvic_ipr2 |= (0x6 \u0026lt;\u0026lt; 5) \u0026lt;\u0026lt; 24; // Enable interrupts for TIMER1_IRQHandler, // TIMER2_IRQHandler \u0026amp; RTC0_IRQHandler volatile uint32_t *nvic_iser = (void *)0xE000E100; *nvic_iser |= (0x1 \u0026lt;\u0026lt; 9) | (0x1 \u0026lt;\u0026lt; 10) | (0x1 \u0026lt;\u0026lt; 11); // Pend an interrupt volatile uint32_t *nvic_ispr = (void *)0xE000E200; *nvic_ispr |= (0x1 \u0026lt;\u0026lt; 9) | (0x1 \u0026lt;\u0026lt; 10) | (0x1 \u0026lt;\u0026lt; 11); // flush pipeline to ensure exception takes effect before we // return from this routine __asm(\u0026quot;isb\u0026quot;); } Let’s call trigger_nvic_int9_int10_int11() and try it out!\n(gdb) c Continuing. Program received signal SIGTRAP, Trace/breakpoint trap. TIMER2_IRQHandler () at ../../../main.c:81 81 __asm(\u0026quot;bkpt 5\u0026quot;); (gdb) So the external interrupt 10 (exception number 26) fired first. It has the same priority as NVIC Interrupt 11 but the ARM core prioritizes higher exception numbers first which is why External Interrupt 10 is the first one that runs. We would expect NVIC Interrupt 11 to run next.\nLet’s check and see what info is in the ICSR register this time:\n(gdb) p/x *(uint32_t*)0xE000ED04 $9 = 0x41b81a (gdb) p/d (*(uint32_t*)0xE000ED04)\u0026amp;0xff $10 = 26 (gdb) p/d (*(uint32_t*)0xE000ED04)\u0026gt;\u0026gt;12\u0026amp;0x1 $11 = 1 (gdb) p/d (*(uint32_t*)0xE000ED04)\u0026gt;\u0026gt;12\u0026amp;0xff $12 = 27 VECTACTIVE is 26 which matches what we expect. This time VECTPENDING is set too! The value is 27 which confirms that External Interrupt 11 (27-16) should be the next one to fire.\nWe can see all the NVIC interrupts that are pended by looking at the NVIC_ISPR register described above. We should see bits 9 and 11 set since those interrupts haven’t run yet\n(gdb) p/x *(uint32_t[16] *)0xE000E200 $13 = {0xa00, 0x0 \u0026lt;repeats 15 times\u0026gt;} Let’s step through the rest of the code and confirm we see bkpt 6 followed by bkpt 4:\n(gdb) c Continuing. Program received signal SIGTRAP, Trace/breakpoint trap. RTC0_IRQHandler () at ../../../main.c:86 86 __asm(\u0026quot;bkpt 6\u0026quot;); (gdb) next main (a=\u0026lt;optimized out\u0026gt;, argv=\u0026lt;optimized out\u0026gt;) at ../../../main.c:127 127 bsp_board_led_invert(i); (gdb) c Continuing. Program received signal SIGTRAP, Trace/breakpoint trap. TIMER1_IRQHandler () at ../../../main.c:76 76 __asm(\u0026quot;bkpt 4\u0026quot;); Closing I hope this post gave you a useful overview of how the ARM Cortex-M Exception model works and that maybe you learned something new! There’s a lot of different reference manuals and books about the topic but I’ve always found it hard to find a single place that aggregates the useful information.\nAre there any other topics related to interrupts you’d like us to delve into? (No pun intended :D) Do you leverage any of ARMs fancy exception configuration features in your products? Let us know in the discussion area below!\nInterested in learning more about debugging HardFaults? Watch this webinar recording..\nSee anything you\u0026rsquo;d like to change? Submit a pull request or open an issue at GitHub\nAdditional Reading If you’d like to read even more here’s some other discussions about Cortex-M exceptions that I’ve found to be interesting:\nCortex-M Exception Handling Cutting Through the Confusion with Arm Cortex-M Interrupt Priorities Interruptible Instructions Reference Links ARMv7-M Specification ↩\nSee “Overview of the exceptions supported” section ↩\nSee “Exception status and control” section ↩\nSee “GPIO tasks and events” for NRF52 GPIOTE interrupt configuration details ↩\nARM Architecture Procedure Calling Standard (AAPCS) ↩\nSee “Lazy context save of FP state” for more details ↩\nCortex-M4F Lazy Stacking and Context Switch App note ↩\nnRF52840 Development Kit ↩\nv15.2 SDK ↩\nJLinkGDBServer ↩\n","permalink":"https://fanyxok.github.io/blog/armv-m-exception/","summary":"几乎所有嵌入式系统都在某种程度上依赖于处理异步事件的能力。 例如，它从加速度计读取外部传感器数据，以便计算步数 或处理周期性计时器事件以触发RTOS的上下文切换。\n在本文中，我们将深入研究ARM Cortex-M 的Exception Model如何支持异步事件的处理。 本文将介绍支持的不同异常类型、术语(NVIC、ISR、Priority)、configuration registers及其常用设置、与Exception有关的Advanced Topic以及一些用C编写的examples。\n注意:在大多数情况下，所有Cortex-M处理器(ARMv6-M、ARMv7-M和ARMv8-M体系结构)的异常处理机制是相同的。本文将在下面的相关章节中指出它们的差异。\nARM Exception Model Overview ARM specification将Exception定义为“a condition that changes the normal flow of control in a program” 1.\n你会经常看到\u0026quot;Exception\u0026quot;和\u0026quot;Interrupt\u0026quot;交替出现. 在ARM文档中, \u0026ldquo;Interrupt\u0026quot;用来描述\u0026quot;Exception\u0026quot;的一种. Exception包含以下3部分标识信息:\nException Number - 表示特定类型Exception的唯一编号(从1开始). 这个编号也用作Vector Table的offset, 这种类型的Exception对应routine的address可以在这里找到. 这样的routine通常被称为Exception Handler或者Interrupt Service Routine (ISR),它是在Exception触发时会被执行的函数. 当Exception被触发时,ARM硬件会自动的从Vector Table中寻找对应的函数指针并执行. Priority Level / Priority Number - 每种Exception都有一个优先级. 大部分exception的Priority Number是可配置的. 反直觉的地方是, 更小的priority number意味着更高的优先级. 所以如果priority level 1的exception和priority level 2的exception同时发生, priority level 1的exception会先被处理. 当本文说一个exception有最高优先级时, 意味着这个exception有最小的Priority Number. 如果两个exception有相同的Priority Number, Exception Number更小的那个会先被处理.","title":"A Practical guide to ARM Cortex-M Exception Handling"},{"content":"","permalink":"https://fanyxok.github.io/blog/armv-m-context-switch/","summary":"","title":"ARM Cortex-M RTOS Context Switching"},{"content":"Control a Bluetooth Device via Bluetoothctl 1. Overview In this tutorial, we’ll learn how to connect to a Bluetooth device via the terminal. This process involves configuring our Bluetooth controller, pairing it to our target device, and then finally connecting.\n2. Using the bluetoothctl Command BlueZ provides support for Bluetooth functionality and protocols through the bluetoothd daemon. To interact with bluetoothd from the terminal, we use the bluetoothctl command.\nLet’s begin by running bluetoothctl without any arguments:\n$ bluetoothctl Agent registered [bluetooth]# Using bluetoothctl by itself will open the interactive shell. This is called interactive mode and is where we can run commands to configure our Bluetooth settings.\nTo get more information about using interactive mode, let’s run the help command in the interactive shell:\nfreestar.config.enabled_slots.push({ placementName: \u0026ldquo;baeldung_leaderboard_mid_1\u0026rdquo;, slotId: \u0026ldquo;baeldung_leaderboard_mid_1\u0026rdquo; });\n[bluetooth]# help Menu main: Available commands: ------------------- advertise Advertise Options Submenu monitor Advertisement Monitor Options Submenu ... Running help in the shell will list all available commands for bluetoothctl interactive mode with a short summary of what they do.\nSometimes, however, we want to run a command outside of the interactive shell. Luckily, there’s also a non-interactive mode in bluetoothctl, which we can use by running an individual command:\n$ bluetoothctl --help bluetoothctl ver 5.64 Usage: bluetoothctl [--options] [commands] Options: --agent Register agent handler: ... 3. Preparing Our Bluetooth Controller Now that we know the basics of using bluetoothctl, let’s begin preparing our Bluetooth controller.\n3.1. Configuring a Bluetooth Controller Let’s begin to configure our Bluetooth controller by using the show command:\n$ bluetoothctl show 00:1A:7D:DA:71:15 Controller 00:1A:7D:DA:71:15 (public) Name: pc-m Alias: pc-m Class: 0x00000000 Powered: no Discoverable: no DiscoverableTimeout: 0x00000000 Pairable: no ... Discovering: no ... Typically, the bluetoothctl show command will output a large amount of information. However, we just need to ensure our controller is powered-on, discoverable, and pairable.\nLet’s start by powering on our controller:\n$ bluetoothctl power on [CHG] Controller 00:1A:7D:DA:71:15 Class: 0x006c0104 Changing power on succeeded We can use the bluetoothctl power on command to power on our controller. It’s important to power on our Bluetooth controller before modifying other controller attributes.\nNext, we should set the controller to be discoverable and pairable:\n$ bluetoothctl discoverable on Changing discoverable on succeeded $ bluetoothctl pairable on Changing pairable on succeeded We set the controller to discoverable using the command bluetoothctl discoverable on, and then we use the bluetoothctl pairable on command to set our controller to pairable. The output of these commands shows that they were successful.\n3.2. Using Multiple Bluetooth Controllers When using multiple Bluetooth controllers, we must ensure we select the correct one before configuring.\nfreestar.config.enabled_slots.push({ placementName: \u0026ldquo;baeldung_leaderboard_mid_3\u0026rdquo;, slotId: \u0026ldquo;baeldung_leaderboard_mid_3\u0026rdquo; });\nLet’s use the bluetoothctl list command to get a list of connected Bluetooth controllers:\n$ bluetoothctl list Controller 00:1A:7D:DA:71:15 pc-m [default] Controller 34:02:86:03:7C:F2 pc-m #2 This command outputs information about the connected Bluetooth controllers, including their MAC addresses and what the default controller is. The default controller is the controller that will be operated on when we run a command.\nTo change the default controller, we use the bluetoothctl select command, passing the MAC address of the controller we want to connect to:\n[bluetooth]# select 34:02:86:03:7C:F2 Controller 34:02:86:03:7C:F2 pc-m [default] [bluetooth]# list Controller 00:1A:7D:DA:71:15 pc-m Controller 34:02:86:03:7C:F2 pc-m #2 [default] In this example, we used the select command in interactive mode. Non-interactive mode opens up a new session per command, but the interactive shell maintains the same session until exited. Since the select command only changes the default controller for the current session, it will only work within interactive mode.\nHowever, there is a way we can use select in a similar way to non-interactive mode:\n$ bluetoothctl \u0026lt;\u0026lt;\u0026lt; $'select 34:02:86:03:7C:F2\\nlist\\n' ... [bluetooth]# select 34:02:86:03:7C:F2 Controller 34:02:86:03:7C:F2 pc-m [default] [bluetooth]# list Controller 00:1A:7D:DA:71:15 pc-m Controller 34:02:86:03:7C:F2 pc-m #2 [default] In this example, we use a here-string to redirect our string to stdin. This causes the bluetoothctl shell to treat the string as user input and allows us to automate the process of using interactive mode. Our string can contain as many commands as want as long as we end each with a newline.\n4. Connect a Bluetooth Device Using the Terminal After configuring our Bluetooth controller, we can begin pairing and connecting our device.\n4.1. Pairing a Bluetooth Device Now, we can begin to pair our Bluetooth device to our controller. To start, we should turn our controller to discovery mode:\n$ bluetoothctl scan on Discovery started [CHG] Controller 00:1A:7D:DA:71:15 Discovering: yes ^Z [1]+ Stopped bluetoothctl scan on To set the controller to discovery mode, we use the bluetoothctl scan on command. However, this command is a foreground job, which means that we won’t be able to use the terminal until it is finished. So, we put it in the background using Ctrl-Z.\nNow, let’s output the discovered devices using bluetoothctl devices:\n$ bluetoothctl devices Device 3C:4D:BE:84:1F:BC MyEarbuds Device 60:B7:6E:35:39:0D MyPhone The output shows us any discovered devices with their names and MAC addresses. The important part is the MAC address, which we use to pair devices.\nOnce we know the MAC address of our device, we can begin pairing. Let’s use the bluetoothctl pair command to pair to our device with the name “MyEarbuds”:\n$ bluetoothctl pair 3C:4D:BE:84:1F:BC Attempting to pair with 3C:4D:BE:84:1F:BC [CHG] Device 3C:4D:BE:84:1F:BC Connected: yes ... [CHG] Device 3C:4D:BE:84:1F:BC Paired: yes Pairing successful We can pair a device by using the device’s MAC address as an argument to the bluetoothctl pair command. The output will tell us if our device paired successfully or not.\nNow that our device is paired, we don’t need to be in discovery mode anymore. To exit discovery mode, we must end the bluetoothctl scan command that we put into the background:\n$ fg bluetoothctl scan on ^C To stop a background job, we use the fg command to bring the bluetoothctl scan into the foreground. Then we press Ctrl-C to stop the program.\n4.2. Connecting a Bluetooth Device Let’s start connecting to our Bluetooth device using bluetoothctl connect:\nfreestar.config.enabled_slots.push({ placementName: \u0026ldquo;baeldung_incontent_2\u0026rdquo;, slotId: \u0026ldquo;baeldung_incontent_2\u0026rdquo; });\n$ bluetoothctl connect 3C:4D:BE:84:1F:BC Attempting to connect to 3C:4D:BE:84:1F:BC ... Connection successful We can connect a paired Bluetooth device by using its MAC address with the bluetoothctl connect command. The output will show whether we successfully connected to our device or not.\n4.3. Disconnecting a Bluetooth Device Likewise, we can disconnect our device using the bluetoothctl disconnect command, again passing the MAC address of the device we want to disconnect:\n$ bluetoothctl disconnect 3C:4D:BE:84:1F:BC Attempting to disconnect from 3C:4D:BE:84:1F:BC ... Successful disconnected The output shows us that our device was successfully disconnected.\n5. Overview In this article, we learned how to connect a Bluetooth device via the Linux terminal. We began by learning the basics of the bluetoothctl command. Then, we learned about configuring a Bluetooth controller. Finally, we learned how to pair and then connect a Bluetooth device.\n","permalink":"https://fanyxok.github.io/blog/control-bluetooth-device-via-bluetoothctl/","summary":"Control a Bluetooth Device via Bluetoothctl 1. Overview In this tutorial, we’ll learn how to connect to a Bluetooth device via the terminal. This process involves configuring our Bluetooth controller, pairing it to our target device, and then finally connecting.\n2. Using the bluetoothctl Command BlueZ provides support for Bluetooth functionality and protocols through the bluetoothd daemon. To interact with bluetoothd from the terminal, we use the bluetoothctl command.\nLet’s begin by running bluetoothctl without any arguments:","title":"Control Bluetooth Device via Bluetoothctl"}]